{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cloud AI SDK","text":"<p>Qualcomm Cloud AI SDKs (Platform and Apps) enable high performance deep learning inference on Qualcomm Cloud AI platforms delivering high throughput and low latency across Computer Vision, Object Detection, Natural Language Processing and Generative AI models. The SDKs enable end to end workflows - from onboarding a pre-trained model to deployment of the ML inference application in production. </p> AI 100 Instances on Cloud User Guide  Quick Start Guide  Qualcomm Transformers Library Models, Tutorials and Sample Code Download SDK Blogs"},{"location":"API/","title":"API","text":"Python API  C++ API  OnnxRT API"},{"location":"Cpp-API/example/","title":"InferenceSet IO Example","text":"<p>The following document describes <code>AIC100</code> Example named <code>InferenceSetIOBuffersExample.cpp</code>.</p> <p>This example contains a single C++ file and a <code>CMakeLists.txt</code> that can be used for compiling as part of <code>Qualcomm Cloud AI 100</code> distributed Platform SDK.</p> InferenceSetIOBuffersExample.cpp InferenceSetIOBuffersExample.cpp<pre><code>//-----------------------------------------------------------------------------\n// Copyright (c) 2023 Qualcomm Innovation Center, Inc. All rights reserved.\n// SPDX-License-Identifier: BSD-3-Clause-Clear \n//-----------------------------------------------------------------------------\n\n#include &lt;string&gt;\n#include &lt;vector&gt;\n#include &lt;iostream&gt;\n#include &lt;random&gt;\n#include \"QAicApi.hpp\"\n\nnamespace {\n\n/**\n* used to generate radnom data into input buffers\n* Input buffers are uint8_t arrays\n*/\nstruct RandomGen final {\n    static constexpr const int from = std::numeric_limits&lt;uint8_t&gt;::min();\n    static constexpr int to = std::numeric_limits&lt;uint8_t&gt;::max();\n    std::random_device randdev;\n    std::mt19937 gen;\n    std::uniform_int_distribution&lt;uint8_t&gt; distr;\n    explicit RandomGen() : gen(randdev()), distr(from, to) {}\n    [[nodiscard]] auto next() { return distr(gen); }\n};\n\n/**\n* Simple helper to return true if the buffer mapping instance is an input one\n* @param bufmap buffer mapping instance\n* @return true if the instance is an input buffer one.\n*/\n[[nodiscard]] bool isInputBuffer(const qaic::rt::BufferMapping &amp;bufmap) {\n    return bufmap.ioType == BUFFER_IO_TYPE_INPUT;\n}\n\n/**\n* Helper function to print input/output buffer counts so far, zero based\n* @param bufmap buffer map instance\n* @param inputCount input count to use if the instance is input\n* @param outputCount output count to use if the instance is output\n* @return string formatted using the above info.\n*/\n[[nodiscard]] std::string getPrintName(const qaic::rt::BufferMapping &amp;bufmap,\n                                    const std::size_t inputCount,\n                                    const std::size_t outputCount) {\n    using namespace std::string_literals;\n    return isInputBuffer(bufmap) ? (\"Input \"s + std::to_string(inputCount))\n                                : (\"Output \"s + std::to_string(outputCount));\n}\n\n/**\n* Populate input, output vectors with QBuffer information\n* @param bufmap Buffer mapping instance\n* @param buf Actual QBufffer that was generated at callsite/caller.\n* @param inputBuffers Vector to use in case this is input instance\n* @param outputBuffers Vector to use in case this is an output instance\n*/\nvoid populateVector(const qaic::rt::BufferMapping &amp;bufmap, const QBuffer &amp;buf,\n                    std::vector&lt;QBuffer&gt; &amp;inputBuffers,\n                    std::vector&lt;QBuffer&gt; &amp;outputBuffers) {\n    if (isInputBuffer(bufmap)) {\n        inputBuffers.push_back(buf);\n    } else {\n        outputBuffers.push_back(buf);\n    }\n}\n\n/**\n* Given a buffer and size, populate it with random [0..128] random data\n* @param buf buffer to populate\n* @param sz size of this buffer\n*/\nvoid populateBufWithRandom(uint8_t *buf, const std::size_t sz) {\nRandomGen gen;\n    for (auto iter = buf; iter &lt; buf + sz; ++iter) {\n        *iter = gen.next();\n    }\n}\n\n/**\n* Prepare buffers, vectors given a single buffer mapping. Depending on the\n* input/output instance of the buffer mapping, handle logic accordingly.\n* Only inputbuffers needs to be populated with random data.\n* @param bufmap buffer mapping passed as const\n* @param inputCount input buffers counter\n* @param outputCount output buffers counter\n* @param inputBuffers input vector of QBuffer to append to new QBuffer\n* @param outputBuffers output vector of QBuffer to append to new QBuffer\n*/\nvoid prepareBuffers(const qaic::rt::BufferMapping &amp;bufmap,\n                    std::size_t &amp;inputCount, std::size_t &amp;outputCount,\n                    std::vector&lt;QBuffer&gt; &amp;inputBuffers,\n                    std::vector&lt;QBuffer&gt; &amp;outputBuffers) {\n    std::cout &lt;&lt; getPrintName(bufmap, inputCount, outputCount) &lt;&lt; '\\n';\n    std::cout &lt;&lt; \"\\tname = \" &lt;&lt; bufmap.bufferName &lt;&lt; '\\n';\n    std::cout &lt;&lt; \"\\tsize = \" &lt;&lt; bufmap.size &lt;&lt; '\\n';\n    QBuffer buf{bufmap.size, new uint8_t[bufmap.size]}; // Need to dealloc\n    populateVector(bufmap, buf, inputBuffers, outputBuffers);\n    //\n    // Provide the input to the inference in \"inputBuffers\". Here random data\n    // is used. For providing input as file, use a different api. This example\n    // is for input in memory.\n    //\n    if (isInputBuffer(bufmap)) {\n        populateBufWithRandom(buf.buf, buf.size);\n        ++inputCount;\n    } else {\n        ++outputCount;\n    }\n}\n\n/**\n* Given input and output buffers, release all heap allocated\n* @param inputBuffers vector of QBuffers - inputs\n* @param outputBuffers vector of Qbuffers - outputs\n*/\nvoid releaseBuffers(std::vector&lt;QBuffer&gt; &amp;inputBuffers,\n                    std::vector&lt;QBuffer&gt; &amp;outputBuffers) {\n    const auto release([](const QBuffer &amp;qbuf) { delete[] qbuf.buf; });\n    std::for_each(inputBuffers.begin(), inputBuffers.end(), release);\n    std::for_each(outputBuffers.begin(), outputBuffers.end(), release);\n}\n\n/**\n* Given buffer mapping instance, return true if this instance does not\n* contain input or output buffers (e.g. it contains uninitialized or invalid)\n* @param bufmap buffer mapping instance\n* @return true if the buffer mapping instance does not container a valid buffer\n*/\n[[nodiscard]] bool notInputOrOutput(const qaic::rt::BufferMapping &amp;bufmap) {\n    const std::initializer_list&lt;QAicBufferIoTypeEnum&gt; bufTypes{\n        BUFFER_IO_TYPE_INPUT, BUFFER_IO_TYPE_OUTPUT};\n    const auto func([type = bufmap.ioType](const auto v) { return v == type; });\n    return std::none_of(bufTypes.begin(), bufTypes.end(), func);\n}\n\n} // namespace\n\nint main([[maybe_unused]] int argc, [[maybe_unused]] char *argv[]) {\n    QID qid = 0;\n    std::vector&lt;QID&gt; qidList{qid};\n\n    // *** QPC ***\n    constexpr const char *qpcPath =\n        \"/opt/qti-aic/test-data/aic100/v2/2nsp/2nsp-conv-hmx\"; // (1)\n    auto qpc = qaic::rt::Qpc::Factory(qpcPath);\n\n    // *** CONTEXT ***\n    constexpr QAicContextProperties_t *NullProp = nullptr;\n    auto context = qaic::rt::Context::Factory(NullProp, qidList);\n\n    // *** INFERENCE SET ***\n    constexpr uint32_t setSize = 10;\n    constexpr uint32_t numActivations = 1;\n    auto inferenceSet = qaic::rt::InferenceSet::Factory(\n        context, qpc, qidList.at(0), setSize, numActivations);\n\n    // *** SETUP IO BUFFERS ***\n    qaic::rt::shInferenceHandle submitHandle;\n    auto status = inferenceSet-&gt;getAvailable(submitHandle);\n    if (status != QS_SUCCESS) {\n        std::cerr &lt;&lt; \"Error obtaining Inference Handle\\n\";\n        return -1;\n    }\n    std::size_t numInputBuffers = 0;\n    std::size_t numOutputBuffers = 0;\n    std::vector&lt;QBuffer&gt; inputBuffers, outputBuffers;\n    const auto &amp;bufferMappings = qpc-&gt;getBufferMappings();\n    for (const auto &amp;bufmap : bufferMappings) {\n        if (notInputOrOutput(bufmap)) {\n            continue;\n        }\n        prepareBuffers(bufmap, numInputBuffers, numOutputBuffers, inputBuffers,\n                    outputBuffers);\n    }\n    submitHandle-&gt;setInputBuffers(inputBuffers);\n    submitHandle-&gt;setOutputBuffers(outputBuffers);\n\n    // *** SUBMIT ***\n    constexpr uint32_t inferenceId = 0; // also named as request ID\n    status = inferenceSet-&gt;submit(submitHandle, inferenceId);\n    std::cout &lt;&lt; status &lt;&lt; '\\n';\n\n    // *** COMPLETION ***\n    qaic::rt::shInferenceHandle completedHandle;\n    status = inferenceSet-&gt;getCompletedId(completedHandle, inferenceId);\n    std::cout &lt;&lt; status &lt;&lt; '\\n';\n    status = inferenceSet-&gt;putCompleted(std::move(completedHandle));\n    std::cout &lt;&lt; status &lt;&lt; '\\n';\n\n    // *** GET OUTPUT ***\n    //\n    // At this point, the output is available in \"outputBuffers\" and can be\n    // consumed.\n    //\n\n    // *** Release user allocated buffers ***\n    releaseBuffers(inputBuffers, outputBuffers);\n}\n</code></pre> <ol> <li> Replace the path with model QPC file of interest.</li> </ol> CMakeLists.txt CMakeLists.txt<pre><code># ==============================================================================\n# Copyright (c) 2023 Qualcomm Innovation Center, Inc. All rights reserved.\n# SPDX-License-Identifier: BSD-3-Clause-Clear \n# ==============================================================================\n\nproject(inference-set-io-buffers-example)\ncmake_minimum_required (VERSION 3.15)\nset(CMAKE_CXX_STANDARD 17)\n\ninclude_directories(\"/opt/qti-aic/dev/inc\")\n\nadd_executable(inference-set-io-buffers-example InferenceSetIOBuffersExample.cpp)\nset_target_properties(\n    inference-set-io-buffers-example\n    PROPERTIES\n    LINK_FLAGS \"-Wl,--no-as-needed\"\n)\ntarget_compile_options(inference-set-io-buffers-example PRIVATE\n                    -fstack-protector-all\n                    -Werror\n                    -Wall\n                    -Wextra\n                    -Wunused-variable\n                    -Wunused-parameter\n                    -Wnon-virtual-dtor\n                    -Wno-missing-field-initializers)\ntarget_link_libraries(inference-set-io-buffers-example PRIVATE\n                    pthread\n                    dl)\n</code></pre>"},{"location":"Cpp-API/example/#main-flow","title":"Main Flow","text":"<p>Main function has 8 parts. The example using few helper functions defined in the top anonymous namespace.</p>"},{"location":"Cpp-API/example/#qid","title":"QID","text":"<p>The first part of the <code>main()</code> example will pick <code>QID 0</code>. This is usually the first Enumerated device ID.</p> <p>Though the API is capable of accepting a list of QID's, in this example we only pass a single one in the <code>vector&lt;&gt; int</code> container.</p>"},{"location":"Cpp-API/example/#qpc","title":"QPC","text":"<p>QPC is a container file that includes various parts of the compiled network.</p> <p>The path is hardcoded in this example and could be changed or passed to the program via other means like environment variable or command line arguments.</p> <p><code>qaic::rt::Qpc::Factory</code> API will accept a path to the QPC and returns pack a QPC object to use in the next steps.</p>"},{"location":"Cpp-API/example/#context","title":"Context","text":"<p><code>QAIC</code> Runtime requires a Context object to be created and passed around to various APIs.</p> <p>In this phase we use <code>qaic::rt::Context::Factory</code> API to obtain a new instance of Context.</p> <p>We pass <code>NullProp</code> for no special <code>QAicContextProperties_t</code> attributes and the QID vector that was instantiated before.</p>"},{"location":"Cpp-API/example/#inference-set","title":"Inference Set","text":"<p>Creating an instance of InferenceSet is the next step.</p> <p>InferenceSet is considered as a top-level entity when it comes to running Inferences on Hardware.</p> <p>In this example, we set the Size of the Software/Hardware backlog as 10 possible pending buffers.</p> <p>We have a single activation requested. This means that the provided program (encapsulated in QPC), will be activated as a single instance on the Hardware. We use the single QID provided when creating the InferenceSet instance.</p>"},{"location":"Cpp-API/example/#io-buffers","title":"IO Buffers","text":"<p>Next step is required for setting up Input as well as output buffers.</p> <p>In both Input and Output buffers, the user application will be allocating buffers and also will need to deallocate the buffers before application tear-down.</p> <p>This part have few subsections:</p> <ol> <li>Obtain InferenceHandle to submit the I/O buffers once these created.</li> <li>Allocate the buffers using BufferMappings container. We iterate over each BufferMapping instance to obtain information that helps us to allocate new buffers.</li> <li>Once we have a vectors of allocated Input and Output buffers, we will use the InferenceHandle to submit it. It will be used during inference.</li> </ol> <p>In this example, the helper functions will populate Input buffers with random data just to demonstrate the capabilities of the system.</p>"},{"location":"Cpp-API/example/#submission","title":"Submission","text":"<p>This is the part where the actual submission request is happening.</p> <p>The inferenceSet is used to submit the request, passing submitHandle and user defined inferenceId (which was picked as ID 0)</p>"},{"location":"Cpp-API/example/#completion","title":"Completion","text":"<p>This is a blocking call to wait on inference completion and device output's buffers received in the Application.</p> <p>We use inferenceSet to obtain the completedHandle passing our inferenceId as mentioned above.</p> <p>User is responsible to return the completedHandle back to the Runtime pool and doing so by calling putCompleted using inferenceSet.</p>"},{"location":"Cpp-API/example/#obtaining-output","title":"Obtaining output","text":"<p>In this example we do not do anything with the obtained Output buffers and real-life Application will consume such output data.</p>"},{"location":"Cpp-API/example/#cleanup","title":"Cleanup","text":"<p>Since the buffers are user-allocated buffers using the System's Heap, the users is also in charge of properly releasing these buffers as demonstrated in the last phase of this example.</p>"},{"location":"Cpp-API/example/#helper-functions","title":"Helper Functions","text":"<p>Throughout this example, the following helper functions and constructs are used:</p> <ul> <li><code>RandomGen</code> : Random data generator - to generate random input buffer data.</li> <li><code>isInputBuffer</code> : Query if a specific BufferMapping instance is an input one.</li> <li><code>getPrintName</code> : Return Input or Output strings for standard output printing.</li> <li><code>populateVector</code> : Populate vector - to populate inputs or output vector&lt;&gt; containers.</li> <li><code>populateBufWithRandom</code> : Populate buffer with random data - using the above mentioned Random data generator, given a buffer, populate it with random values.</li> <li><code>prepareBuffers</code> : Prepare buffers - iterates over the BufferMappings container and for each BufferMapping instance, populate input/output vector&lt;&gt; as well as invoke helper function to populate inputs buffers with random data.</li> <li><code>releaseBuffers</code> : Release buffers - iterates over allocated inputs/outputs and release/delete buffers (return memory back to the system's Heap).</li> <li><code>notInputOrOutput</code> : Not input our Output boolean function - Given a BufferMapping instance  return true if this instance is not Input, nor Output buffer instance. (For example could be invalid, or uninitialized). We skip these kind of instances.</li> </ul>"},{"location":"Cpp-API/example/#compile-and-run-commands","title":"Compile and Run Commands","text":"<p>Copy the <code>InferenceSetIOBuffersExample.cpp</code> and <code>CMakeLists.txt</code> in a folder</p> <p>Then compile the example with following commands <pre><code>mkdir build\ncd build\ncmake ..\nmake -j 8\n</code></pre> Finally, run the executable <code>./inference-set-io-buffers-example</code>, accordingly change the <code>qpcPath</code>.</p>"},{"location":"Cpp-API/features/","title":"Features","text":""},{"location":"Cpp-API/features/#profiling-support-in-runtime","title":"Profiling Support in Runtime","text":"<p>When running inferences on AIC100, we might want to get deeper insights into the performance of the AIC100 stack to either triage low performance or for general monitoring.</p> <p>Current AIC100 stack provides mechanism to get performance metrics of key milestones through the inference cycle.</p> <p>Profiling data can be broadly classified into two components:</p> <ol> <li>Host metrics    An inference passes through various software layers on host before it make it to the network on device. Examining the performance on host we can identify if tweaking in network pre/post processing stages or host side multi-threading knobs is required.</li> <li>Network/device metrics    Network metrics provide plethora of information, starting from whole network execution time to detailed operator level performance. This information can be put to use to make the most out of existing network or to optimize network itself.    Note: Network perf collection is not baked into the network by default. It needs to be enabled during network compilation itself. The amount of network perf details present depends on the parameters passed to AIC compiler.</li> </ol>"},{"location":"Cpp-API/features/#profiling-report-types","title":"Profiling Report Types","text":"<p>Using AIC100 software stacks, profiling information can be requested in following types (layouts):</p>"},{"location":"Cpp-API/features/#latency-type","title":"Latency type","text":"<p>Latency type is a CSV style table of key of AIC100 stack. Contains both host and device side information.</p>"},{"location":"Cpp-API/features/#trace-type","title":"Trace type","text":"<p>Trace type is a json formatted Chrome trace data. It can be viewed on any interface that consumes Chrome traces. Contains both host and device side information.</p>"},{"location":"Cpp-API/features/#profiling-report-collection-methods","title":"Profiling Report Collection methods","text":"<p>The above information can be requested from AIC100 stack using two mechanisms. The core content is the same in both mechanisms, just the delivery mechanism changes. The two ways are as follows:</p> <ol> <li>Num-iter based profiling</li> <li>Duration based profiling</li> </ol>"},{"location":"Cpp-API/features/#num-iter-based-profiling","title":"Num-iter based profiling","text":"<p>Alias: Legacy profiling</p> <p>When user creates a profiling handle for num-iter based profiling, they need to specify:</p> <ol> <li>The program to profile,</li> <li>Number of samples to collect,</li> <li>Profiling callback and,</li> <li>Type of profiling output type i.e. latency or trace.</li> </ol> <p>The fundamental idea is that during the creation of profiling handle, the user specifies the number of inferences that needs to be sampled. After profiling is started by the user, the profiling stops and calls user provided callback when:</p> <p>a. Number of samples requested by user has been collected.</p> <p>b. User explicitly stops profiling    In this case, the number of samples collected might be less than that requested during profiling handle creation.</p> <p>After the profiling is stopped, the user can again call start profiling using the same handle. The behavior of the infra will be as if the handle is being triggered for the first time.</p> <p>Refer to section <code>ProfilingHandle</code>_ for HPP API interface.</p> <p>Note in the APIs how at the creation of profiling handle, the user needs to be aware of the program needs to be profiled. Only 1 program can be profiled by a given profiling handle. If user wants to profile multiple programs, multiple profiling handles needs to be created, one for each program.</p>"},{"location":"Cpp-API/features/#duration-based-profiling","title":"Duration based profiling","text":"<p>Alias: Stream profiling</p> <p>When user creates a profiling handle of type duration based profiling or stream profiling, the user needs to specify:</p> <ol> <li>Reporting rate,</li> <li>Sampling rate,</li> <li>Callback,</li> <li>RegEx, and</li> <li>Type of profiling output type i.e. latency or trace.</li> </ol> <p>Notice how there is no condition to specify when profiling should automatically end, hence once the user calls start profiling, the samples are collected till user explicitly calls stop profiling. Also, we do not specify which program we want to profile. We add and remove programs at runtime (even after profiling has started) using appropriate API. Hence, allowing more than one program to be profiled by same profiling handle.</p>"},{"location":"Cpp-API/features/#reporting-rate","title":"Reporting Rate","text":"<p>The profiling callbacks are called at every reporting rate boundary, i.e. suppose the reporting rate set by user is 500ms, and profiling is started at 4seconds and 400ms time-point, the first callback will be called at 4 seconds and 500ms time-point (not the callback did not get called after 500ms but at 500ms boundary). Next callback at 5 seconds time-point and then at 5 seconds 500ms time-point and so on. The callback contains profiling data for inferences that took place between the last report and the current report.</p>"},{"location":"Cpp-API/features/#sampling-rate","title":"Sampling rate","text":"<p>User may not be interested in performance of each and every inference, they may want to just get an over view of the performance and hence can choose to record data for every - second, fourth, eighth or sixteenth inference using the sampling rate knob.</p>"},{"location":"Cpp-API/features/#regex","title":"RegEx","text":"<p>User may want to profiling all the programs running under the process matching a regular expression say \"Resnet50*\". If the user creates a profiling handle with a specific regular expression, any new program created, whose name passes the regEx filter, will automatically start getting profiled. Once the program is released by the user, it automatically is also removed from the profiling handle's list of programs.</p> <p>Note: Addition/removal of program can lead to a report generation getting delayed or preponed. Note: RegEx engine used is ECMAScript.</p> <p>Refer to section <code>StreamProfilingHandle</code>_ for HPP API interface.</p>"},{"location":"Cpp-API/features/#device-partitioning-tool","title":"Device Partitioning Tool","text":"<p>Device partitioning is a virtualization technique for Qualcomm AIC 100 card that creates a Shadow device with discrete set of resources on the native device. The Shadow Device is tied to a \"Resource Group Reservation\" made by the application and is presented with Device id(QID) greater than 100 [Eg: QID 100]. The Shadow Device is allowed to use only the pre-reserved set of resources tied to its reservation and is non-persistent in nature.</p> <p>Either of the below two scenarios will destroy the device and release the resources associated with it.   a. The device would be destroyed when the Resource Group Reservation is      released by application either explicitly or when the application dies.   b. A hard reset is performed on the device.</p> <p>Below listed are the set of resources that are available for reservation under \"Resource Group\"   - Neural Signal Processors(NSP)   - Multicast IDs (MCID)   - Virtual Channels for DMA data   - Device Semaphores   - Device Memory (DDR, L2)</p> <p>qaic-dev-partition, is an application tool to create the Shadow Device(s) with the specified set of device resources.</p> <p>Since the device allocation is non-persistent in nature, below use cases are UNSUPPORTED as they may invalidate the QID allocated by an already existing instance of application. 1. Multiple instances of qaic-dev-partition tool. 2. Hotplug a QAic device when there is a active qaic-dev-partition.</p>"},{"location":"Cpp-API/features/#usage","title":"Usage","text":"<pre><code>/opt/qti-aic/tools/qaic-dev-partition -h\n</code></pre>"},{"location":"Cpp-API/features/#create-and-test-the-shadow-devices","title":"Create and Test the Shadow Device(s)","text":""},{"location":"Cpp-API/features/#1-create-the-shadow-device","title":"1. Create the Shadow device","text":"<p>Launch the qaic-dev-partition utility as a daemon, either in foreground or background to keep    the Shadow device alive.</p> <p>To request resource reservation using the QPC file on Device-id 2</p> <pre><code>  sudo qaic-dev-partition -q &lt;path to qpc&gt;/programqpc.bin -d 2\n</code></pre> <p>To request resource reservation using json config file:</p> <pre><code>  sudo qaic-dev-partition -p &lt;path_to_json_config&gt;\n</code></pre> <p>Multiple reservations can be created through json configuration. The first resource set is allotted QID 100,    and QID is incremented by 1 for every resource set listed thereafter.</p>"},{"location":"Cpp-API/features/#2-verify-the-derived-device-creation","title":"2. Verify the Derived device creation:","text":"<p>Run qaic-util tool to list the newly generated device. Typically the    Shadow devices have QID &gt;= 100. Note that PCI fields are invalid with dummy values as it is an    emulated device, and PCI Hardware attributes are not applicable.    Example: <code>sudo qaic-util -q -d 101</code></p>"},{"location":"Cpp-API/features/#3-destroy-the-device","title":"3. Destroy the device.","text":"<p>On killing the utility the Shadow device should disappear and one may verify that the resources are added    back to the native device's resource pool.</p> <p>Example configuration files are present in <code>/opt/qti-aic/test-data/json/qaic-dev-partition</code></p>"},{"location":"Cpp-API/features/#partition-device-configuration","title":"Partition Device Configuration","text":"<p>Alias names: Partition Device/ Shadow Device/Derived Device.</p> <p>Documentation uses the term \"Derived Device\". The starting Device ID for Derived Device(s) is QID 100. Multiple Shadow/Derived devices can be created on the same QAIC device. Each shadow device would have unique QID and Dev link assigned as shown below.</p> <p>Example:</p> <pre><code>Parent device:  QID 17\n                Dev Link: /dev/qaic_aic100_15\n\nShadow devices for QID 17\n QID 112 :\n           Dev Link : /dev/qaic_aic100_31\n     Parent Dev Link: /dev/qaic_aic100_15\n\n QID 113 :\n           Dev Link : /dev/qaic_aic100_33\n     Parent Dev Link: /dev/qaic_aic100_15\n</code></pre>"},{"location":"Cpp-API/runtime/","title":"Runtime","text":"<p>The following document describes Qualcomm AIC 100 User space Linux Runtime classes design and implementation.</p>"},{"location":"Cpp-API/runtime/#qpc-elements","title":"QPC Elements","text":""},{"location":"Cpp-API/runtime/#qpc","title":"QPC","text":"<p>Class <code>Qpc</code> is a main QPC API class type that provides functionality to allocate a QPC from a filename or buffer, and also provides API to query information related to the loaded <code>QPC</code>.</p> <p>Class <code>Qpc</code> has 2 Factory functions to create a <code>std::shared_ptr&lt;&gt;</code> of <code>Qpc</code>. There are couple of ways to create <code>QPC</code> object.</p> <p>The class is non-copyable, nor movable.</p> <ol> <li>Creating from buffer and size.</li> <li>Creating from a given filename (base-path plus filename).</li> </ol> <p>If <code>Factory</code> instance creation is successful, the functions will return an instance of <code>std::shared_ptr&lt;&gt;</code>, otherwise a proper exception will be thrown.</p> <p>Important API in class type <code>Qpc</code> includes the following:</p> <ol> <li><code>getInfo()</code> - returns <code>QpcInfo</code>. See below for more info.</li> <li><code>getBufferMappings()</code> - returns a container of <code>BufferMappings</code>. Each <code>BufferMapping</code> instance in the container includes information on the buffers as obtained from <code>Qpc</code> file. Name, size, direction and index of buffers.</li> <li><code>getBufferMappingsDma</code> - Same return type as above (<code>getBufferMapping()</code>) but for DMA allocated buffers instead of user's heap allocated ones.</li> <li><code>getIoDescriptor()</code> - returns pointer to QData which is a buffer and length of the QPC buffer.</li> <li><code>get()</code> - returns a const pointer to QAicQpcObj - which is an opaque Data structure to Internal Program Container. Users have no visibility to the API of QAicQpcObj since it is Opaque in the C layer.</li> <li><code>buffer()</code> - returns the QPC buffer as a const pointer to uint8_t.</li> <li><code>size()</code> - returns the size of the QPC buffer.</li> </ol>"},{"location":"Cpp-API/runtime/#qpcfile","title":"QpcFile","text":"<p>Class <code>QpcFile</code> is encapsulating the QPC file basepath and filename as well as <code>DataBuffer&lt;QData&gt;</code> data member which holds the buffer of the QPC file. <code>QpcFile</code> takes the base-path and the filename of the QPC, where <code>programqpc.bin</code> is a default filename provided in the constructor. <code>QpcFile</code> is a non-movable, non-copyable type.</p> <p>It has a <code>load()</code> function that will load the content of the QPC into  <code>DataBuffer&lt;&gt;</code> which holds <code>QData</code> buffer representation internally.</p> <p>There are few APIs provided by the <code>QpcFile</code> class type.</p> <ol> <li><code>getBuffer()</code> - returns a <code>QData</code> buffer const reference.</li> <li><code>data()</code> - returns a <code>const uint8_t</code> pointer to buffer.</li> <li><code>size()</code> - returns the size of the loaded QPC file buffer.</li> </ol>"},{"location":"Cpp-API/runtime/#qpcinfo","title":"QpcInfo","text":"<p>Struct <code>QpcInfo</code> is a simple struct type that aggregates a collection of <code>QpcProgramInfo</code> (also referred to as \"program\") and corresponding collection of <code>QpcConstantsInfo</code> (also referred to as \"constants\").</p>"},{"location":"Cpp-API/runtime/#qpcprograminfo","title":"QpcProgramInfo","text":"<p>Struct <code>QpcProgramInfo</code> is a simple struct aggregating information related to the content of the loaded QPC.</p> <p>For example:</p> <ol> <li>BufferMappings, user allocated or DMA.</li> <li>Name identifying a segment in <code>QPC</code> file.</li> <li>Number of cores requires to run the program.</li> <li>Program Index in the QPC.</li> <li>Size of the program</li> <li>Batch size.</li> <li>Number of Semaphores, number of MC(MultiCast) IDs.</li> <li>Total required memory to run the program.</li> </ol>"},{"location":"Cpp-API/runtime/#qpcconstantsinfo","title":"QpcConstantsInfo","text":"<p>Struct <code>QpcConstantsInfo</code> defines the Constants info that are obtained from the QPC file. It has the following attributes:</p> <ol> <li><code>name</code></li> <li><code>index</code></li> <li><code>size</code></li> </ol>"},{"location":"Cpp-API/runtime/#buffermappings","title":"BufferMappings","text":"<p>Vector <code>BufferMappings</code> is a vector of <code>BufferMapping</code>.</p> <p><code>BufferMappings</code> is created from QPC.</p> <p><code>BufferMappings</code> is used by API to store the Input/Output buffer information that is also used to create <code>inferenceVector</code>.</p>"},{"location":"Cpp-API/runtime/#buffermapping","title":"BufferMapping","text":"<p>Struct <code>BufferMapping</code> is a simple struct type that describes the information of a buffer.</p> <p>Struct <code>BufferMapping</code> has two constructors:</p> <ol> <li>Creating by providing all of the data members.</li> <li>Default constructor that creates an uninitialized <code>BufferMapping</code> instance.</li> </ol> <p>Struct <code>BufferMapping</code> has following structure data members:</p> <ol> <li><code>bufferName</code> - string name identifying the buffer.</li> <li><code>index</code> - an unsigned int that represent the index in an array of buffers.</li> <li><code>ioType</code> - define the direction of a buffer from user's perspective. An input buffer is from user to device. An output buffer is from device to user.</li> <li><code>size</code> - buffer size in bytes.</li> <li><code>isPartialBufferAllowed</code> - <code>Partial buffer</code> is a feature that allows buffer to have actual size that is smaller than what is specified in IO descriptor. <code>isPartialBufferAllowed</code> is set by IO descriptor. By setting <code>isPartialBufferAllowed</code> true, this buffer takes user buffer that is smaller than what is specified by <code>size</code>.</li> <li><code>dataType</code> - define the format of buffer. The types of format is defined in struct <code>QAicBufferDataTypeEnum</code></li> </ol>"},{"location":"Cpp-API/runtime/#qaicbufferdatatypeenum","title":"QAicBufferDataTypeEnum","text":"<p>Struct <code>QAicBufferDataTypeEnum</code> is a simple struct type that defines the data type of the <code>BufferMapping</code>.</p> <p>Struct <code>QAicBufferDataTypeEnum</code> defines following data types:</p> <ol> <li><code>BUFFER_DATA_TYPE_FLOAT</code> - 32-bit float type (float)</li> <li><code>BUFFER_DATA_TYPE_FLOAT16</code> - 16-bit float type (half, fp16)</li> <li><code>BUFFER_DATA_TYPE_INT8Q</code> - 8-bit quantized type (int8_t)</li> <li><code>BUFFER_DATA_TYPE_UINT8Q</code> - unsigned 8-bit quantized type (uint8_t)</li> <li><code>BUFFER_DATA_TYPE_INT16Q</code> - 16-bit quantized type (int16_t)</li> <li><code>BUFFER_DATA_TYPE_INT32Q</code> - 32-bit quantized type (int32_t)</li> <li><code>BUFFER_DATA_TYPE_INT32I</code> - 32-bit index type (int32_t)</li> <li><code>BUFFER_DATA_TYPE_INT64I</code> - 64-bit index type (int64_t)</li> <li><code>BUFFER_DATA_TYPE_INT8</code> - 8-bit type (int8_t)</li> <li><code>BUFFER_DATA_TYPE_INVAL</code> - invalid type</li> </ol>"},{"location":"Cpp-API/runtime/#context-elements","title":"Context Elements","text":""},{"location":"Cpp-API/runtime/#context","title":"Context","text":"<p>There are various Linux Runtime core components like <code>qpc</code>, <code>program</code>, <code>execObj</code>, and <code>queue</code> etc. which are needed to run inference and enhance performance/usability. Class <code>Context</code> is a primary class which helps to link all LRT core components. <code>Context</code> object should be created first. Application creates a context to obtain access to other API functions, the context is passed in other API calls. The caller can also register for logging and error callbacks. A context ID is passed to the error handler to uniquely identify the <code>Context</code> object.</p> <p>Class <code>Context</code> has a Factory functions to create a <code>std::shared_ptr&lt;&gt;</code> of <code>Context</code>.</p> <p>Context object is created from context properties, list of devices used by this context, logging callback function, specific user data to be included in log callback, an error handler to call in case of critical errors and specific user data to be included in error handler callback. If logging callback and error handler are not provided then default <code>defaultLogger</code> and <code>defaultErrorHandler</code> will be used.</p> <p>If <code>Factory</code> instance creation is successful, the functions will return an instance of <code>std::shared_ptr&lt;&gt;</code>, otherwise a proper exception will be thrown.</p> <p>Important API in class type <code>Context</code> includes the following:</p> <ol> <li><code>findDevice()</code> - returns a suitable device for the network and check selected device has enough resources.</li> <li><code>setLogLevel()</code> - set new logging level to get logging information while running the program. See below for more details about <code>QLogLevel</code>.</li> <li><code>getLogLevel()</code> - returns current logging level for given <code>Context</code>.</li> <li><code>get()</code> -  returns a const pointer to <code>QAicContext</code>. Users have no visibility to the API of <code>QAicContext</code> since it is Opaque in the C layer.</li> <li><code>getId()</code> - returns an unsigned int that represent id of the <code>Context</code>. This id will be returned in error reports to refer a specific created context.</li> <li><code>objName()</code> - returns <code>const std::string</code> which is name of the object. For context object name is <code>Context</code>.</li> <li><code>objNameCstr()</code>- returns a pointer to an array that contains a null-terminated sequence of char representing the name of an object.</li> </ol>"},{"location":"Cpp-API/runtime/#qloglevel","title":"QLogLevel","text":"<p>There are diffrent type of logging level to see different kind of logs.</p> <ol> <li><code>QL_DEBUG</code> : set to this level to see debug logs</li> <li><code>QL_INFO</code> : set to this level to see informative logs</li> <li><code>QL_WARN</code> : set to this level to see warning logs</li> <li><code>QL_ERROR</code> : set to this level to see error logs</li> </ol> <p><code>LogCallback</code> - It is a logging callback lambda function.</p> <p><code>ErrorHandler</code> - It is an error handler lambda function to call in case of critical errors.</p>"},{"location":"Cpp-API/runtime/#device-log-capture","title":"Device Log capture","text":"<p>When a <code>Context</code> is created with QAIC_CONTEXT_COLLECT_DEVICE_LOGS property bitmask set, it configures devices to start streaming logs to host. Logs are passed to user application using QAicLogCallback, which is registered while creating context. There is time delay between actual log event in device and host receiving them. Due to this Host logs are buffered for few seconds and dumped chronologically along with device logs.</p> <p>Context object keeps a track of all programs that are associated with it. Logs from all devices and NSPs, used within a context object, are captured. It is done in a separate thread, and will not impact the data path flow.</p>"},{"location":"Cpp-API/runtime/#prerequisite","title":"Prerequisite","text":"<p>qaic-monitor-grpc-server should be running in background. User can use below commands.</p> <ul> <li>systemd-run --unit=qmonitor-proxy /opt/qti-aic/tools/qaic-monitor-grpc-server  # starts in background</li> <li>systemctl stop qmonitor-proxy  # stops background service</li> </ul>"},{"location":"Cpp-API/runtime/#limitation","title":"Limitation","text":"<p>QSM logs from different inference sessions cannot be filtered. If multiple inference sessions are running in a device, QSM Logs captured by a context will have logs from all of them.</p>"},{"location":"Cpp-API/runtime/#profiling-elements","title":"Profiling Elements","text":"<p>For overview of profiling feature refer to Profiling Support in Runtime.</p>"},{"location":"Cpp-API/runtime/#profilinghandle","title":"ProfilingHandle","text":"<p><code>ProfilingHandle</code> provides interface to use num-iter based profiling. Refer to Num-iter based profiling for more details on num-iter based profiling feature.</p> <p>A <code>ProfilingHandle</code> object should be created using the <code>Factory</code> method. User needs to specify the <code>Program</code> that should be profiled, number of samples to collect, callback to call to deliver report, and type of profiling output expected.</p> Note <p>Profiling type parameter has a default value set to Latency type.</p> <p>Important API in class type <code>ProfilingHandle</code> includes the following:</p> <ol> <li><code>start()</code> - Start profiling. After the API call, profiling data from all the inferences for specified <code>Program</code> will be collected till either user calls <code>stop()</code> or number of requested samples have been collected.</li> <li><code>stop()</code> Stop profiling. Stops profiling even if the num-samples requirement has not been met. This API calls triggers a callback to the user specified callback with profiling report of all collected samples.</li> </ol> Note <p>If <code>stop()</code> is called without any inferences being complete for the specified <code>Program</code>, callback will not get triggered.</p>"},{"location":"Cpp-API/runtime/#streamprofilinghandle","title":"StreamProfilingHandle","text":"<p>ProfilingHandle provides interface to use duration based profiling. Refer to Duration based profiling for more details on duration based profiling feature.</p> <p>A <code>StreamProfilingHandle</code> object should be created using the <code>Factory</code> method. User needs to specify the sampling rate, reporting rate, callback to call to deliver report and profiling output format expected. User may optionally specify a name for the handle, and regEx to auto add/remove programs.</p> Note <p>Profiling type is specified using ProfilingProperties field. ProfilingProperties has a default param nullptr which results in profiling type to be Latency type.</p> <p>Important API in class type <code>StreamProfilingHandle</code> includes the following:</p> <ol> <li> <p><code>start()</code> - Start profiling. After the API call, user should expect a callback at every reporting rate boundary containing information of profiling inferences during that duration.</p> Note <p>User will get callback even if there are no samples collected.</p> </li> <li> <p><code>stop()</code> Stop profiling. A final report will be delivered to user when the profiling is stopped with the profiling data of samples collected from last report till the point <code>stop()</code> is called.</p> </li> <li> <p><code>addProgram()</code> - Add a program to list of program being profiled.</p> Note <p>Adding program when profiling is active can cause spurious report callback or a delayed report callback.</p> </li> <li> <p><code>removeProgram()</code> - Remove a program from list of program being profiled.</p> Note <p>Removing program when profiling is active can cause spurious report callback or a delayed report callback.</p> </li> <li> <p><code>flushReports()</code> - After profiling is stopped using <code>stop()</code> API, user should make sure that all the reports on the queue of the profiling infrastructure are delivered to user as callback before application exit. <code>flushReports()</code> API only returns after there are no more reports left to be delivered to the user, thus ensuring a clean application exit.</p> </li> </ol>"},{"location":"Cpp-API/runtime/#inferencing-elements","title":"Inferencing Elements","text":""},{"location":"Cpp-API/runtime/#qbuffer","title":"QBuffer","text":"<p><code>QBuffer</code> is a struct that contains pointer to the buffer and its size. It can have Input or output buffer address from heap or DMA memory. <code>handle</code>, <code>offset</code> and <code>type</code> are considered only when type is QBUFFER_TYPE_DMABUF or QBUFFER_TYPE_PMEM. It has following Members:</p> <ol> <li><code>size</code> - Total size of memory pointed by buf pointer or handle.</li> <li><code>buf</code> - Buffer Pointer, must be valid in case of heap buffer.</li> <li><code>handle</code> - Buffer Handle, must be valid in case of DMA (or PMEM) buffer.</li> <li><code>offset</code> - Offset within handle.</li> <li><code>type</code> - Type of the buffer heap, DMA or PMEM.</li> </ol>"},{"location":"Cpp-API/runtime/#inferencevector","title":"InferenceVector","text":"<p><code>InferenceVector</code> contains a vector of <code>QBuffer</code>. Vector of <code>QBuffer</code> is a vector containing both input and output buffers. User can create <code>InferenceVector</code> from multiple sources like files from disk or create <code>QBuffer</code> with data available with the user and set them in <code>InferenceVector</code> with <code>setBuffers()</code> API of this class. The input buffer will be used for inference and result of inference will be stored in output buffer. User needs to keep reference of InferenceVector until inference is complete and user can read output buffers from inference vector after completion of inference.</p> <p><code>InferenceVector</code> APIs</p> <ul> <li><code>getVector()</code>: Returns a vector of <code>QBuffer</code>. Vector contains both input and output buffers. <code>QBuffer</code> is a struct that contains pointer to the buffer and its size.                 User can call this after the inference is completed to read output buffers.</li> <li><code>setBuffers()</code>: Sets input and output buffers of this <code>InferenceVector</code></li> <li><code>Factory()</code>: Instantiates InferenceVector</li> </ul>"},{"location":"Cpp-API/runtime/#inferencehandle","title":"InferenceHandle","text":"<p><code>InferenceHandle</code> contains <code>InferenceVector</code> and id given at the time of submission of inference. <code>InferenceHandle</code> cannot be created directly by the user, user can get an available <code>InferenceHandle</code> by calling <code>getAvailable()</code> API of <code>InferenceSet</code>. <code>InferenceHandle</code> is a container that has data needed for inference stored in <code>InferenceVector</code>. Number of <code>InferenceHandle</code> objects created depends on the set_size and num_activations parameters passed during instantiation of <code>InferenceSet</code>. Number of <code>InferenceHandle</code> and number of <code>ExecObj</code> created will be the same.</p>"},{"location":"Cpp-API/runtime/#lifecycle-of-inferencehandle","title":"LifeCycle of <code>InferenceHandle</code>","text":"<ul> <li><code>InferenceHandle</code> objects are created when <code>InferenceSet</code> is instantiated and all objects are moved to availableList vector from which user can retrieve it by calling <code>getAvailable()</code> API of <code>InferenceSet</code></li> <li>When user calls <code>getAvailable()</code> if availableList vector has an <code>InferenceHandle</code>, it is popped out from the availableList and returned to user, otherwise this call is blocked until the user puts the used <code>InferenceHandle</code> using <code>putCompleted()</code> API</li> <li>User sets buffers in the InferenceHandle it got using <code>setBuffers()</code> API</li> <li>User submits InferenceHandle using <code>submit()</code> API of <code>InferenceSet</code></li> <li>To get the completed <code>InferenceHandle</code> user can call <code>getCompleted()</code> or <code>getCompletedId()</code> and extract/read the output of inference from <code>InferenceHandle</code></li> <li>After processing the output of inference, user needs to call <code>putCompleted()</code> API of <code>InferenceSet</code> to put completed <code>InferenceHandle</code> back to availableList vector otherwise <code>getAvailable()</code> call will be blocked</li> </ul>"},{"location":"Cpp-API/runtime/#inferenceset","title":"InferenceSet","text":"<p><code>InferenceSet</code> is a C++ class that is used to submit inference. It abstracts out lower level classes like Queue, Program and ExecObj and provides an easier way of handling multiple activations in a single group to submit inference.</p> <p>List of APIs of <code>InferenceSet</code></p> <ul> <li><code>submit()</code>: Submit an inference through <code>InferenceVector</code>. The submission will be blocked until an <code>InferenceHandle</code> is available</li> <li><code>submit()</code>: Submit an inference through <code>InferenceHandle</code></li> <li><code>getCompleted()</code>: Returns a completed <code>InferenceHandle</code> object. User can access output of the inference using this <code>InferenceHandle</code> object by calling <code>getBuffers()</code> method</li> <li><code>getCompletedId()</code>: Returns a completed <code>InferenceHandle</code> object with specified ID</li> <li><code>putCompleted()</code>: Move a completed InferenceHandle back into the availableList vector</li> <li><code>getAvailable()</code>: Returns an available <code>InferenceHandle</code> object from availableList</li> <li><code>waitForCompletion()</code>: Wait for all inferences submitted to be completed on all activations</li> <li><code>Factory()</code>: Instantiates InferenceSet</li> </ul>"},{"location":"Cpp-API/runtime/#numactivations-and-setsize","title":"NumActivations and SetSize","text":"<p><code>NumActivations</code> and <code>SetSize</code> are arguments of <code>InferenceSet::Factory</code> API.</p> <ul> <li><code>NumActivations</code>: <code>InferenceSet</code> creates this many numbers of network instances inside device. User can decide <code>NumActivations</code> based on number of cores required to run his network and number of available cores.</li> <li><code>SetSize</code>: For each network instance, user application can simultaneously enqueue this many numbers of input/output buffers to run inferences. Recommended value is between 2 to 10. User should find an optimal value to achieve desired throughput (inferences/sec) and latency.</li> </ul> <p> </p> Activations and SetSize"},{"location":"Cpp-API/runtime/#inference-flow","title":"Inference Flow","text":"<p>Inference flow using <code>InferenceSet</code> would be as follows:</p> <ol> <li>Instantiate <code>InferenceSet</code> using the Factory method</li> <li>Acquire one of the available <code>InferenceHandle</code>.</li> <li>Set Input and Output buffers in that <code>InferenceHandle</code>.</li> <li>Submit <code>InferenceHandle</code> to <code>InferenceSet</code> to run inference in device.</li> <li>Call <code>getCompletedId</code> API to wait for Inference to complete. Inference results are available in Output buffers, once this API returns.</li> <li>Once application reads output data, <code>putCompleted</code> must be called to return the <code>InferenceHandle</code> back to available List of handles.</li> </ol> <p> </p> Inference Flow"},{"location":"Cpp-API/runtime/#inferencesetproperties","title":"InferenceSetProperties","text":"<p><code>InferenceSetProperties</code> defines properties to be consumed by <code>InferenceSet</code></p> <p>List of members of <code>InferenceSetProperties</code></p> <ul> <li><code>programProperties</code>: User can set different program properties which will be consumed internally by <code>Program</code> object. Notable programProperties are:<ul> <li><code>SubmitRetryTimeoutMs</code>: After submission of inference, runtime waits for this milliseconds timeout period, if inference is not complete in this timeout period, error is returned</li> <li><code>SubmitNumRetries</code>: Number of times submission should be retried when the above timeout occurs</li> <li><code>devMapping</code>: devMapping specifies the physical devices to be used by the program and is valid only for the network's that need multiple devices to run</li> </ul> </li> <li><code>queueProperties</code>: User can set queue properties which will be consumed internally by <code>Queue</code> object. Notable queueProperties are:<ul> <li><code>numThreadsPerQueue</code>: Number of threads spawned to process elements in the queue. Default 4</li> </ul> </li> <li><code>name</code>: Defines name of the InferenceSet Object</li> <li><code>id</code>: Defines id of the InferenceSet Object</li> </ul>"},{"location":"FAQ/","title":"Frequently Asked Questions","text":""},{"location":"FAQ/#general","title":"General","text":"What is Cloud AI 100 Accelerator? <p>Cloud AI 100 accelerators enable high performance inference on deep learning models. The accelerators are available in multiple form factors and associated SKUs. Cloud AI SDKs enable end to end workflows - from onboarding a pre-trained model to deployment of the ML inference application in production. </p>"},{"location":"FAQ/#cloud-ai-sdk-installation-and-platformos-support","title":"Cloud AI SDK Installation and Platform/OS Support","text":"What operating systems and platforms are supported? <p>See Operating System and Platform Support</p> Where do I download the SDKs? <p>Cloud AI SDK consists of a Platform and Apps SDK. Refer to Cloud AI SDKs for more information. </p> <p>For Platform SDK download, see Platform SDK Download</p> <p>For Apps SDK download, see Apps SDK Download</p> What environment variables need to be set to resolve toolchain errors such as libQAic.so? <p>Set the environment variables as mentioned here</p>"},{"location":"FAQ/#deep-learning-frameworks-and-networks","title":"Deep Learning frameworks and networks","text":"Which deep learning frameworks for supported by Cloud AI SDKs? <p>Onnx, tensorflow, pytorch, caffe or caffe2 are supported by the compiler.  <code>qaic-exec</code> can dump the operators supported across different frameworks. Onnx has the best operator support. </p> Which deep learning neural networks are supported? <p>Cloud AI platforms supports many network categories - Computer vision, object detection, Semantic segmentation, Natural language processing, ADAS and Generative AI networks.  Performance information can be found in the Qualcomm Cloud AI 100 page  Model recipes can be found in the <code>cloud-ai-sdk</code> github. </p> I have a neural network that I would like to run on Cloud AI platforms. How do I go about it? <p>There are 3 steps run an inference on Cloud AI platforms. </p> <ol> <li>Export the model in ONNX format (preferred due to operator support) and prepare the model </li> <li>Compile the model to generate a QPC (Qaic Program Container)</li> <li>Execute, integrate and deploy into production pipeline</li> </ol> <p>The quick start guide provides a quick overview of the steps involved in running inference using a vision transformer model as an example. </p> <p>Refer to Inference Workflow for detailed information how to onboard and run inference on Cloud AI platforms. </p> <p>Users can also refer to the model recipes that provide the best performance for several networks across several categories. </p> <p>Tutorials are another resource that walks through the workflows to onboard models, tune for best performance, profile inferences etc. </p>"},{"location":"FAQ/#runtime-errors-during-inference","title":"Runtime errors during inference","text":"While running inference I encounter 'IOCTL: Connection timed out ERROR'. What is the fix? <p>There are 3 parameters that we recommend users to increase significantly when this issue is encountererd. If the issue is not fixed, please raise a case through the Qualcomm case support system or <code>Cloud-ai-sdk</code> GitHub.  <pre><code>echo 4000000 &gt; /sys/module/qaic/parameters/wait_exec_default_timeout\necho 4000000 &gt; /sys/module/qaic/parameters/control_resp_timeout\necho 4000000 &gt; /sys/module/qaic/parameters/mhi_timeout\n</code></pre></p>"},{"location":"FAQ/#system-management","title":"System management","text":"Which utility/tool is used to query health, telemetry etc of all Cloud AI cards in the server? <p>Use the qaic-util CLI tool to query health, telemetry etc of Cloud AI cards in the server. </p> The Cloud AI device shows <code>Status:Error</code>. How do i fix it? <p><code>Status: Error</code> could be due to one of the following:</p> <ul> <li>indicates the respective card(s) has not booted up completely </li> <li>user has not used <code>sudo</code> prefix if user has not been added to <code>qaic</code> group. <code>sudo /opt/qti-aic/tools/qaic-util</code></li> <li>unsupported OS/platforms, secure boot etc</li> </ul> <p>Users can try to issue an soc_reset to see if the device recovers. </p>"},{"location":"Getting-Started/","title":"User Guide","text":"<p>Cloud AI SDKs enable developers to optimize trained deep learning models for high-performance inference. The SDKs provide workflows to optimize the models for best performance,  provides runtime for execution and supports integration with ONNX Runtime and Triton Inference Server for deployment.</p> <p>Cloud AI SDKs support</p> <ul> <li>High performance Generative AI, Natural Language Processing, and Computer Vision models</li> <li>Optimizing performance of the models per application requirements (throughput, accuracy and latency) through various quantization techniques</li> <li>Development of inference applications through support for multiple OS and Docker containers.  </li> <li>Deployment of inference applications at scale with support for Triton inference server</li> </ul>"},{"location":"Getting-Started/#cloud-ai-sdks","title":"Cloud AI SDKs","text":"<p>The Cloud AI SDK consists of the Application (Apps) SDK and Platform SDK.</p> <p>The Application (Apps) SDK is used to convert models and prepare runtime binaries for Cloud AI platforms.  It contains model development tools, a sophisticated parallelizing graph compiler, performance and integration tools, and code samples. Apps SDK is supported on x86-64 Linux-based systems.</p> <p>The Platform SDK provides driver support for Cloud AI accelerators, APIs and tools for executing and debugging model binaries, and tools for card health, monitoring and telemetry. Platform SDK consists of a kernel driver, userspace runtime with APIs and language bindings, and card firmware. Platform SDK is supported on x86-64 and ARM64 hosts.  </p> <p> </p>"},{"location":"Getting-Started/#installation","title":"Installation","text":"<p>The installation guide covers</p> <ul> <li>Platforms, operating systems and hypervisors supported and corresponding pre-requisites</li> <li>Cloud AI SDK (Platform and Apps SDK) installation</li> <li>Docker support</li> </ul>"},{"location":"Getting-Started/#inference-workflow","title":"Inference Workflow","text":"<p>Inference workflow details the Cloud AI SDK workflow and tool support - from onboarding a pre-trained model to deployment on Cloud AI platforms. </p>"},{"location":"Getting-Started/#release-notes","title":"Release Notes","text":"<p>Cloud AI release notes provide developers with new features, limitations and modifications in the Platform and Apps SDKs.</p>"},{"location":"Getting-Started/#sdk-tools","title":"SDK Tools","text":"<p>SDK Tools provides details on usage of the tools in the SDKs used in the Inference workflow as well as card management. </p>"},{"location":"Getting-Started/#tutorials","title":"Tutorials","text":"<p>Tutorials, in the form of Jupyter Notebooks walk the developer through the Cloud AI inference workflow as well as the tools used in the process. Tutorials are divided into CV and NLP to provide a better developer experience even though the inference workflows are quite similar. </p>"},{"location":"Getting-Started/#model-recipes","title":"Model Recipes","text":"<p>Model recipes provide the developer the most performant and efficient way to run some of the popular models across categories. The recipe starts with the public model. The model is then exported to ONNX, some patches are applied if required, compiled and executed for best performance. Developers can use the recipe to integrate the compiled binary into their inference application.   </p>"},{"location":"Getting-Started/#sample-code","title":"Sample Code","text":"<p>Sample code helps developers get familiar with the usage of Python and C++ APIs for inferencing on Cloud AI platforms. </p>"},{"location":"Getting-Started/#system-management","title":"System Management","text":"<p>System Management details management for Cloud AI Platforms. </p>"},{"location":"Getting-Started/#architecture","title":"Architecture","text":"<p>Architecture provides insights into the architecture of Cloud AI SoC and AI compute cores. </p>"},{"location":"Getting-Started/Architecture/","title":"Architecture","text":""},{"location":"Getting-Started/Architecture/#cloud-ai-platforms","title":"Cloud AI Platforms","text":"<p>Cloud AI Platforms/cards contain one or more Cloud AI 100 SoCs per card and are designed to operate at a specific TDP. The cards interface to the host via PCIe and to the BMC via SMBus/I2C. The table below provides the key hardware specs across the available SKUs. </p> <p>The Cloud AI 100 Ultra SKU has 4 AI 100 SoCs and a PCIe switch on the card. The interface to the host is through the PCIe switch via a PCIe gen4 x16 interface. The PCIe switch connects to the 4 AI 100 SoCs via a PCIe Gen4 x 8 interface.      </p> <p>The Cloud AI 100 Standard and Pro SKUs have 1 AI 100 SoC on the card interfacing to the host via a PCIe Gen4 x 8 interface. </p> Accelerator SKU AI 100 Standard AI 100 Pro AI 100 Ultra AI 100 SoC Count 1 1 4 DDR Capacity 16 GB 32 GB 128 GB DDR BW 137 GB/s 137 GB/s 548 GB/s On-chip SRAM 126 MB 144 MB 576 MB FP16 110 TFLOPs 125 TFLOPs 290 TFLOPs Int8 325 TOPs 375 TOPs 870 TOPs Connection to host via PCIe Gen4 x8 Gen4 x8 Gen4 x16 TDP 75 W 75 W 150 W Formfactor PCIe HHHL PCIe HHHL PCIe FH3/4L"},{"location":"Getting-Started/Architecture/#cloud-ai-100-soc","title":"Cloud AI 100 SoC","text":"<p>Cloud AI100 SOC\u2019s multi-core architecture, shown in figure below, is purpose-built for deep learning inference in the Cloud.  </p> <p>The SOC is composed of 16 seventh-generation AI cores delivering 400+ Int8 TOPs and 200+ FP16 TOPs of compute performance, with 144 MB of on-chip memory for data storage. </p> <p>The on-chip memory sub-system is connected to an external LPDDR4X memory subsystem, comprising of 4 channels of 64b width(4 x 64b), providing 136 GB/s of memory bandwidth and up to 32 GB of memory capacity. </p> <p>The SoC also provides 8 lanes of PCIe Gen4 IO interfaces (PCIe-gen4 x 8) to connect to the host CPU complex and other peripherals. </p> <p>The AI cores and all internal subsystems are connected by three advanced NoCs that deliver 186 GB/s of data bandwidth and support multicast and AI core synchronization. The Compute NoC connects the AI cores and PCIe, the Memory NoC connects the AI cores to the DDR memory, and the Configuration NoC is used for boot and hardware configuration. </p> <p>The SoC is also equipped with a sophisticated power management system, optimizing both transient and peak power consumption. The SOC also implements thermal detection and control. </p> <p>The SOC implements several cloud-readiness security features, including ECC, secure boot and DDR memory zero-out on reset.</p>"},{"location":"Getting-Started/Architecture/#ai-core","title":"AI Core","text":"<p>The seventh-generation AI Core, leveraging over a decade of Qualcomm AI research, is shown in Figure below. The AI Core implements the architecture principle of separation-of-concerns, with three types of compute units for tensor, vector and scalar operations. </p> <p> </p> <p>The tensor unit implements two 2D MAC arrays\u20148K for 8b integer, and 4K for 16b floating point, with 125+ instructions conducive for linear algebra, providing throughput of 8192/4096 MAC operations per clock for 8/16b integer/floating point operations respectively.  </p> <p>The vector unit implements over 700+ rich instructions for AI, content verification, and image processing supporting 8/16b integer and 16/32b precision floating point, providing throughput of 512/256 MAC operations per clock for 8/16b integer/floating point operations respectively. </p> <p>The scalar processor is a 4-way VLIW (very large instruction word) machine supporting six hardware threads, each with a local scalar register file, instruction and data caches, and support for 8/16b integer and 16/32b floating point operations\u2014a rich instruction set of over 1800 instructions that provide flexibility in compute operations. </p> <p>An 8 MB Vector Tightly Coupled Memory (VTCM) provides scratch-pad data storage for both the vector unit as well as the tensor unit. The 1 MB L2 cache in the core is shared by all three compute units (scalar, vector, and tensor), and it implements hardware for the intelligent prefetching of data and for advanced DMA operations. </p>"},{"location":"Getting-Started/Features/","title":"Features","text":"<p>This section provides insights into the Cloud AI SDK capabilities to enable high-performance inference and improve development/deployment of deep learning inference on Cloud AI accelerators. </p> <ul> <li>Data Types</li> <li>Custom Operators</li> <li>Model Sharding </li> </ul>"},{"location":"Getting-Started/Features/custom_ops/","title":"Custom Operations (C++)","text":"<p>Custom operation allows users to add an operation into the graph, which is unknown to Cloud AI (aka QAic) Compiler. For example, a certain graph is not part of the operation set supported for the various models that the compiler knows to load and compile.  As will be described later in the page, with custom operations, users can register a new operation prior to loading/defining the graph, and then use this new operation in the new graph created. QAic Compiler will use the registration information to create a Graph which can be compiled.</p> <p></p> <p>As an example, ONNX node definition supports Domains. A user can create an ONNX model file which has nodes that come from the non-default domain, register these nodes definition in QAic Compiler.</p>"},{"location":"Getting-Started/Features/custom_ops/#custom-operations-workflow","title":"Custom Operations workflow","text":"<p>There are three high-level stages to create and work with a custom operation.</p> <p></p> <p>The first stage is to create the custom operation in one of the machine learning frameworks (such as pyTorch, TensorFlow, Caffe2, and so on). After creating this operation, it is added to a machine learning model and the model is trained. The resulted model trained is stored for inference - either as ONNX file, Torchscript, TensorFlow savedmodel, or else. For each framework/trained-model, the way to specify a custom operation is a bit different, and as such the way QAic Compiler knows to identify the custom operation varies. Current version supports Custom Operations for ONNX Models. More details on it below.</p> <p>The second stage is to prepare a package for the custom operation/s, which contains:</p> <ul> <li>The information needed for QAic Compiler to integrate the operations in the ML graph.</li> <li>Implementations to be used when compiling the graph to a binary. The content of this stage is explained in details on next sections.</li> </ul> <p>The third stage is compilation. In order to compile a model that has the custom operation inside, the operation package needs to be registered first. After successful registration, the model can be loaded and compiled using qaic-exec.</p>"},{"location":"Getting-Started/Features/custom_ops/#custom-operation-package","title":"Custom Operation package","text":"<p>The custom operations package is prepared by the developer, and contains all files needed to register operations within the compiler. Custom operations package is tagged with a version number to ensure that users custom op package is compatible with the installed Apps SDK.</p> <p>As described in previous section, the developer created a custom operation in a machine learning framework, and used it to train a model, and export a model with this custom op. The files in the custom operations package are used by the QAic Compiler to compile that model into a binary.</p> <p>The drawing below shows the content of a package, and the next sections describe each component in details.</p> <p></p> <p>Briefly:</p> <ul> <li> <p>The configuration file describes the version number, custom operations and related files, and is used by the QAic Compiler to bootstrap the package, registering its content within the compiler.</p> </li> <li> <p>Custom Op Function files (<code>Foo1OpFunc.so</code>, <code>Foo2OpFunc.so</code>) contain utility functions used by the compiler during compilation of the model.</p> </li> <li> <p>Implementation files contain the implementations of the operations for different compilation targets. Also, per operation it is possible to provide multiple implementations (selected by the <code>selection</code> function - see below).</p> </li> </ul> <p>Although the custom op package can be written as described in the next sections, it is recommended to use the utility script provided here - /opt/qti-aic/tools/custom-op/gen_custom_op_package.py to generate skeleton structure of the above described custom op package. Script takes in a ONNX or Tensorflow model and generates package for each custom operation present in the input model. For more details run:</p> <pre><code>python3 /opt/qti-aic/tools/custom-op/gen_custom_op_package.py --help\n</code></pre> <p>After the skeleton code is generated for custom ops, user needs to write the necesary functionality that is specific to custom op, like the the kernel, verification, selection, shape inference function as described in the next sections.</p>"},{"location":"Getting-Started/Features/custom_ops/#modeling-a-custom-operation","title":"Modeling a custom operation","text":"<p>To better understand the content of the package, this section briefly discusses what is needed to model any machine learning operation, so that it can be placed in a graph and compiled. The folloiwng drawing shows modeling of one operation.</p> <p></p> <p>Operation Info block represents the modeling of the operation itself as a black box which needs to be connected inside a graph. In order to do that, need to provide basic information on the operation (name, type), as well as describing the inputs, outputs and parameters of the operation.</p> <p></p> <p>Any operation in a machine learning graph (not just custom operations) can be modeled by defining its set of inputs, outputs and parameters. The inputs and outputs are tensors. Inputs come from the operations before this one, and outputs are the tensors created by the operation, and provided to the operation/s that come afterwards. For the inputs and outputs, the developer needs to specify the maximal rank supported by the operation. The actual values per dimension will be populated by the compiler during graph creation Parameters are the constant values provided to the operation as part of the configuration: for example, kernel size, alpha value, and so on. Parameters can be scalar (single value) or a vector of values.</p> <p>In addition to the Operation Info, the modeling of the operation includes a collection of utility functions and implementations.</p> <p>Utility functions help the compiler during the compilation. The developer is required to implement the following functions.</p> <ul> <li> <p>Verification function: receives a set of inputs, outputs and parameters, populated with actual values that the operation is going to work on. The function should validate that the values are correct and the operation can support this combination.</p> </li> <li> <p>Shape inference function: receives a set of inputs and parameters, populated with actual values that the operation is going to work on. The function is doing shape inference of the outputs: for example, it returns what will be the tensor size on each dimension.</p> </li> <li> <p>Selection function: receives a set of inputs, outputs and parameters, populated with actual values that the operation is going to work on. In addition it receives the compilation target (AIC or Interpreter). The function returns a string containing the name of the adequate implementation to use for this configuration. More details on that below.</p> </li> </ul>"},{"location":"Getting-Started/Features/custom_ops/#configuration-file","title":"Configuration File","text":"<p>The configuration file is part of the custom operation package provided by the developer. There is a single configuration file which provides the required information for all operations in package. The file is YAML formatted. Below is an example of a configuration file for a single operation.</p> <p></p> <p>Configuration file contains a <code>version</code> number which indicates the custom op package version. It is used by the compiler to verify compatibility of custom op pakage and the installed Apps SDK.</p> <p>Operations are described in <code>CustomOps</code> section. Each operation section has the following:</p> <ul> <li> <p>Operation Info, defining the operation <code>name</code>, <code>type</code>, <code>inputs</code>, <code>outputs</code> and <code>params</code>.</p> </li> <li> <p>For each input / output, the relevant fields are the <code>name</code> and <code>maxDims</code> (maximal rank of the tensor)</p> </li> <li> <p>For input, there is also optional constant field, which can be true/false, to indicate if it is a constant.</p> </li> <li> <p>For each param, the relevant fields are <code>name</code>, <code>datatype</code> (bool/float/int), <code>scalar</code> (true/false).</p> </li> <li> <p>If param <code>scalar</code> field is <code>false</code>, meaning this is an array (1d vector), and then need to provide <code>size</code>.</p> </li> <li> <p>Optionally, user can request for scratch memory which can be used to store and load intermediate computations inside the kernel.</p> </li> <li> <p>Location of the custom operation functions</p> </li> <li> <p>Implementations</p> </li> </ul> <p>The information provided per implementation is:</p> <ul> <li> <p>target <code>Backend</code>.</p> </li> <li> <p><code>type</code>: used by the backend during selection process (see explanation next).</p> </li> <li> <p><code>impl</code>: location of the implementation file.</p> </li> <li> <p><code>config</code> (optional): additional information on this implementation, to be used by the backend.</p> </li> <li> <p><code>compilerArgs</code> (optional): hexagon-clang compiler options specific to the AIC backend implementation. This can be used to specify additional include directories, compile time MACROS, and so on. User is responsible for passing valid compilation options using this field.</p> </li> </ul> <p>The configuration file can provide the information for multiple operations. in this case, each operation is defined in a separated YAML section. Below is an example for how such configuration file would look like.</p> <p></p>"},{"location":"Getting-Started/Features/custom_ops/#custom-operation-functions","title":"Custom Operation Functions","text":"<p>The developer is required to provide a shared library which has three functions: Verification, Shape inference, and selection. The path to the shared library, as well as the library name, is specified in <code>functionsLibrary</code> field of the configuration file.</p> <p>The API of these functions is C, and defined in CustomOpFunctions.h. The types used are defined in CustomOpTypes.h.</p>"},{"location":"Getting-Started/Features/custom_ops/#verification-function","title":"Verification Function","text":"<p>The verification function is used by the compiler at graph construction stage, to validate that the custom operation implementation supports the combination of inputs, outputs, and parameters.</p> <pre><code>{C++}\n   bool customOpVerify(CustomOpIOTensor *inputs, int32_t numInputs,\n                       CustomOpIOTensor *outputs, int32_t numOutputs,\n                       CustomOpParam *params, int32_t numParams)\n</code></pre>"},{"location":"Getting-Started/Features/custom_ops/#shape-inference-function","title":"Shape Inference Function","text":"<p>The shape inference function is used by the compiler at graph construction stage to receive the output dimensions expected by the custom operation.</p> <p>In some models, specifying the output dimensions is optional. As such, the compiler has no way to known what will be the output dimensions of a custom operation (for internal known operations, it has shape inference functions).</p> <p>The function receives the input dimensions and parameters, and is expected to fill in the correct output dimensions.</p> <pre><code>{C++}\n   bool customOpInferShape(CustomOpIOTensor *outputs, const int32_t numOutputs,\n                           const CustomOpIOTensor *inputs, const int32_t numInputs,\n                           const CustomOpParam *params, const int32_t numParams)\n</code></pre>"},{"location":"Getting-Started/Features/custom_ops/#selection-function","title":"Selection Function","text":"<p>By design, the Custom Op functionality allows user to provide multiple implementations suitable for different configurations.</p> <p>For example, provide implementation for float vs one for integer, provide specialized implementation for specific parameter value, and so on.</p> <p>The developer implements this function to return the adequate flavor of custom op implementation based on the node configuration (inputs, outputs, parameters) The returned string represents the flavor. Used by the Backend Op repository to select the proper implementation based on configuration info (see details on config info).</p> <pre><code>{C++}\n   const char \\*customOpSelectImpl(CustomOpIOTensor *inputs, int32_t numInputs,\n                                   CustomOpIOTensor *outputs, int32_t numOutputs,\n                                   CustomOpParam *params, int32_t numParams,\n                                   const char *backend)\n</code></pre>"},{"location":"Getting-Started/Features/custom_ops/#custom-operation-implementations","title":"Custom Operation Implementations","text":"<p>The developer is required to provide suitable implementations for the compilation targets (Interpreter, AIC) and correctly register them in the configuration file.</p> <p>This section describes the implementations signatures and relation with the configuration file. The SDK comes with custom op examples which show how such implementations look like, and how do they get build</p>"},{"location":"Getting-Started/Features/custom_ops/#interpreter-implementation","title":"Interpreter Implementation","text":"<p>The interpreter implementation is provided to the compiler as a shared library (or collection of shared libraries). Each shared library can contain multiple versions (flavors) of implementations of the operation, refered onwards as kernels. A kernel is selected at model compilation time by the selection function. The developer is responsible for compilation of these shared libraries. As the interface is C, the shared libraries can be compiled by various compilers (GCC, CLANG, and so on). In addition, as these shared libraries are running on the Host CPU, the developer can open files, dump results, use stdout/stderr for printing debug messages, and so on. This makes the Interpreter implementation a very effective way for debugging the operation functionality as part of model execution.</p> <p>The signature of the kernel (implementation) is generic, and fits any custom operation. It contains array of input tensors, array of output tensors, array of parameters and a pointer to custom op context.</p> <pre><code>{C++}\n   typedef void (*customOpInterpreterKernel_t)(\n                  CustomOpIOTensor *inputs, int32_t numInputs,\n                  CustomOpIOTensor *outputs, int32_t numOutputs,\n                  CustomOpParam *params, int32_t numParams,\n                  CustomOpContext *ctx)\n</code></pre> <ul> <li> <p>The signature is defined in /opt/qti-aic/dev/inc/CustomOpInterpreterInterface.h</p> </li> <li> <p>The relevant types defined in /opt/qti-aic/dev/inc/CustomOpTypes.h</p> </li> <li> <p>CustomOpContext is defined in /opt/qti-aic/dev/inc/CustomOpContext.h</p> </li> </ul> <p>Note that the order of inputs, outputs and parameters has to match the order defined in the configuration file. During the model compilation, the compiler is organizing the inputs, outputs and params passed to the kernel based on that order.</p> <p>When creating an implementation, the developer needs to create a kernel (or multiple kernels) which has the signature as above and compile it into a shared library. For example:</p> <pre><code>{C++} \n   void customFoo (CustomOpIOTensor *inputs, int32_t numInputs,\n                   CustomOpIOTensor *outputs, int32_t numOutputs,\n                   CustomOpParam *params, int32_t numParams,\n                   CustomOpContext *ctx) {\n   // Foo implementation is here\n   }\n</code></pre> <p>The next section explains how the mapping between library / kernel name and compile-time selection is happening.</p>"},{"location":"Getting-Started/Features/custom_ops/#configuration-information-and-implementations","title":"Configuration information and implementations","text":"<p>The Compiler uses the information in the implementations section of the configuration file, together with the developers provided selection function, to allocate the proper implementation/kernel and use it. When the target is interpreter, and the compiler encounters a custom operation, it does the following:</p> <ul> <li> <p>Calls the selection function.</p> </li> <li> <p>Based on returned string, looks for the specific implementation in the implementations section, by trying to make the <code>type</code> field of each.</p> </li> <li> <p>Once found, opens the shared library which appears under <code>impl</code> field.</p> </li> <li> <p>Next, tries to get a function pointer (using <code>dlsym()</code>), using the name specified in type.</p> </li> </ul> <p></p> <p>As such, developers needs to make sure that the kernel names in the code/shared-library match the string appears in the type field.</p>"},{"location":"Getting-Started/Features/custom_ops/#aic-implementation","title":"AIC Implementation","text":"<p>The AIC implementation is provided as a C/C++ file. Just like in the case of interpreter target, the developer can provide a file (or multiple files) with multiple kernels (implementations) of the custom operation. The AIC compilation target API is similar to the Interpreter API, except for an additional thread identifier, <code>threadId</code>, passed to it.</p> <pre><code>{C++}\n   typedef void (*customOpAICKernel_t)(\n                  const CustomOpIOTensor *inputs, const int32_t numInputs,\n                  const CustomOpIOTensor *outputs, const int32_t numOutputs,\n                  const CustomOpParam *params, const int32_t numParams,\n                  const CustomOpContext *ctx, const int32_t threadID)\n</code></pre> <p>threadID: Developer can use up to 4 threads available on the AI Core. When the compiler generates the code, it will call the implementation 4 times, one time from each thread, passing to it the current threadID. This allows the compiler to maximize performance during compilation by taking full advantage of the HW. AIC implementation signature is defined in /opt/qti-aic/dev/inc/CustomOpAICInterface.h.</p>"},{"location":"Getting-Started/Features/custom_ops/#configuration-information-and-aic-implementations","title":"Configuration information and AIC implementations","text":"<p>Similarly to the Interpreter target, when compiling to AIC target the Compiler uses the information in the implementations section of the configuration file, together with the developers provided selection function, to allocate the proper implementation/kernel and use it.</p> <ul> <li> <p>Calls the selection function.</p> </li> <li> <p>Based on returned string, looks for the specific implementation in the implementations section, by trying to make the <code>type</code> field of each.</p> </li> <li> <p>Once found, looks for the C/C++ file in <code>impl</code> field.</p> </li> <li> <p>Next, tries to find a kernel using the name specified in <code>type</code>.</p> </li> </ul> <p></p>"},{"location":"Getting-Started/Features/custom_ops/#writing-tiled-custom-ops","title":"Writing tiled custom ops","text":"<p>Custom operations, by default, are configured to compile and run on one NSP. Custom ops can be configured to run in a tiled manner so that computation, along with inputs and outputs, can be split and distributed across multiple NSPs. Computing on tiled inputs and outputs helps boost performance as computation is split to execute in parallel on multiple NSPs with reduced memory footprint. Steps that should be followed to tile an operation are outlined below.</p> <ul> <li> <p>Specify tilability of each output of the CustomOp. This is done by configuring fields of <code>TileConfig</code> datastructure accessible in <code>customOpSetTileConfig</code> function. This step describes two things:</p> </li> <li> <p>axis along which an output can be sliced to form tiles, and</p> </li> <li> <p>alignment requirement for each axis that should be considered before slicing an output to form a tile. See /opt/qti-aic/dev/inc/CutomOpTileCon8fig.h for details.</p> </li> <li> <p>Provide the mapping of each output tile to its corresponding input tiles using <code>customOpMapTiles</code> function. Compiler uses each outputs <code>TileConfig</code> information to determine its tile size. The tiled outputs are made available in the <code>customOpMapTiles</code> function so that user can provide a mapping of output tiles to corresponding input tiles. For example, if compiler splits each output of a custom op into N tiles then <code>customOpMapTiles</code> will be invoked N times, once per output tile.</p> </li> </ul> <p>If the parameters configured by the user in <code>customOpSetTileConfig</code> and <code>customOpMapTiles</code> are valid then compiler will tile the custom operation, else the op will not be tiled.</p> <p>Note: NSP cores on the AIC hardware do not have a shared memory. So, when an op is tiled, it is important to make sure that all the necessary dependency on inputs to compute a tiled output is mapped accurately in <code>customOpMapTiles</code> function.</p>"},{"location":"Getting-Started/Features/custom_ops/#requesting-scratch-memory","title":"Requesting scratch memory","text":"<p>Custom operations may require scratch memory to store and load intermediate computations within the kernel. User can request for such scratch memory in the configuration file under Operation Info section by specifying the <code>name</code> and <code>size</code> of the required buffer(s) as follows:</p> <pre><code>scratchBuffers: \n  - name: scratchBuf1 \n    size: 1024  # size in bytes.\n  - name: scratchBuf2 \n    size: 2048  # size in bytes.\n</code></pre> <ul> <li> <p>Scratch buffers, allocated by the compiler, exist throughout the lifespan of custom op function call, but content is not preserved between calls.</p> </li> <li> <p>Compiler makes an effort to allocate these buffers in the device VTCM. However, its allocation in the VTCM is not gauranteed.</p> </li> <li> <p>Scratch buffers are accessible only within custom op implementation through the <code>CustomOpContext*</code>, which is available to every implementation.</p> </li> <li> <p>Scratch buffers remain untiled even when the custom op is tiled and the scratch buffer size requested by the user is available to each tiled custom op.</p> </li> </ul> <pre><code>{C++} \n   float *buf1 = (float *)customOpContext-&gt;scratchBuffers[0].buf;\n   int *buf2 = (int *)customOpContext-&gt;scratchBuffers[1].buf;\n   int buf1Size = customOpContext-&gt;scratchBuffers[0].size;\n   int buf2Size = customOpContext-&gt;scratchBuffers[1].size;\n</code></pre>"},{"location":"Getting-Started/Features/custom_ops/#memory-configuration-for-the-aic-backend","title":"Memory configuration for the AIC backend","text":"<p>VTCM, DDR and CacheableDDR memory locations are available on the device. <code>memoryConfig</code> in the yaml configuration file can be used to specify a memory placement preference for each input, output and scratch buffer associated with custom op so that compiler can prioritize buffer placement:</p> <pre><code>- implementation \n  - backend: AIC \n      type: funcName\n      impl: fileName.cpp\n      memoryConfig:\n        #DDR/VTCM/CacheableDDR: [&lt;comma separated list of in/out/scratch names&gt;]\n        DDR: [inName1, outName2]\n        CacheableDDR: [inName2]\n        VTCM: [scratchBufName1]\n        requiredFor: [inName2, scratchBufName1]\n</code></pre> <p>Three preference lists, <code>VTCM</code>, <code>DDR</code> and <code>CacheableDDR</code>, as shown above, are available. If an input, output, or scratch buffer is listed under <code>VTCM</code> list, compiler will make an effort to place the listed buffer in the devices VTCM location. However, the placement in VTCM is not guaranteed by the compiler. Same holds true for the entries in the rn whenever the user preferred memory configuration is not satisfied during model compilation.</p> <p>If placement of a particular buffer in the specified memory location is a stringent requirement then user can indicate it to the compiler using <code>requiredFor</code> list. If compiler fails to allocate a buffer that is listed in <code>requiredFor</code> list as per the preference, then model compilation will fail with an error message.</p> <p>Altering the default memory configuration may change overall performance of the model as the memory locations are also used by the compiler for other ops in the model. General guideline for better performance is that VTCM is efficient for vector memory accesses, <code>CacheableDDR</code> is efficient for scalar memory accesses, and DDR access is relatively slower.</p>"},{"location":"Getting-Started/Features/custom_ops/#synchronize-threads-on-the-aic-backend","title":"Synchronize threads on the AIC backend","text":"<p>Custom op kernels on the AIC backend can take advantage of 4 threads running on the NSP. User can synchronize all threads, if needed, using sync function pointer from the CustomOpContext. Usage is shown below:</p> <pre><code>customOpContext-&gt;syncThread(threadId);\n</code></pre> <p>Execution will wait for all threads to reach <code>syncThread</code> before proceeding.</p>"},{"location":"Getting-Started/Features/custom_ops/#logging-messages-on-aic-backend","title":"Logging messages on AIC backend","text":"<p>AIC backend makes logging available through printf style macros <code>AIC_PRINT_*</code>, defined in CustomOpLog.h. This allows runtime logging on AIC target. 64 bit values require wrapping the argument with <code>AIC_LOG64</code> macros.</p> <pre><code>AIC_PRINT_INFO(ctx, \"decimal num: %d; unsigned num: %u; C-string: %s;\",\n                     -1, 0xFF, \"myCustomOp\");\nAIC_PRINT_INFO(ctx, \"int64_t HEX num: 0x\" AIC_LOG64X_FMT, AIC_LOG64_DATA(0x1122334455667788));\nAIC_PRINT_INFO(ctx, \"fp32 num: %f\", 3.141590f);\n</code></pre> <p>Qmonitor service must be running on host for logs to be collected automatically. This requires to issue a one-time command on host to start Qmonitor:</p> <pre><code>systemd-run --unit=qmonitor-proxy /opt/qti-aic/tools/qaic-monitor-grpc-server\n</code></pre> <p>Different log message types are available: <code>AIC_PRINT_DEBUG</code>, <code>AIC_PRINT_INFO</code>, <code>AIC_PRINT_WARN</code>, <code>AIC_PRINT_ERROR</code>, and <code>AIC_PRINT_FATAL</code>, each corresponding to decreasing level of verbosity. Compiler option, <code>AIC_CUSTOMOP_LOG_COMPILE_LEVEL</code>, controls verbosity level which can be set to: <code>AIC_CUSTOMOP_LOG_LEVEL_FATAL</code>, <code>AIC_CUSTOMOP_LOG_LEVEL_ERROR</code>, <code>AIC_CUSTOMOP_LOG_LEVEL_WARN</code>, <code>AIC_CUSTOMOP_LOG_LEVEL_INFO</code> (default), or <code>AIC_CUSTOMOP_LOG_LEVEL_DEBUG</code>. All message types above given verbosity level are filtered out. <code>AIC_CUSTOMOP_LOG_COMPILE_LEVEL</code> can be overriden through the <code>compilerArgs</code> field on the custom op configuration yaml file.</p> <pre><code>implementations: \n   - backend: AIC \n     compilerArgs: -DAIC_CUSTOMOP_LOG_COMPILE_LEVEL=AIC_CUSTOMOP_LOG_LEVEL_ERROR\n</code></pre> <p>Logs are collected under /var/log/qti-aic/QID_* folders, with each AIC device having its own folder. A maximum of ten format specifiers can be added to a single format string when using <code>AIC_PRINT*()</code> with AIC backend. <code>AIC_PRINT_*</code> macros will map to print when a custom op is compiled for backends other than AIC.</p> <p>Log verbosity level may also be configured at run time using qaic-log tool with <code>-c n</code> and <code>-s NNNetwork</code> flags.</p> <pre><code>/opt/qti-aic/tools/qaic-log -l &lt;log_level&gt; -c n -s NNNetwork\n</code></pre> <p>where <code>&lt;log-level&gt;</code> is first letter of log level, that is any of <code>f/e/w/i(default)/d</code>. For more details run <code>qaic-log -h</code>.</p>"},{"location":"Getting-Started/Features/custom_ops/#datatype-support","title":"Datatype support","text":"<p>Custom ops support various data types for tensors and parameters as listed in <code>CustomOpDataType</code>. The datatype of input and output tensors of custom op (<code>CustomOpIOtensor</code>) can be one of the following types: <code>float</code>, <code>float16_ty</code> (AIC backend only), and <code>int8_t</code>, depending on the compilation options chosen by the user. <code>CustomOpTensor</code> will be of <code>float</code> type when model is compiled with float precision. Similarly, when model is compiled with <code>-convert-to-fp16</code> flag or using a quantization profile, tensors will be of <code>float16_ty</code> or <code>int8_t</code> type, respectively. Using different datatypes for tensors is currently limited. <code>int8_t</code>, <code>int16_t</code>, <code>int32_t</code>, and <code>int64_t</code> can also be used to represent index types. Tensors will not be quantized when index types are specified in the input model, irrespective of the model compilation options chosen.</p> <p>When model is quantized using a quantization profile <code>CustomOpIOTensor_s</code> are quantized using a scale and offset. User can dequantize input and output <code>_CustomOpIOTensor</code> data using the <code>scale</code> and <code>offset</code> as follows: <code>dequant(x) = (x - offset) * scale</code>, where <code>scale</code> and <code>offset</code> are accessible from <code>CustomOpContext</code> using following APIs:</p> <pre><code>{C++}\n   float getInputScale(const CustomOpContext *ctx, const int32_t inputIdx); \n   int32_t getInputOffset(const CustomOpContext *ctx, const int32_t inputIdx); \n   float getOutputScale(const CustomOpContext *ctx, const int32_t outputIdx); \n   int32_t getOutputOffset(const CustomOpContext *ctx, const int32_t outputIdx);\n</code></pre> <p>Note that other software components, like AIMET or PGQ profiler, may represent quantization parameters differently, but that does not affect how custom ops represent scale and offset.</p>"},{"location":"Getting-Started/Features/custom_ops/#capabilities-and-limitations","title":"Capabilities and limitations","text":"<p>The custom operation code can use any basic C/C++ functionality, including control code, loops, etc. In addition, the code can use HVX intrinsic for utilizing the embedded vector units. See next section for details. The code is limited to run on a single AI Core of the overall number of available cores. This limitation may be lifted on future releases.</p> <p>The developer should be aware of the following limitations when creating an AIC implementation:</p> <ul> <li> <p>Typical OS services do not exists on the HW. Code cannot use STDIO to write logs, attempt to open files or create new threads.</p> </li> <li> <p>Dynamic Memory allocation is not supported. Only static memory allocation is allowed within the implementation. Avoid using 1) malloc/new, 2) STD data types (like <code>std::vector</code> which requires dynamic allocation), 3) defining arrays with size not known at compile time (like <code>array[argument #6]</code>).</p> </li> </ul>"},{"location":"Getting-Started/Features/custom_ops/#qualcomm-hexagon-hvx-documentation","title":"Qualcomm Hexagon HVX documentation","text":"<p>When writing code to run on AIC, the developer can use HVX intrinsic to utilize the internal vector units of the AI Core.</p> <p>For documentation on HVX intrinsic, please go to Qualcomm Developer Network website, Hexagon SDK SW page: https://developer.qualcomm.com/software/hexagon-dsp-sdk/tools and download the document named Qualcomm Hexagon V66 HVX Programmers Reference Manual.</p>"},{"location":"Getting-Started/Features/custom_ops/#compilation","title":"Compilation","text":"<p>As explained in Compiler Tools, qaic-exec is a command-line sample application, provided in the Apps SDK both as code and as prebuilt binary. It supports compiling and running a model.</p> <p>For supporting custom operations, a new command-line option was added to qaic-exec.</p> <pre><code>-register-custom-op=&lt;config-file&gt;           Register custom op using this configuration file\n                                            Specify multiple times to register multiple configs\n</code></pre> <p>Here is an example for running qaic-exec with a model which has custom operations, and providing a configuration file.</p> <pre><code>/opt/qti-aic/exec/qaic-exec -m=./mlp_custom.onnx -register-custom-op=./CustomReluConfig.yaml \n-aic-hw -aic-hw-version=2.0 -input-list-file=input.list -write-output-dir=./ \n-write-output-start-iter=1 -write-output-num-samples=1\n</code></pre>"},{"location":"Getting-Started/Features/custom_ops/#examples","title":"Examples","text":"<p>End-to-end examples of custom operations is available in the APPS SDK at <code>/opt/qti-aic/examples/apps/custom-op/</code>. </p>"},{"location":"Getting-Started/Features/model_sharding/","title":"Model Sharding","text":"<p>Cloud AI SDK enables model sharding which provides the benefits of running larger models and improve throughput/latency/batch-size support across SoCs/Cards connected to the same host. Topologies supported are with or without a PCIe switch. Cards connected to a PCIe switch with peer-to-peer communication enabled provide the best performance. </p>"},{"location":"Getting-Started/Features/model_sharding/#use-cases","title":"Use Cases","text":"<p>There are 2 primary use cases of model sharding via tensor slicing. </p> <ul> <li>Execute models that do not fit in the memory footprint of a single SoC. </li> <li>Optimize performance (latency/throughput) for models that can fit within a single SoC but still benefit from tensor-slicing.  </li> </ul>"},{"location":"Getting-Started/Features/model_sharding/#architecture","title":"Architecture","text":"<p>For tensor slicing to achieve the best performance (latency/throughput), the server architecture in particular, accelerator card inter-connect performance is critical. The image below shows 8 AI 100 Ultra accelerator cards connected via a PCIe switch to the host. There are two approaches regarding card-to-card communication.  </p> <ul> <li>P2P communication between the cards through a PCIe switch. This architecture provides the best performance. </li> <li>Multi-device through host: Card to card communication happens through the host. This approach will have inferior performance compared to P2P.  </li> </ul> <p>This sample configuration allows model sharding via tensor slicing across 8 cards (typically used for &gt; 15B parameter models).     </p> <p></p>"},{"location":"Getting-Started/Features/model_sharding/#tensor-slicing","title":"Tensor Slicing","text":"<p>Model operations are split across muliple SoCs (maximum across 16 SoCs in P2P config). The image provides a sample graph execution that is tensor sliced across 4 AI 100 accelerator cards. As seen from the image, there is a lot of inter-card traffic across models layers. Inter-card data bandwidth available plays a critical role in the performance. Hence, the need to enable P2P inter-card communication via PCIe switch.  The AI 100 Ultra card has a PCIe switch between the 4 SoCs on the card. In a server with many AI 100 accelerators the PCIe hierarchy plays a critical role in the performance.  </p> <p></p>"},{"location":"Getting-Started/Features/model_sharding/#platform-setup","title":"Platform setup","text":""},{"location":"Getting-Started/Features/model_sharding/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Host should be able to support large BAR sizes. Each AI 100 accelerator card requires 2+ GB of BAR space per SoC.  </li> <li>BAR region 4 for every AI 100 SoC is 2G (size=2G as shown below). <pre><code>lspci -s &lt;PCIe address of AI 100 SoC&gt; -vv | grep \"Region 4\"\nRegion 4: Memory at xyz (64-bit, prefetchable) [size=2G]\n</code></pre>     If the region 4 for every SoC is not 2G, contact your System Integrator.</li> </ul>"},{"location":"Getting-Started/Features/model_sharding/#card-configuration","title":"Card configuration","text":"<ol> <li> <p>Disable PCIe Switch ACS to enable P2P communication between PCIe ports. See instructions here. </p> </li> <li> <p>Enable multi-device partitioning on all the SoCs.      Download enable_mdp.json from here.      <pre><code>systemd-run --unit=qmonitor-proxy /opt/qti-aic/tools/qaic-monitor-grpc-server\n/opt/qti-aic/tools/qaic-monitor-json -i enable_mdp.json\u200b \n</code></pre></p> <p>Reset all the SoCs in the server. <pre><code>sudo /opt/qti-aic/tools/qaic-util -s\n</code></pre> 3. Verify that multi-device partitioning (MDP) feature is enabled for all devices.</p> <p><pre><code>/opt/qti-aic/tools/qaic-util -q | grep MDP\n</code></pre> <code>MDP+</code> indicates that multi-device feature is enabled on the device. </p> </li> </ol>"},{"location":"Getting-Started/Features/model_sharding/#compilation","title":"Compilation","text":"<p>Model partitioning across multiple devices is done by the compiler. The user is required to specify the number of SoCs/devices and the connectivity between the devices. Here a few examples of the device partition config files based on the connectivity and number of devices. The device partition config file is passed to the compiler <code>qaic-exec</code> CLI. </p> <p>Example 1: Model, tensor sliced across 4 SoCs with P2P communication between the SoCs.  <pre><code>mdp_4soc_p2p.json \n\n{\n    \"connections\": [\n        {\n            \"devices\": [0,1,2,3],\n            \"type\": \"p2p\"\n        }\n    ],\n    \"partitions\": [\n        {\n            \"name\": \"Partition0\",\n            \"devices\": [\n                {\n                    \"deviceId\": 0,\n                    \"numCores\": 16\n                },\n                {\n                    \"deviceId\": 1,\n                    \"numCores\": 16\n                },\n                {\n                    \"deviceId\": 2,\n                    \"numCores\": 16\n                },\n                {\n                    \"deviceId\": 3,\n                    \"numCores\": 16\n                }\n            ]\n        }\n    ]\n}\n</code></pre></p> <p>Example 2: Model, tensor sliced across 2 SoCs with communication through the host between the SoCs. If no <code>connections</code> are defined, the conectivity is assumed to be through the host</p> <pre><code>mdp_2soc_host.json \n\n{\n    \"partitions\": [\n        {\n            \"name\": \"Partition0\",\n            \"devices\": [\n                {\n                    \"deviceId\": 0,\n                    \"numCores\": 16\n                },\n                {\n                    \"deviceId\": 1,\n                    \"numCores\": 16\n                }\n            ]\n        }\n    ]\n}\n</code></pre> <p>To compile the model with the tensor sliced configurations, pass the device paritioning config file to <code>qaic-exec</code> using <code>mdp-load-partition-config</code> flag as shown below. </p> <pre>\n/opt/qti-aic/exec/qaic-exec \\\n    -m=$model_path \\\n    -aic-hw \\\n    -aic-hw-version=2.0 \\\n    -network-specialization-config=specializations.json \\\n    -retained-state \\\n    -convert-to-fp16 \\\n    -aic-num-cores=${CORES} \\\n    -custom-IO-list-file=${model_name}/custom_io.yaml \\\n    -compile-only \\\n    -aic-binary-dir=qpc/${model_name}-${BS}bs-${PL}pl-${CL}cl-${CORES}c-${SOCS}soc-${MX} \\\n    -mdp-load-partition-config=mdp.json\n</pre>"},{"location":"Getting-Started/Features/model_sharding/#execution","title":"Execution","text":"<p>Refer to Cloud-ai-sdk example for executing inference on multi-SoCs. </p>"},{"location":"Getting-Started/Features/model_sharding/#recommendations","title":"Recommendations","text":"<p>For very large models which are compiled for inter-SoC communication through the host, the host memory requirements can be large. If inference fails due to host or device resource exhaustion, try below options. </p> <ul> <li> <p>Increase the maximum number of memory mappings allowed for a process from the default value of 65k      <pre><code>sudo bash -c \"sysctl -w 'vm.max_map_count=2048000'\"\n</code></pre>     Verify the new setting using      <pre><code>cat /proc/sys/vm/max_map_count\n</code></pre></p> </li> <li> <p>Increase system memory (RAM) to 1TB and CPU count to 32 cores or higher. </p> </li> </ul>"},{"location":"Getting-Started/Glossary/","title":"Cloud AI Glossary","text":"Term Description AI Cores / AI compute cores / NSP These terms are interchangably used to refer to the Neural Signal Processor cores in the Cloud AI Platforms. The NSP cores run the inference. Instance / activation A single Instance or activation refers to a QPC executing on a set of AI cores. QAIC / AIC / Cloud AI/ AIC100 These terms are interchangably used to refer to Cloud AI. These terms are typically followed by the subject (like Compiler / SDK / library etc) being discussed. QPC (Qaic Program Container) / model binary / network binary Cloud AI platforms can execute a QPC. A pre-trained model needs to be compiled to generate a QPC. QPC / model binary / network binary terms are used interchangably. TCM / VTCM (Vector) Tightly-Coupled Memory.  On-chip or in-NSP memory as featured in the ARM specification. ECC Error Correcting Code.  Error correction in the TCM as featuerd in the ARM specification. VM/KVM (Kernel-based) Virtual Machine. ONNX Open Neural Network Exchange file format."},{"location":"Getting-Started/Inference-Workflow/","title":"Inference workflow on Cloud AI","text":"<p>Developers can deploy a pre-trained model for inference on any Cloud AI platform using three easy steps:</p> <ol> <li>Export the model in ONNX format and prepare the model </li> <li>Compile the model to generate a QPC (Qaic Program Container)</li> <li>Execute, integrate and deploy into production pipeline</li> </ol> <p></p> <p>Cloud AI SDK provides tools for each of the steps as shown below. </p> <p></p>"},{"location":"Getting-Started/Inference-Workflow/#export-and-prepare-the-model","title":"Export and prepare the Model","text":"<p>Exporting and preparing the pre-trained model as explained in this section is a requirement to extract the best performance and accuracy. Exporting the model in ONNX format is strongly recommended due to operator support. The ONNX file is then passed through a few checks (for model accuracy and performance) before it can be compiled in the next step.   </p>"},{"location":"Getting-Started/Inference-Workflow/#compile-the-model","title":"Compile the Model","text":"<p>Compilation of the prepared ONNX file generates a QPC which can be loaded and executed on Cloud AI devices. To get the best performance from the device based on user requirements of throughput and latency, compilation needs to be done with the right parameters. Tune the performance goes over how to derive the best parameters for compilation. </p>"},{"location":"Getting-Started/Inference-Workflow/#execute-integrate-and-deploy-in-production-pipeline","title":"Execute, integrate and deploy in production pipeline","text":"<p>This section will go over the different ways (CLI, C++, Python etc) for developers to execute inferences on Cloud AI platforms and all the tools/utilities available to integrate into inference application and deploy in the production pipeline.  </p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Export-the-Model/","title":"Exporting ONNX Model from Different Frameworks","text":"<p>We will start with the process of exporting an ONNX model from various deep learning frameworks such as PyTorch, TensorFlow, and Caffe2.</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Export-the-Model/#exporting-from-pytorch","title":"Exporting from PyTorch","text":"<p>PyTorch is a popular deep learning framework that provides dynamic computation graphs. To export a PyTorch model to ONNX format, you need to follow these steps:</p> <pre><code>import torch\nimport torchvision.models as models\n\n# Load a pre-trained PyTorch model\nmodel = models.resnet18(pretrained=True)\n\n# Set the model to evaluation mode\nmodel.cpu().eval()\n\n# Define example input tensor\ninput_tensor = torch.randn(1, 3, 224, 224).cpu()  # Replace with your own input shape\n\n# Export the model to ONNX format\ntorch.onnx.export(model, input_tensor, \"model.onnx\", verbose=True, opset=13)\n</code></pre> <p>In the above code, we import the necessary libraries and load a pre-trained ResNet-18 model. We then set the model to evaluation mode and define an example input tensor. Finally, we use the <code>torch.onnx.export</code> function to export the model to the ONNX format. You can replace <code>\"model.onnx\"</code> with the desired output file path.</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Export-the-Model/#exporting-from-tensorflow","title":"Exporting from TensorFlow","text":"<p>TensorFlow is another widely used deep learning framework that provides both static and dynamic computation graphs. Here's how you can export a TensorFlow model to ONNX format:</p> <pre><code>import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nimport tf2onnx\n\n# Load a pre-trained TensorFlow model\nmodel = ResNet50(weights='imagenet')\n\n# onnx_model = tf2onnx.convert.from_keras(model)\ninput_tensor_spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name=\"input\"),)\n\n# Convert the TensorFlow model to ONNX format\n# Save the ONNX model to a file\nmodel_proto, _ = tf2onnx.convert.from_keras(model, input_signature=input_tensor_spec, opset=13, output_path=\"model.onnx\")\n</code></pre> <p>In the above code, we import the necessary libraries and load a pre-trained ResNet-50 model using Keras. We then convert the TensorFlow model to the ONNX format using the <code>tf2onnx.convert.from_keras</code> function. Finally, we save the ONNX model to a file using the <code>tf2onnx.save_model</code> function.</p> <p>In case you have save tensorflow model or saved checkpoint path of tensorflow model, you can load the model in tensorflow/keras or you can directly use the following commands from <code>tf2onnx</code> libraries.</p> <pre><code># You have path to your saved TensorFlow model\npython -m tf2onnx.convert --saved-model tensorflow-model-path --opset 11 --output model.onnx\n\n# For checkpoint format:\npython -m tf2onnx.convert --checkpoint  tensorflow-model-meta-file-path --output model.onnx --inputs input0:0,input1:0 --outputs output0:0\n\n# For graphdef format:\npython -m tf2onnx.convert --graphdef  tensorflow-model-graphdef-file --output model.onnx --inputs input:0,input:0 --outputs output0:0\n</code></pre> <p>You can refer this notebook from tf2onnx</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Export-the-Model/#exporting-from-caffe2","title":"Exporting from Caffe2","text":"<p>Caffe2 is a deep learning framework primarily developed by Facebook. It focuses on performance and is widely used for production deployments. Here's an example of exporting a Caffe2 model to ONNX format:</p> <pre><code>from caffe2.python.onnx import backend\nfrom caffe2.python.predictor import predictor_exporter\n\n# Load the Caffe2 model from .pb and .init files\ninit_net = \"model.init\"  # Replace with the path to the .init file\npredict_net = \"model.pb\"  # Replace with the path to the .pb file\n\n# Export the Caffe2 model to ONNX format\nonnx_model = backend.prepare(model, device='CPU')\nonnx_model.export_onnx(\"model.onnx\")\n</code></pre> <p>In the above code, we pre-trained caffe2 model from .init and .pb file. Finally, we use the <code>caffe2.python.onnx.backend.prepare</code> function to export the model to the ONNX format. You can replace <code>\"model.onnx\"</code> with the desired output file path. Please refer caffe2-installation to setup the environment</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Export-the-Model/#loading-onnx-model","title":"Loading ONNX Model","text":"<p>After exporting the model from your preferred framework to ONNX, you can load it for inference using the <code>onnx</code> package. Here's an example:</p> <pre><code>import onnx\nimport onnx.checker\n\n# Load the ONNX model\nmodel = onnx.load(\"path/to/exported_model.onnx\")\n\ntry:\n    onnx.checker.check_model(model)\nexcept:\n    onnx.checker.check_model_path(\"path/to/exported_model.onnx\")\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Operator-and-Datatype-support/","title":"Operator and Datatype support","text":""},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Operator-and-Datatype-support/#operator-support","title":"Operator support","text":"<p>In this section we will discuss the operator support available for the device across various frameworks. </p> <p>To determine the list of operators supported by the device for various frameworks, you can execute the following command</p> <p>Example command to generate operators supported for onnx<pre><code>/opt/qti-aic/exec/qaic-exec -operators-supported=onnx\n</code></pre> This command generators a file <code>OnnxSupportedOperators.txt</code> which comprehensive list of ops supported. </p> <p>It is important to note that the operator support keep expanding with the release of new SDK versions.</p> Note <pre><code>-operators-supported supports only onnx, tensorflow, pytorch, caffe, and caffe2.\n</code></pre> Note <p><code>onnx</code> is the preferred format to compile the model for the device. </p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Operator-and-Datatype-support/#handling-unsupported-operators","title":"Handling Unsupported Operators","text":"<p>In some cases, you might encounter errors related to unsupported operations while compiling the model for the device. </p> <p>For instance, certain operations like <code>einsum</code> present in the model file might not be directly supported by the device. </p> <p>In such scenarios, the Model Preparator tool can be employed to modify the model and substitute these unsupported operations with their corresponding mathematical equivalent subgraphs.</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Operator-and-Datatype-support/#datatype-support","title":"Datatype Support","text":""},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Operator-and-Datatype-support/#fp32-single-precision-floating-point","title":"FP32 (Single Precision Floating Point)","text":"<p>Models can be executed in FP32 for use cases where accuracy is critical and computational efficiency is not a primary concern. It is essential to note that FP32 models tend to have larger sizes and will exhibit lower throughput performance. FP32 execution is supported but not recommended. Do not use the <code>-convert-to-fp16</code> with <code>qaic-exec</code> (compiler CLI) during compilation. </p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Operator-and-Datatype-support/#fp16-half-precision-floating-point","title":"FP16 (Half Precision Floating Point)","text":"<p>FP16 strikes a balance between accuracy and efficiency, making it suitable for most deep learning workloads.</p> <p>If a model is originally trained in FP32 format, it can be down-converted to FP16 during the compilation process using the <code>-convert-to-fp16</code> flag. However, certain scenarios may involve constants beyond the FP16 range. In such cases, it is recommended to clip values to the FP16 range (as demonstrated in the fix_onnx_fp16 function in the NLP tutorials in Cloud-ai-sdk repo).</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Operator-and-Datatype-support/#shared-micro-exponents-narrow-precision-format","title":"Shared Micro-exponents (Narrow Precision Format)","text":"<p>The shared micro-exponent spec is here. The Cloud AI compiler will support all of the formats over time. Currently, MXFP6 is supported by the compiler. </p> <p>Models compiled in MXFP6 format stores FP32/FP16 weights using a 6-bit format. By doing so, it significantly reduces model sizes due to the compact representation. This format is particularly beneficial for models that require high data bandwidth, such as large language models (LLMs).</p> <p>AI100 stores Matmul weights in MXFP6 format while keeping the rest of the weights in FP16 format. Computation/activations on the NSP still occur in FP16.</p> <p>LLMs experience up to 2x throughput with minimal accuracy loss with MXFP6 format. Within a constant memory footprint, a larger model can be supported with MXFP6. </p> <p>FP32 models can be compiled into MXFP6 format using the compiler flag <code>-mxfp6-matmul</code>. FP16 execution should use both <code>-convert-to-fp16</code> and <code>-mxfp6-matmul</code> flags for the <code>qaic-exec</code> compiler CLI. </p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Operator-and-Datatype-support/#uint8-unsigned-8-bit-integer","title":"UINT8 (Unsigned 8-bit Integer)","text":"<p>AI100 also supports executing INT8 quantized models, especially relevant for Natural Language Processing (NLP) and Computer Vision (CV) tasks.</p> <p>Quantization Methods Supported: - Quantization Schema for Weights and Activations: Both symmetric and asymmetric. - Quantization Calibration: Options include KLMinimization, KLMinimizationV2, MSE, SQNR, and Percentile (with percentile calibration values: 99.9, 99.99, 99.999, 99.9999).</p> <p>The SDKs provide tools to run a PGQ sweep, allowing you to identify the optimal quantization parameters for your specific requirements.</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Operator-and-Datatype-support/#bf16-bfloat16","title":"BF16 (BFloat16)","text":"<p>If a model is trained in BF16 (bfloat16), ensure that the weights are scaled down using an appropriate scaling factor. This prevents intermediate activations from overflowing into <code>fp16</code>. Qualcomm can provide a script to identify the scaling factors to scale down the weights of the models such that intermediate activations will not overflow <code>fp16</code>.</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/","title":"Introduction to the Model Preparator Tool","text":"<p>The QAic Model Preparator is an optional tool that automates generating Optimal AIC Friendly models for usage. It applies various optimizations and cleaning techniques for generation of the model. Developers can use this tool if the exported model fails compilation. </p> <p>The current version supports ONNX and TF models. The tool checks the model, applies shape inference, cleans the model, applies various optimization, handles the pre-processing/post-processing nodes in the graph, and generates models as per the given user configuration (YAML).</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#model-preparator-overview","title":"Model Preparator Overview","text":"<p>Checkout the Model Export page on details on exporting the model from one framework to another framework, the most preferred and tested framework on MP is Onnx.</p> <p>For detailed usage of the tool, refer to this link</p> <p></p> <p>usage: Sample Config Details are  For Single model:</p> <pre><code>MODEL: \n    INFO: \n        MODEL_TYPE: EFFICIENTDET\n        INPUT_INFO: \n        - - input:0\n          - - 1\n            - 512\n            - 512\n            - 3\n        DYNAMIC_INFO: \n        - - input:0\n          - - batch_size\n            - 512\n            - 512\n            - 3\n        EXPORT_TYPE: ONNX\n        OPSET: 13\n        MODEL_PATH: efficientdet-d0.pb\n        NAME: Sample Model\n        DESCRIPTION: SSD MobileNet V1 Opset 10 Object Detection Model\n        VALIDATE: False\n        AIC_DEVICE_ID: 0\n        WORKSPACE: WORKSPACE\n        VERBOSE: INFO\n        INPUT_LIST_FILE: None\n        CUSTOM_OP_INFO_FILEPATH: None\n\nPRE_POST_HANDLE: \n    NMS_PARAMS: \n        MAX_OUTPUT_SIZE_PER_CLASS: 100\n        MAX_TOTAL_SIZE: 100\n        IOU_THRESHOLD: 0.65\n        SCORE_THRESHOLD: 0.25\n        CLIP_BOXES: False\n        PAD_PER_CLASS: False\n\n    PRE_PLUGIN: True\n    TRANSFORMER_PACKED_MODEL: False\n    COMPRESSED_MASK: False\n    POST_PLUGIN: SMARTNMS\n    ANCHOR_BIN_FILE: None\n\nATTRIBUTES: None\n</code></pre> <p>Usage on Multi Model Config:</p> <pre><code>MODELS: \n  MODEL: \n      INFO: \n          MODEL_TYPE: CLASSIFICATION\n          INPUT_INFO: \n          - - model_1_input\n            - - 1\n              - 100\n          DYNAMIC_INFO: \n          - - model_1_input\n            - - batch\n              - 100\n          EXPORT_TYPE: ONNX\n          OPSET: 13\n          MODEL_PATH: model_1.onnx\n          NAME: Model_1\n          DESCRIPTION: Sample model-1\n          VALIDATE: True\n          AIC_DEVICE_ID: 0\n          WORKSPACE: ./workspace\n          VERBOSE: INFO\n          INPUT_LIST_FILE: None\n          CUSTOM_OP_INFO_FILEPATH: None\n\n  PRE_POST_HANDLE: \n      NMS_PARAMS: \n          MAX_OUTPUT_SIZE_PER_CLASS: 100\n          MAX_TOTAL_SIZE: 500\n          IOU_THRESHOLD: 0.5\n          SCORE_THRESHOLD: 0.01\n          CLIP_BOXES: False\n          PAD_PER_CLASS: False\n\n      PRE_PLUGIN: True\n      TRANSFORMER_PACKED_MODEL: False\n      COMPRESSED_MASK: False\n      POST_PLUGIN: NONE\n      ANCHOR_BIN_FILE: None\n\n  ATTRIBUTES: None\n\n\n  MODEL: \n    INFO: \n        MODEL_TYPE: CLASSIFICATION\n        INPUT_INFO: \n        - - model_2_input\n          - - 1\n            - 10\n        DYNAMIC_INFO: \n        - - model_2_input\n          - - batch\n            - 10\n        EXPORT_TYPE: ONNX\n        OPSET: 13\n        MODEL_PATH: model_2.onnx\n        NAME: Model_2\n        DESCRIPTION: Sample model-2\n        VALIDATE: True\n        AIC_DEVICE_ID: 0\n        WORKSPACE: ./workspace\n        VERBOSE: INFO\n        INPUT_LIST_FILE: None\n        CUSTOM_OP_INFO_FILEPATH: None\n\n  PRE_POST_HANDLE: \n      NMS_PARAMS: \n          MAX_OUTPUT_SIZE_PER_CLASS: 100\n          MAX_TOTAL_SIZE: 500\n          IOU_THRESHOLD: 0.5\n          SCORE_THRESHOLD: 0.01\n          CLIP_BOXES: False\n          PAD_PER_CLASS: False\n\n      PRE_PLUGIN: True\n      TRANSFORMER_PACKED_MODEL: False\n      COMPRESSED_MASK: False\n      POST_PLUGIN: NONE\n      ANCHOR_BIN_FILE: None\n\n  ATTRIBUTES: None\n</code></pre> <p>CHAINING_INFO:      EDGES:      - - Model_1::model_1_output       - Model_2::model_2_input</p> <pre><code>WORKSPACE: ./workspace_chaining/\nEXPORT_TYPE: ONNX\nOPSET: 13\nAIC_DEVICE_ID: 0\n</code></pre> <p>qaic-preparator options</p> <p>Config Options:   --config CONFIG  Path to config file   --silent         Run in silent mode</p> <p>For more details checkout the help menu in the tool: \"python3 qaic-model-preparator.py --help\"  Tool location in the SDK: \"/opt/qti-aic/tools/qaic-pytools\"</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#sample-details-on-model-configuration-settings-file","title":"Sample details on Model Configuration Settings file","text":""},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#model-preparator-configurations","title":"Model Preparator Configurations","text":"<ul> <li>MODEL <ul> <li> DESCRIPTION : Model Description</li> <li> MODEL_TYPE  : Provide the Model Category it belongs to</li> <li> MODEL_PATH  : Provide path to the downloaded original model file</li> <li> INPUT_INFO  : Provide Dictionary of Input names and its corresponding shapes.</li> <li> DYNAMIC  : Provide Dynamic Info to generate dynamic models</li> <li> EXPORT_TYPE  : ONNX or TENSORFLOW</li> <li> VALIDATE  : Validate the prepared model outputs with native framwork. (Ex: QAic FP16 vs OnnxRuntime FP32)</li> <li> WORKSPACE   : Provide the workspace directory to save the log files and final prepared model.</li> </ul> </li> <li>PRE_POST_HANDLE<ul> <li> ANCHOR_BIN_FILE   : Path to Anchor bin files or None.</li> <li> POST_PLUGIN  : None by Default, If Object Detection Models, Provide \"Smart-nms\" or \"QDetect\" to either generate split model or full model.</li> <li> PRE_PLUGIN   : Handle pre plugin, This will be either keep/remove the pre processing from the graph.</li> <li> NMS_PARAMS   : This fields are specific to Object Detection models, and are helpful while creating full model based on QDetect nodes.<ul> <li> MAX_OUTPUT_SIZE_PER_CLASS   : A scalar integer representing the maximum number of boxes to be selected by non-max suppression per class</li> <li> MAX_TOTAL_SIZE   : A integer representing maximum number of boxes retained over all classes. Note that setting this value to a large number may result in OOM error depending on the system workload.</li> <li> IOU_THRESHOLD   : A float representing the threshold for deciding whether boxes overlap too much with respect to IOU.</li> <li> SCORE_THRESHOLD   : A float representing the threshold for deciding when to remove boxes based on score.</li> <li> CLIP_BOXES   : If true, the coordinates of output nmsed boxes will be clipped to [0, 1]. If false, output the box coordinates as it is. Defaults to true.</li> <li> PAD_PER_CLASS   : If false, the output nmsed boxes, scores and classes are padded/clipped to max_total_size.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#optimizations-around-pre-and-post-processing-handling-in-preparator","title":"Optimizations Around Pre and Post Processing Handling in Preparator","text":"<ul> <li>PRE_PLUGIN<ul> <li>The  PRE_PLUGIN  field in the config is a boolean flag indicating the application of pre-processing optimizations if any. For example: In most of the computer vision models from the TensorFlow network we have seen that the preprocessing and post processing is part of the model graph making it dynamic in nature.</li> <li>Since we are working mostly on static models (due to AOT), this will cause issues in the model compilation. Making the PRE_PLUGIN True will help in handling such pre-processing in the model graph and making it AIC friendly.</li> <li>The preprocessing which is part of the model is handled and details regarding the same is tapped onto the log files.</li> <li> <p>The TF Optimizer looks for control-flow operators inside the model and replaces them with the appropriate/relevant operators.Major Optimizations include:</p> <ul> <li> <p>Switch-Merge Optimizer:</p> <ul> <li>This arises due to the \u201ctf.while_loop()\u201d and \u201ctf.cond()\u201d APIs. The \u201cprediction\u201d flag of the switch operator is evaluated, and the corresponding path is selected for the model. A switch operator whose prediction flag is not constant cannot be optimized.</li> </ul> </li> <li> <p>Loop Unrolling:</p> <ul> <li>This arises due to the \u201ctf.while_loop()\u201d APIs. The loop is unrolled to make a Directed Acyclic Graph (DAG) based on the number of loop iterations. It cannot be applied in cases where the number of loop iterations is not fixed or there is a nested loop.</li> </ul> </li> <li> <p>TensorArray Optimizations:</p> <ul> <li>This includes all tensor-array operators like (TensorArray, TensorArrayWrite, TensorArrayRead, TensorArrayGather, TensorArrayScatter, and so on). This optimization replaces tensor-array operators with relevant known and supported operators.</li> <li>For most of the NLP models, we have tokenization encodings of the inputs which we do in the host and pass the tokenized inputs to the model.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p></p> <ul> <li>POST_PLUGIN<ul> <li>This field is mostly applicable to the Object Detection CV models, at QAic we have two options to run the ABP + NMS. Smart-NMS and QDetect.</li> <li>SMART-NMS is a software application to help run the post processing part of the Object Detection (OD) models on host. Enabling this in the config will help add SMART-NMS to the model. Here we have intelligent way of identifying the post processing of the model if any and generating SMART-NMS specific model.</li> <li>QDetect is a known custom operator from QAic to enabling running the post processing of the OD models on the device. Enabling this in the config, will help generate add QDetect to the model.</li> <li>For NLP networks, the post-processing is decoding the tokens generated from model. Here we can use HuggingFace generation utils to do the post processing on the host.</li> </ul> </li> </ul>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#tutorial-converting-and-executing-a-yolov5-model-with-qaic","title":"Tutorial: Converting and executing a YoloV5 model with QAic","text":"<p>The following tutorial will demonstrate the end to end usage of Model Preparator Tool and the QAic Execution. This process begins with a trained source framework model, which is converted and built into a optimization model using Model Preparator, which are then executed on a QAic backend.</p> <p>The tutorial will use YoloV5 as the source framework model.</p> <p>The sections of the tutorial are as follows:</p> <ul> <li>Tutorial Setup: Download the Original Model</li> <li>Model Conversion: Generate the Optimized Model using Model Preparator APIs</li> <li>Executing Example Model: Run the model on QAic Backend.</li> <li>Plot Model Outputs on Sample Image</li> </ul>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#tutorial-setup","title":"Tutorial Setup","text":"<p>The tutorial assumes general setup instructions have been followed at Setup.</p> <pre><code>import os\ncwd = os.getcwd()\n\n# Steps to generate the models from ultralytics repo.\n!rm -rf yolov5\n\n# Clone the Ultralytics repo.\n!git clone --branch v6.0 --depth 1 https://github.com/ultralytics/yolov5.git\n\n# Install the requirements\n%cd yolov5\n!git checkout v6.0\n\n!pip3 install seaborn\n# Export the Original yolo models.\n!python3 -W ignore export.py --weights ./yolov5s.pt --include onnx --opset 13\n\n# Copy the original model to the relevant paths in sdk\n!cp yolov5s.onnx /opt/qti-aic/tools/qaic-pytools\n%cd ..\n!rm -rf yolov5/\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#model-conversion","title":"Model Conversion","text":"<p>After the model assets have been acquired the model can be converted to optimized QAic model, and subsequently built for use by an application. In the below example we will be generating the optimized QDetect model as explained in the above pre/post processing section.</p> <pre><code># Path to the installed SDK apps tools\n%cd /opt/qti-aic/tools/qaic-pytools\n\n# Run the preparator plugin with the relevant config file\n!python -W ignore qaic-model-preparator.py --config /opt/qti-aic/tools/qaic-pytools/model_configs/samples/preparator/public/yolov5_ultralytics_model_info_qdetect.yaml --silent\n\n# More details on various configs can be found here: /opt/qti-aic/tools/qaic-pytools/model_configs/samples/preparator/public\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#qaic-backend-execution","title":"QAic Backend Execution","text":"<p>Compile the model binary using the qaic-exec and execute it using the qaic-runner with the following commands:</p> <pre><code>!rm -rf yolo-binaries-qdetect\n\n!/opt/qti-aic/exec/qaic-exec -m=/opt/qti-aic/tools/qaic-pytools/WORKSPACE/yolov5s_preparator_aic100.onnx \\\n-aic-hw -onnx-define-symbol=batch_size,1 -aic-binary-dir=yolo-binaries-qdetect -convert-to-fp16 -compile-only\n</code></pre> <pre><code>!/opt/qti-aic/exec/qaic-runner -t yolo-binaries-qdetect -a 1 -d 0 --time 15\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#plot-model-outputs-on-sample-image","title":"Plot Model Outputs on Sample Image","text":"<p>Once we have the performance numbers, we will setup some model artifacts based on sample real image, Pre-Process, convert it to raw generate model outputs based on qaic-runner. Then using the generated outputs, we will plot it on the input images and get the final bounding boxes.</p> <pre><code># Imports\nimport os\nimport time\nimport cv2\nimport numpy as np\nimport torch\nfrom PIL import Image\n\ndef letterbox_image(image, size):\n    iw, ih = image.size\n    w, h = size\n    scale = min(w / iw, h / ih)\n    nw = int(iw * scale)\n    nh = int(ih * scale)\n\n    image = image.resize((nw, nh), Image.BICUBIC)\n    new_image = Image.new(\"RGB\", size, (128, 128, 128))\n    pad_left = (w - nw) // 2\n    pad_top = (h - nh) // 2\n    new_image.paste(image, (pad_left, pad_top))\n    return new_image, scale, pad_left, pad_top\n\ndef preprocessImg(image_path, input_h, input_w):\n\n    image_src = Image.open(image_path)\n    resized, scale, pad_left, pad_top = letterbox_image(image_src, (input_w, input_h))\n    img_in = np.transpose(resized, (2, 0, 1)).astype(np.float32)  # HWC -&gt; CHW\n    img_in = np.expand_dims(img_in, axis=0)\n    img_in /= 255.0\n    return img_in, np.array(image_src, dtype=np.uint8), scale, pad_left, pad_top\n</code></pre> <pre><code># Download the sample image and pre-process\n!rm -rf *.jpg\n!wget -c https://ultralytics.com/images/zidane.jpg\nimg_preproc, img_orig, scale, pad_left, pad_top = preprocessImg(\"zidane.jpg\", 640, 640)\nimg_preproc.tofile(\"input.raw\")\n</code></pre> <pre><code># Generate the outputs\n!/opt/qti-aic/exec/qaic-runner -t yolo-binaries-qdetect -i input.raw --write-output-dir yolo-output-qdetect -d 0\n</code></pre> <pre><code># Model artifacts to post process and plot the outputs on the image\ndef scale_coords(\n    coords, img_orig_h, img_orig_w, scale, pad_left, pad_top, xy_swap=True\n):\n    # Rescale coords (xyxy) from preprocessed img to original img resolution\n\n    if xy_swap:\n        coords[:, :, [0, 2]] -= pad_top  # y padding\n        coords[:, :, [1, 3]] -= pad_left  # x padding\n    else:\n        coords[:, :, [0, 2]] -= pad_left  # x padding\n        coords[:, :, [1, 3]] -= pad_top  # y padding\n    coords[:, :, :4] /= scale\n\n    # Clip bounding xyxy bounding boxes to image shape (height, width)\n    if xy_swap:\n        coords[:, :, 0] = coords[:, :, 0].clip(0, img_orig_h - 1)  # y1\n        coords[:, :, 1] = coords[:, :, 1].clip(0, img_orig_w - 1)  # x1\n        coords[:, :, 2] = coords[:, :, 2].clip(0, img_orig_h - 1)  # y2\n        coords[:, :, 3] = coords[:, :, 3].clip(0, img_orig_w - 1)  # x2\n    else:\n        coords[:, :, 0] = coords[:, :, 0].clip(0, img_orig_w - 1)  # x1\n        coords[:, :, 1] = coords[:, :, 1].clip(0, img_orig_h - 1)  # y1\n        coords[:, :, 2] = coords[:, :, 2].clip(0, img_orig_w - 1)  # x2\n        coords[:, :, 3] = coords[:, :, 3].clip(0, img_orig_h - 1)  # y2\n    return coords\n\n\nclass_names = [\n    \"person\",\n    \"bicycle\",\n    \"car\",\n    \"motorcycle\",\n    \"airplane\",\n    \"bus\",\n    \"train\",\n    \"truck\",\n    \"boat\",\n    \"traffic light\",\n    \"fire hydrant\",\n    \"stop sign\",\n    \"parking meter\",\n    \"bench\",\n    \"bird\",\n    \"cat\",\n    \"dog\",\n    \"horse\",\n    \"sheep\",\n    \"cow\",\n    \"elephant\",\n    \"bear\",\n    \"zebra\",\n    \"giraffe\",\n    \"backpack\",\n    \"umbrella\",\n    \"handbag\",\n    \"tie\",\n    \"suitcase\",\n    \"frisbee\",\n    \"skis\",\n    \"snowboard\",\n    \"sports ball\",\n    \"kite\",\n    \"baseball bat\",\n    \"baseball glove\",\n    \"skateboard\",\n    \"surfboard\",\n    \"tennis racket\",\n    \"bottle\",\n    \"wine glass\",\n    \"cup\",\n    \"fork\",\n    \"knife\",\n    \"spoon\",\n    \"bowl\",\n    \"banana\",\n    \"apple\",\n    \"sandwich\",\n    \"orange\",\n    \"broccoli\",\n    \"carrot\",\n    \"hot dog\",\n    \"pizza\",\n    \"donut\",\n    \"cake\",\n    \"chair\",\n    \"couch\",\n    \"potted plant\",\n    \"bed\",\n    \"dining table\",\n    \"toilet\",\n    \"tv\",\n    \"laptop\",\n    \"mouse\",\n    \"remote\",\n    \"keyboard\",\n    \"cell phone\",\n    \"microwave\",\n    \"oven\",\n    \"toaster\",\n    \"sink\",\n    \"refrigerator\",\n    \"book\",\n    \"clock\",\n    \"vase\",\n    \"scissors\",\n    \"teddy bear\",\n    \"hair drier\",\n    \"toothbrush\",\n]\n\n\ndef drawBoxes(\n    detections,\n    img_orig,\n    scale=1.0,\n    pad_left=0,\n    pad_top=0,\n    xy_swap=True,\n    filter_score=0.2,\n    line_thickness=None,\n    text_bg_alpha=0.0,\n):\n    detection_boxes = detections[0]  # [1, N, 4]\n    detection_scores = detections[1]  # [1, N]\n    detection_classes = detections[2]  # [1, N]\n    num_detections = detections[3]  # [1]\n\n    img_orig_h, img_orig_w = img_orig.shape[:2]\n\n    detection_boxes = scale_coords(\n        detection_boxes, img_orig_h, img_orig_w, scale, pad_left, pad_top, xy_swap\n    )\n\n    tl = line_thickness or round(0.002 * (img_orig_w + img_orig_h) / 2) + 1\n\n    assert (\n        detection_boxes.shape[0] == 1\n    ), \"Currently plotting for single batch size only.\"\n    for i in range(num_detections[0]):\n        box = detection_boxes[0][i]\n        score = detection_scores[0][i]\n        cls = detection_classes[0][i]\n        if score &lt; filter_score:\n            continue\n        if xy_swap:\n            y1, x1, y2, x2 = map(int, box)\n        else:\n            x1, y1, x2, y2 = map(int, box)\n        np.random.seed(int(cls) + 2020)\n        color = [np.random.randint(0, 255), 0, np.random.randint(0, 255)]\n        cv2.rectangle(\n            img_orig,\n            (x1, y1),\n            (x2, y2),\n            color,\n            thickness=max(int((img_orig_h + img_orig_w) / 600), 1),\n            lineType=cv2.LINE_AA,\n        )\n        label = \"%s %.2f\" % (class_names[int(cls)], score)\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=1)[0]\n        c2 = x1 + t_size[0] + 3, y1 - t_size[1] - 5\n        if text_bg_alpha == 0.0:\n            cv2.rectangle(img_orig, (x1 - 1, y1), c2, color, cv2.FILLED, cv2.LINE_AA)\n        else:\n            alphaReserve = text_bg_alpha\n            BChannel, GChannel, RChannel = color\n            xMin, yMin = int(x1 - 1), int(y1 - t_size[1] - 3)\n            xMax, yMax = int(x1 + t_size[0]), int(y1)\n            img_orig[yMin:yMax, xMin:xMax, 0] = img_orig[\n                yMin:yMax, xMin:xMax, 0\n            ] * alphaReserve + BChannel * (1 - alphaReserve)\n            img_orig[yMin:yMax, xMin:xMax, 1] = img_orig[\n                yMin:yMax, xMin:xMax, 1\n            ] * alphaReserve + GChannel * (1 - alphaReserve)\n            img_orig[yMin:yMax, xMin:xMax, 2] = img_orig[\n                yMin:yMax, xMin:xMax, 2\n            ] * alphaReserve + RChannel * (1 - alphaReserve)\n        cv2.putText(\n            img_orig,\n            label,\n            (x1 + 3, y1 - 4),\n            0,\n            tl / 3,\n            [255, 255, 255],\n            thickness=1,\n            lineType=cv2.LINE_AA,\n        )\n        print(\n            f\"X1:{x1}, Y1:{y1}, X2:{x2}, Y2:{y2}, Score:{score:.4f}, Cls_id:{class_names[int(cls)]}\"\n        )\n\n    return img_orig\n</code></pre> <pre><code>boxes = np.fromfile(\"yolo-output-qdetect/detection_boxes-activation-0-inf-0.bin\", dtype=\"float32\").reshape(1, 100, 4)\nscores = np.fromfile(\"yolo-output-qdetect/detection_scores-activation-0-inf-0.bin\", dtype=\"float32\").reshape(1, 100)\nclasses = np.fromfile(\"yolo-output-qdetect/detection_classes-activation-0-inf-0.bin\", dtype=\"int32\").reshape(1, 100)\ndetections = np.fromfile(\"yolo-output-qdetect/num_detections-activation-0-inf-0.bin\", dtype=\"int32\").reshape(1)\n\ndecoded_output = [boxes, scores, classes, detections]\nimage_src = Image.open(\"zidane.jpg\")\nimg_orig = np.array(image_src, dtype=np.uint8)\nimg_orig_plotted = drawBoxes(decoded_output, img_orig, scale, pad_left, pad_top, True)\nimg_orig_plotted = cv2.cvtColor(img_orig_plotted, cv2.COLOR_RGB2BGR)\ncv2.imwrite(\"output_qdetect.jpg\", img_orig_plotted)\n</code></pre> <pre><code># Plot the outputs on the image\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\nimg = np.array(Image.open(\"output_qdetect.jpg\"))\nfig, ax = plt.subplots(figsize=(15, 15))\nax.imshow(img)\n</code></pre> <p>For more examples on various models, Checkout our notebooks in SDK. Location to notebooks in SDK: <code>/opt/qti-aic/tools/qaic-pytools/docs/preparator/examples/notebooks</code></p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Compile-the-Model/","title":"Compile the Model","text":"<p>The AI/ML models compilation step compiles a pre-trained model defined in other formats (ONNX is preferred) into QPC (Qaic Program Container) format. This is required, since  Cloud AI devices works on this format, to run inference.</p> <p>A pretrained model can be compiled in three ways:</p> <ol> <li>Using qaic-exec (Binary executable shipped with the Apps SDK). This CLI tool has support for all the latest compiler flags.  </li> <li>Using High Level Python APIs </li> <li>Using C++ APIs </li> </ol>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Compile-the-Model/#before-compiling-model-into-qpc-format","title":"Before compiling model into QPC format:","text":"<ol> <li>Use ONNX format as input.</li> <li>Try to squash the original model into a single file ONNX format.</li> </ol>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Compile-the-Model/#compilation-using-qaic-exec","title":"Compilation using qaic-exec","text":"<p>The qaic-exec (QAic executor), a CLI tool, is used to compile the model. <code>qaic-exec</code> is a sophisticated tool with a number of arguments. </p> <p>qaic-exec CLI is located at <code>/opt/qti-aic/exec/qaic-exec</code></p> <p>The tutorials and models folder (#FIXME) provides several examples of <code>qaic-exec</code> usage. </p> <p>The help/usage command provides the extensive list of arguments and their descriptions for usage. <pre><code>/opt/qti-aic/exec/qaic-exec -h\n/opt/qti-aic/exec/qaic-exec --usage\n</code></pre></p> <p>Few of the frequently used arguments are additionally explained: <pre><code>-m=&lt;path&gt;;, -model=&lt;path&gt; # specifies the path of input ONNX model \n\n-onnx-define-symbol=&lt;sym, value&gt; # defines the names and values of the ONNX symbols that needed to be passed into the QPC.\n  For example\n -onnx-define-symbol=sequence,10 # For single symbol\n -onnx-define-symbol=sequence,10 -onnx-define-symbol=batch_size,8 # For more than one symbol\n\n-aic-num-cores=&lt;numCores&gt;\n  The Cloud AI cards can have a number of Neural Signal Processing cores (a.k.a AI cores) based on the SKU. The Inferencing workload can be distributed among different cores, so that they can execute concurrently and can produce more efficient inferencing. Refer to Tune Performance section on how to set this value.\n\n-ols= &lt;1,2,4,8.. numCores&gt;\n  Factor to increase splitting of network operations for more fine-grained parallelism. Refer to Tune Performance section on how to set this value.\n\n-mos= &lt;1,2,4,8.. numCores&gt;\n  Effort level to reduce on-chip memory usage. Refer to Tune Performance section on how to set this value.\n\n-multicast-weights\n  Reduce DDR bandwidth by loading weights used on multiple-cores only once and multicasting to other cores. \n\n-convert-to-fp16\n  mandatory flag, ensures that compiled QPC executes all floating point computations on the AIC 100 device is 16-bit precision. \n\n-batchsize=&lt;numBatch&gt;\n  batchsize refers to number of number of input samples that can be passed to the model during inferencing. Ideally a careful selection of batch size can facilitate better parallel processing and hence a better throughput from the device. Tune performance section expands on batchsize selection. \n\n-stats-batchsize=&lt;numBatch&gt;\n  Set this value to numBatch. Used in performance statistics reporting\n\n-aic-binary-dir=&lt;path&gt;\n  specifies the output QPC path.\n\n-aic-hw\n  Mandatory flag. This flag enables the QPC to be run on hardware.\n\n-compile-only \n  Mandatory flag, allows to only compile and produce the QPC format file for the model and does not run the model with random data. Use qaic-runner CLI to execute the QPC. \n\n-aic-hw-version=2.0\n  The version string must be passed as \"2.0\" which is the .\n\n-aic-profiling-type=&lt;stats|trace|latency&gt;\n  Used in device level profiling. Refer to Profiler notebook under tutorials. \n</code></pre></p> <p><code>qaic-exec</code> can also be used to dump the operators supported across onnx, tensorflow, pytorch, caffe or caffe2 ML frameworks.</p> <pre><code>/opt/qti-aic/exec/qaic-exec -operators-supported=&lt;onnx | tensorflow | pytorch | caffe | caffe2&gt;\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune-performance/","title":"Tune Performance","text":"<p>Throughput (inferences per second) and latency (time per inference) tuning techniques for Cloud AI platforms are discussed in this section. </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune-performance/#key-performance-parameters","title":"Key Performance Parameters","text":"<p>Cores, Batch Size, Instances (a.k.a activations) and Set-size are the key performance parameters that need to be tuned to extract the best performance. </p> <ul> <li>Cores and Batch Size are compile parameters.</li> <li>Instances and Set-size are runtime parameters.</li> </ul>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune-performance/#core","title":"Core","text":"<p>Cloud AI Platforms contain multiple AI cores depending on the SKU. Each AI core contains one or more scalar, vector and tensor engines which provide a rich instruction set to accelerate ML operations. A model can be compiled for one or more AI cores.   </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune-performance/#instance-aka-activation","title":"Instance (a.k.a activation)","text":"<p>Instance refers to the compiled binary of a model executing inferences on a set of AI cores. Let's say bert-large was compiled for 2 AI cores and the Cloud AI device has 14 AI cores. Each instance will run on 2 AI cores. Up to 7 instances (on 14 AI cores) can be executed in parallel.  </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune-performance/#batch-size","title":"Batch Size","text":"<p>Batch size refers to the number of input elements inferred by an instance. </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune-performance/#set-size","title":"Set-size","text":"<p>Set-size denotes the number of inferences that can be queued up on the host per activation. Set-size helps hide host side overhead by pipelining inferences. Models that require large input/output data to be transferred (from/to host and device) or some pre-processing/post-processing on the host can see throughput increases with increasing set-size up to a certain value beyond which the device utilization cannot be improved. </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune-performance/#instance-and-batch-size","title":"Instance and Batch size","text":"<p>The product of number of instances and batch size provides the total input samples that can be inferred in parallel on a single Cloud AI device. </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune-performance/#cores-and-instances","title":"Cores and Instances","text":"<p>The product of 'number of instances' and 'number of AI cores used per instance' cannot exceed the total number of AI cores available on the Cloud AI platform/card.</p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune-performance/#optimizing-for-best-throughput-least-latency","title":"Optimizing for Best Throughput / Least Latency","text":"<p>Model Configurator is a hardware-in-loop test tool that runs through various configurations and identifies the best configuration for a model. Model configurator tool offers two workflows - highest throughput and least latency. </p> <p>Refer to the Performance-Tuning Tutorial #1 - CV and Tutorial #2 - NLP for step by step walkthrough for performance tuning. </p> <p>Refer to Performance tuning tutorial for the workflow for optimizing for best throughput and least latency. </p> <p>For least latency configuration, batch-size should be set to 1. Set-size of 1 provides the least latency. </p> <p>Higher set-sizes may improve throughput significantly for a slight increase in latency for models that require some host side pre-processing (eg. CV models).  </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune-performance/#general-performance-tuning-observations","title":"General Performance Tuning Observations","text":"<p>Now that we have covered the key performance tuning parameters , there are some general observations that would help developers compile models for best performance. </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune-performance/#throughput-and-latency-vs-batch-size","title":"Throughput and Latency vs Batch-size","text":"<p>For an instance on a fixed number of cores, increasing the batch size (BS) from 1 will typically improve throughput. Increasing beyond the optimal BS will cause the performance to drop. </p> <p></p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune-performance/#throughput-vs-cores-and-instances","title":"Throughput vs Cores and Instances","text":"<p>Based on the total number of AI cores available on the device, a few combinations of cores and instances exist and some of these combinations provide better performance that others. </p> <p>Lets use the Standard SKU of Cloud AI 100 with 14 AI cores to illustrate this. Throughput for different combinations of cores and instances are shown in the figure below. The Batch-size is fixed. For example: C1 x I12 represents the model compiled on 1 core and 12 instances deployed on 12 cores. </p> <p></p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Inference-Profiling/","title":"Inference Profiling","text":"<p>Cloud AI supports both system and device level profiling to aid developers to identify performance bottlenecks. System profiling can be performed without any changes to the QPC while device profiling requires the model to be recompiled. </p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Inference-Profiling/#system-level-profiling","title":"System Level Profiling","text":"<p>System level profiling includes the breakdown of the inference time between Application, linux runtime, kernel mode driver and device processing. This can provide insights into the inference time spent on the host vs device per inference. Developers can optimize their application or the model with this information. </p> <p>Refer to Profiler notebooks in the tutorials section for the complete system level profiling workflow for a model using the qaic-runner CLI. </p> <p>Profiling using Python APIs is also supported. Refer to this page.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Inference-Profiling/#device-level-profiling","title":"Device Level Profiling","text":"<p>Device level profiling is aimed at advanced developers looking to identify bottlenecks in inference execution on the device. This requires a good understanding of the AI core and SoC architecture. There are 3 key features that would be interesting to developers -</p> <ul> <li>Memory metrics - This provides a compiler estimate of the usage of on-board DDR vs VTCM (Vector tightly coupled memory) for a model.</li> <li>Summary view - This provides a histogram of the operations, total time taken by every operation, where the operands are stored (DDR vs VTCM), effective usage of the individual IP blocks in the AI cores etc. This feature is for debug only as it  may impact performance based on the size of the model. </li> <li>Timeline view - This provides a timeline view of all the operations executing across all IP blocks from start of an inference till the end. This feature is primarily used to zoom into the operations to understand bottlenecks. This feature is for debug only as it will impact performance. </li> </ul> <p>Refer to Profiler Jupyter notebook for the complete device level profiling workflow for a model using the qaic-exec and qaic-runner CLI. </p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/","title":"Triton Inference Server","text":"<p>Triton Inference Server is an open source inference serving software that streamlines AI inferencing. </p> <p>Cloud AI SDK enables two backends for inference execution workflow.  These backends, once used on a host with AI 100 cards, will detect the AI 100 cards during initialization and route the inferencing call (requested to Triton server) to the hardware.</p> <ul> <li>onnxruntime_onnx as platform with QAic EP(execution provider) for deploying ONNX graphs.</li> <li>QAic as a customized C++ backend for deploying compiled binaries optimized for AIC.</li> </ul> <p> </p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#creating-an-ai-100-backend-enabled-triton-docker-image-using-ai-100-development-kits","title":"Creating an AI 100 backend enabled Triton Docker image using AI 100 development kits","text":"<p>In order to add customized backends (to process inferencing on AI 100 hardware) into a vanilla Triton server image, we need to run a few scripts by passing sdk_path as parameters. <code>docker-build.sh</code> script will generate a Docker image as output.This script is a part of Cloud AI Aps SDK contents and can be run after unzipping it.</p> <p></p> <p><pre><code>sample&gt; cd &lt;/path/to/app-sdk&gt;/tools/docker-build\n\nsample&gt; python3 build_image.py --tag 1.16.1.29-triton --log_level 2 --user_specification_file /opt/qti-aic/tools/docker-build-gen2/sample_user_specs/user_image_spec_triton_model_repo.json --apps-sdk /apps/sdk/path --platform-sdk /platform/sdk/path\n</code></pre> The above command may take 15-20 minutes to complete and generate incremental images for Triton Docker image in local Docker repository.</p> <p><pre><code>sample&gt; docker image ls\nREPOSITORY                                                                                                                      TAG                 IMAGE ID       CREATED         SIZE\nqaic-x86_64-triton-release-py38-qaic_platform-qaic_apps-pybase-onnxruntime-triton-pytools-triton_model_repo               1.16.1.29-triton          a0968cf3711b   3 days ago      28.2GB\nqaic-x86_64-triton-release-py38-qaic_platform-qaic_apps-pybase-onnxruntime-triton-pytools                                 1.16.1.29-triton          038dc80fd8e4   3 days ago      27.1GB\nqaic-x86_64-triton-release-py38-qaic_platform-qaic_apps-pybase-onnxruntime-triton                                         1.16.1.29-triton          760fb9dc5314   3 days ago      24GB\nqaic-x86_64-triton-release-py38-qaic_platform-qaic_apps-pybase-onnxruntime                                                1.16.1.29-triton          a47266156b7f   3 days ago      23.9GB\nqaic-x86_64-triton-release-py38-qaic_platform-qaic_apps-pybase                                                            1.16.1.29-triton          d620a1bdb6b6   3 days ago      20.1GB\nqaic-x86_64-triton-release-py38-qaic_platform-qaic_apps                                                                   1.16.1.29-triton          8c87eb44f2db   3 days ago      15.3GB\nqaic-x86_64-triton-release-py38-qaic_platform                                                                             1.16.1.29-triton          e3ba2ce282c1   3 days ago      14.7GB\nqaic-x86_64-triton-release-py38                                                                                           1.16.1.29-triton          73b225d7e358   3 days ago      14.2GB\nqaic-x86_64-triton-release                                                                                                1.16.1.29-triton          914fa376e865   3 days ago      14.1GB\nqaic-x86_64-triton                                                                                                        1.16.1.29-triton          2090680d4d59   3 days ago      14.1GB\n</code></pre> Docker can be launched using docker <code>run</code> command passing the desired image name. Please note the shared memory argument <code>--shm-size</code> for supporting ensembles and python backends.</p> <pre><code>sample&gt; docker run -it --rm --privileged --shm-size=4g --ipc=host --net=host &lt;triton-docker-image-name&gt;:&lt;tag&gt; /bin/bash\n\nsample&gt; docker ps\nCONTAINER ID   IMAGE                                                                                                            COMMAND                  CREATED      STATUS      PORTS     NAMES\nb88d5eb98187   qaic-x86_64-triton-release-py38-qaic_platform-qaic_apps-pybase-onnxruntime-triton-pytools-triton_model_repo      \"/opt/tritonserver/n\u2026\"   2 days ago   Up 2 days             thirsty_beaver\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#creating-a-model-repository-and-configuration-file","title":"Creating a model repository and configuration file","text":"<p>The model configuration file specifies the execution properties of a model. It indicates input/output structure, backend, batchsize, parameters, etc.  User needs to follow Triton's model repository and model configuration rules while defining a config file.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#model-configuration-onnxruntime","title":"Model configuration - onnxruntime","text":"<p>For onnxruntime configuration, platform should be set to <code>onnxruntime_onnx</code>. The <code>use_qaic</code> parameter should be passed and set to true.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#ai-100-specific-parameters","title":"AI 100 specific parameters","text":"<p>Parameters are user-provided key-value pairs which Triton will pass to backend runtime environment as variables and can be used in the backend processing logic.</p> <ul> <li>config : path for configuration file containing compiler options.</li> <li>device_id : id of AI 100 device on which inference is targeted. (not mandatory as the server auto picks the available device)</li> <li>use_qaic : flag to indicate to use qaic execution provider.</li> <li>share_session : flag to enable the use of single session of runtime object across model instances.</li> </ul> <p>sample example of a <code>config.pbtxt</code></p> <pre><code>name: \"resnet_onnx\"\nplatform: \"onnxruntime_onnx\"\nmax_batch_size : 16\ndefault_model_filename : \"aic100/model.onnx\"\ninput [\n  {\n    name: \"data\"\n    data_type: TYPE_FP32\n    dims: [3, 224, 224 ]\n  }\n]\noutput [\n  {\n    name: \"resnetv18_dense0_fwd\"\n    data_type: TYPE_FP32\n    dims: [1000]\n  }\n]\nparameters [\n  {\n    key: \"config\"\n    value: { string_value: \"1/aic100/resnet.yaml\" }\n  },\n  {\n    key: \"device_id\"\n    value: { string_value: \"0\" }\n  },\n  {\n    key: \"use_qaic\"\n    value: { string_value: \"true\" }\n  },\n  {\n    key: \"share_session\"\n    value: { string_value: \"true\" }\n  }\n]\ninstance_group [\n  {\n    count: 2\n    kind: KIND_MODEL\n  }\n]\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#model-configuration-qaic-backend","title":"Model configuration - qaic backend","text":"<p>For qaic backend configuration, the <code>backend</code> parameter should be set to <code>qaic</code>.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#ai-100-specific-parameters_1","title":"AI 100 specific parameters","text":"<p>Parameters are user-provided key-value pairs which Triton will pass to backend runtime environment as variables and can be used in processing logic of backend.</p> <p>Parameters are user-provided key-value pairs which Triton will pass to backend runtime environment as variables and can be used in processing logic of backend.</p> <ul> <li>qpc_path : path for compiled binary of model.(programqpc.bin) (if not provided the server searches for QPC in the model folder)</li> <li>device_id : id of AI 100 device on which inference is targeted. device is set 0 (not mandatory as the server auto picks the available device)</li> <li>set_size : size of inference queue for runtime,default is set to 20</li> <li>no_of_activations : flag to enable multiple activations of a model\u2019s network,default is set to 1</li> </ul> <p>sample example of a <code>config.pbtxt</code></p> <pre><code>name: \"yolov5m_qaic\"\nbackend: \"qaic\"\nmax_batch_size : 4\ndefault_model_filename : \"aic100/model.onnx\"\ninput [\n  {\n    name: \"images\"\n    data_type: TYPE_FP32\n    dims: [3, 640, 640 ]\n  }\n]\noutput [\n  {\n    name: \"feature_map_1\"\n    data_type: TYPE_FP32\n    dims: [3, 80, 80, 85]\n  },\n  {\n    name: \"feature_map_2\"\n    data_type: TYPE_FP32\n    dims: [3, 40, 40, 85]\n  },\n  {\n    name: \"feature_map_3\"\n    data_type: TYPE_FP32\n    dims: [3, 20, 20, 85]\n  }\n]\nparameters [\n  {\n    key: \"qpc_path\"\n    value: { string_value: \"/path/to/qpc\" }\n  },\n  {\n    key: \"device_id\"\n    value: { string_value: \"0\" }\n  }\n]\ninstance_group [\n  {\n    count: 2\n    kind: KIND_MODEL\n  }\n]\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#launching-triton-server-inside-container","title":"Launching Triton server inside container","text":"<p>To launch Triton server, execute the <code>tritonserver</code> binary within Triton Docker with the model repository path.</p> <pre><code>/opt/tritonserver/bin/tritonserver --model-repository=&lt;/path/to/repository&gt;\n</code></pre> <p></p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#supported-features","title":"Supported Features","text":"<ul> <li>Model Ensemble</li> <li>Dynamic Batching</li> <li>Auto device-picker</li> <li>Support for ARM64</li> <li>Support for auto complete configuration</li> <li>LLM support for LlamaForCausalLM, AutoModelForCausalLM categories.</li> </ul>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#triton-config_generation-tool","title":"Triton Config_generation tool","text":"<p>Model configuration file <code>config.pbtxt</code> is required for each model to run on the Triton server. The <code>triton_config_generator.py</code> tool helps to generate a minimal model configuration file if the <code>programqpc.bin</code> or <code>model.onnx</code> file is provided. The script can be found in \"/opt/qti-aic/integrations/triton/release-artifacts/config-generation-script\" path inside the container.</p> <p>The script takes three arguments:</p> <ul> <li>--model_repository: Model repository for which config.pbtxt needs to be generated (QAic backend)</li> <li>--all: Generate config.pbtxt for ONNX (used with --model-repository)</li> <li>--model_path: QAic model or ONNX model file path for which model folder needs to be generated.</li> </ul> <p>The <code>model_repository</code> argument can be passed, and the script goes through the models and generates <code>config.pbtxt</code> for models that do not contain config (the --all option needs to be passed if config needs to be generated for ONNX models) or model path can be provided to generate model folder structure with <code>config.pbtxt</code> using random model names.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#examples","title":"Examples","text":"<p>Triton example applications are released as part of the Cloud AI Apps SDK. Inside the Triton Docker container the sample model repositories are available at \"/opt/qti-aic/aic-triton-model-repositories/\" - <code>--model-repository</code> option can be used to launch the models.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#stable-diffusion","title":"Stable diffusion","text":"<p>1) If we built the Docker with Triton model repo application then the stable diffusion model repo is available at this path: \"/opt/qti-aic/aic-triton-model-repositories/ensemble-stable-diffusion\".</p> <p>2) To generate a model repo inside a Triton container:</p> <ul> <li>Run the <code>generate_SD_repo.py</code> script. The script is located at \"/opt/qti-aic/integrations/triton/release-artifacts/stable-diffusion-ensemble\", which will create a ensemble-stable-diffuison model repo</li> </ul> <p>3) Start the Triton server \"/opt/tritonserver/bin/tritonserver --model-repository=/path/to/ensemble-stable-diffusion\"     Example: \"/opt/tritonserver/bin/tritonserver --model-repository=/opt/qti-aic/aic-triton-model-repositories/ensemble-stable-diffusion\"</p> <p>4) Triton server takes about 2 minutes to start on the first go as it needs to compile QPC.</p> <p>5) Run the client_example.py from the same container for testing purpose or client_example.py can also be copied to Triton client container and executed from there.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#triton-llm","title":"Triton LLM","text":"<ul> <li>LLM serving through Triton is enabled using <code>triton-qaic-backend-python</code></li> <li>It supports execution of QPC binaries for Causal models, KV Cache models of LlamaForCausalLM, AutoModelForCausalLM categories.</li> <li>It supports two modes of server to client response - batch, decoupled (stream). In batch response mode, all of the generated tokens are cached and composed as a single response at the end of decode stage. In decoupled (stream) response mode, each generated token is sent to client as a separate response.</li> <li>Currently we include sample configurations for Mistral and Starcoder models.</li> <li>Sample client scripts are provided to test kv, causal models in stream-response, batch-response transaction modes.</li> </ul>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#instructions-to-launch-llm-models-on-triton-server","title":"Instructions to launch LLM models on Triton server","text":""},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#launch-triton-server-container","title":"Launch Triton server container","text":"<p><pre><code>docker run -it --shm-size=4g --rm --privileged --net=host -v /path/to/custom/models/:/path/to/custom/models/ &lt;triton-docker-image-name&gt;:&lt;tag&gt; bash\n</code></pre> - qefficient virtual environment comprises compatible packages for compilation/execution of a wide range of LLMs. Activate this environment within the container. <pre><code>. /opt/qeff-env/bin/activate\n</code></pre></p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#generating-a-model-repository","title":"Generating a model repository","text":""},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#sample-models","title":"Sample models","text":"Model Folder Model Type Response Type starcoder_15b Causal Batch starcoder_decoupled Causal Decoupled (Stream) mistral_7b KV cache Batch mistral_decoupled KV cache Decoupled (Stream) <ul> <li>Pass in the QPC to <code>generate_llm_model_repo.py</code> script available at /opt/qti-aic/integrations/triton/release-artifacts/llm-models/ within Triton container. <pre><code>python generate_llm_model_repo.py --model_name mistral_7b --aic_binary_dir &lt;path/to/qpc&gt; --python_backend_dir /opt/qti-aic/integrations/triton/backends/qaic/qaic_backend_python/\n</code></pre></li> </ul>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#custom-models","title":"Custom models","text":"<ul> <li><code>generate_llm_model_repo.py</code> script uses a template to auto-generate config for custom models. Configure required parameters such as <code>use_kv_cache</code>, <code>model_name</code>, decoupled transaction policy through command line options to the script. Choosing <code>model_type</code> will configure <code>use_kv_cache</code> parameter. If not provided, it will be determined by loading QPC object which may take several minutes for large models. This creates a model folder for  in  /opt/qti-aic/integrations/triton/release-artifacts/llm-models/llm_model_dir <pre><code>python generate_llm_model_repo.py --model_name &lt;custom_model&gt; \\\n                                  --aic_binary_dir &lt;path/to/qpc&gt; \\\n                                  --hf_model_name \\\n                                  --model_type &lt;causal/kv_cache&gt; \\\n                                  --decoupled\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#launch-tritonserver-and-load-models","title":"Launch tritonserver and load models","text":"<ul> <li>Pre-requisite: Users need to get access for necessary models from Hugging Face and login with Hugging Face token using 'huggingface-cli login` before launching the server.</li> <li>Launch the Triton server with llm_model_dir. <pre><code>/opt/tritonserver/bin/tritonserver --model-repository=&lt;path/to/llm_model_dir&gt;\n</code></pre></li> </ul>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#running-the-client","title":"Running the client","text":""},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#launch-client-container","title":"Launch client container","text":"<p><pre><code>docker run -it --rm -v /path/to/unzipped/apps-sdk/integrations/triton/release-artifacts/llm-models/:/llm-models --net=host nvcr.io/nvidia/tritonserver:22.12-py3-sdk bash\n</code></pre> - Once the server has started you can run example Triton client (client_example_kv.py/client_example_causal.py) provided to submit inference requests to loaded models. - Decoupled model transaction policy is supported only over gRPC protocol. Therefore, decoupled models (stream response) use gRPC clients whereas batch response mode uses HTTP client as a sample. <pre><code># mistral_decoupled\npython /llm-models/tests/stream-response/client_example_kv.py --prompt \"My name is\"\n\n# mistral_decoupled (QPC compiled for batch_size=2)\npython /llm-models/tests/stream-response/client_example_kv.py --prompt \"My name is|Maroon bells\"\n\n# mistral_7b\npython /llm-models/tests/batch-response/client_example_kv.py --prompt \"My name is\"\n\n# starcoder_decoupled\npython /llm-models/tests/stream-response/client_example_causal.py --prompt \"Write a python program to print hello world\"\n\n# starcoder_15b\npython /llm-models/tests/batch-response/client_example_causal.py --prompt \"Write a python program to print hello world\"\n</code></pre></p> <p>Note: For batch-response tests, the default network timeout in <code>client_example_kv.py</code>, <code>client_example_causal.py</code> is configured as 10 min (600 sec), 100 min (6000 sec) respectively.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/","title":"Model Execution","text":"<p>There are 4 ways to call inference on Qualcomm Cloud AI 100 device.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#1-cli-api","title":"1. CLI API","text":"<p><code>qaic-runner</code> is a CLI (command line inferface) tool designed to run inference on the device using precompiled binary (also called QPC (Qaic Program Container) binary). It provides various options and functionalities to facilitate inference and performance/benchmarking analysis. In this document, we will discuss the different options available in <code>qaic-runner</code> tool, their default values and provide examples for their usage.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#options-and-default-values","title":"Options and Default values","text":"Parameter Description Default <code>-d, --aic-device-id &lt;id&gt;</code> Specify AIC device ID. 0 <code>-D, --dev-list &lt;qid&gt;[:&lt;qid&gt;]</code> Map of device IDs for a multi-device network. 0[:1] <code>-d, --aic-device-id &lt;id&gt;</code> AIC device ID, default Auto-pick 0 <code>-D, --aic-device-map &lt;qid&gt;[:&lt;qid&gt;]</code> Map of Device IDs for multi-device network, 0[:1] <code>-t, --test-data &lt;path&gt;</code> Location of program binaries <code>-i, --input-file &lt;path&gt;</code> Input filename from which to load input data. Specify multiple times for each input file.     If no -i is given, look for available inputs in bindings.json at the -t directory. If bindings.json is not available, random input will be generated. <code>-n, --num-iter &lt;num&gt;</code> Number of iterations, 40 <code>--time &lt;t&gt;</code> Duration (in seconds) for which to submit inferences <code>-l, --live-reporting</code> Enable Live reporting periodic at 1 sec interval off <code>-r, --live-reporting-period</code> Set Live Reporting Period in Ms 1000 <code>-s  --stats</code> Enable Live Profiling Stats reporting periodically at 1 sec interval <code>-a, --aic-num-of-activations &lt;num&gt;</code> Number of activations 1 <code>--aic-profiling-start-iter &lt;num&gt;</code> Profiling Start Iteration 0 <code>--aic-profiling-start-delay &lt;num&gt;</code> Profiling Start delay (in milliseconds). Profiling will start after given delay period has elapsed <code>--aic-profiling-num-samples &lt;num&gt;</code> Profiling Num Samples to save to file 1 <code>--aic-profiling-format &lt;level&gt;</code> Deprecated DEF <code>--aic-profiling-type &lt;type&gt;</code> Profiling Type, <code>'stats'\\|'trace'\\|'latency'</code> for legacy profiling and <code>'trace_stream' \\| 'latency_stream'</code> for stream profiling. Set multiple times for multiple formats none <code>--aic-profiling-duration &lt;num&gt;</code> Profiling duration to run profiling for (in ms). After starting profiling, it will stop at the expiry of profiling duration <code>--aic-profiling-sampling-rate &lt;num&gt;</code> Profiling sampling rate [<code>full/half/fourth/eighth/sixteenth</code>]. Programs will generate profiling samples at the requested rate.  To profile all samples select full, for every second sample select half and so on full <code>--aic-profiling-reporting-rate &lt;num&gt;</code> Profiling report generation rate (in ms) [<code>500/1000/2000/4000</code>]. Profiling report will be generated at every requested interval for profiling duration 500 <code>--aic-profiling-out-dir &lt;path&gt;</code> Location to save files, dir should exist and be writable '.' <code>--write-output-start-iter &lt;num&gt;</code> Write outputs start iteration 0 <code>--write-output-num-samples &lt;num&gt;</code> Number of outputs to write 1 <code>--write-output-dir &lt;path&gt;</code> Location to save output files, dir should exist and be writable '.' <code>--aic-lib-path DEPRECATED</code> Deprecated, please set env variable QAIC_LIB to the full path of the custom library, by default loads libQAic.so from install location <code>--aic-batch-input-directory</code> Batch mode: process all files from input directory. Only the networks with single input file are currently supported DEF <code>--aic-batch-input-file-list</code> Batch mode: Specify an input file containing comma-separated absolute path for buffers. Each line is 1 inference and must have number and size of buffers required by program DEF <code>--aic-batch-max-memory &lt;mb&gt;</code> Batch mode: Limit memory usage when loading files, provide parameter in Mb 1024 <code>--submit-timeout &lt;num&gt;</code> Time to wait for an inference request completion on kernel. default 0 ms. When 0, kernel defaults to 5000ms <code>--submit-retry-count &lt;num&gt;</code> Number of wait-call retries when an inference request times out. 5 <code>--unbound-random</code> When populating random values in buffer, do not consider input buffer format and fill each byte with random input between 0 to 255. This can result in unexpected behavior from certain network. <code>--dump-input-buffers</code> Dump input buffers used in benchmarking mode <code>-S, --set-size &lt;num&gt;</code> Set Size for inference loop execution, min:1 10 <code>-T, --aic-threads-per-queue</code> Number of threads per queue 4 <code>--auto-batch-input</code> Automatically batch inputs to meet batchsize requirements of network. Inputs should be for Batch size 1 1 <code>-p, --pre-post-processing</code> Pre-post processing [<code>on\\|off</code>] on <code>-v, --verbose</code> Verbose log from program <code>-h, --help</code> help"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#usage-examples","title":"Usage Examples","text":""},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#1-running-inference-with-random-inputs","title":"1. Running inference with Random inputs","text":"<p>Example: <pre><code>sudo /opt/qti-aic/exec/qaic-runner -t /path/to/qpc  -a 3 -n 5000 -d 0 -v\n</code></pre></p> <ul> <li>In this example we are using a precompiled binary that is already generated. <ul> <li>3 activations - activations here refers to number of instances of network you want to run on the device. In this case 3 copies of the network can run parallely on the device. </li> <li>Lets assume each network was compiled with 4 cores (<code>sudo /opt/qti-aic/tools/qaic-qpc validate -i /path/to/qpc/programqpc.bin</code> look for Number of NSP required value in output of this command.)<ul> <li>Make sure the device you are using has at least 12 i.e. 3x4 cores Free.</li> </ul> </li> <li>Since input is not provided we are feeding a randomly generated input (with appropriate dimensions and type inferred from qpc) to the device. </li> <li>This single randomly generated input is used for 5000 inferences.</li> <li>on device ID <code>0</code>. Check your device ID using <code>/opt/qti-aic/tools/qaic-util -q</code> CLI tool, look for QID value in it.</li> <li>verbose log is enabled with <code>-v</code> option.</li> </ul> </li> <li>Tool in this configuration can be used to measure performance. </li> </ul>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#2-running-inference-on-a-set-of-inputs","title":"2. Running inference on a set of inputs","text":"<p>Before running inference, it is necessary to convert the inputs to the appropriate format based on input size and type.  Look at this Jupyter notebook example</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#3-generating-dumps-for-latency-capture","title":"3. Generating dumps for latency capture","text":"<ul> <li><code>aic-profiling-format latency</code></li> <li><code>aic-profiling-out-dir</code> : output directory for latency capture (needs to exist before this command is run)</li> <li><code>aic-profiling-start-iter</code> : Set this value high enough to start capturing samples after device warmup </li> <li><code>aic-profiling-num-samples</code> : # of samples to be captured. Can be set greater than # of inferences</li> </ul> <p>Example command to generate latency stats<pre><code>!/opt/qti-aic/exec/qaic-runner -t ./BERT_LARGE -a 8 -S 1 -d 0 \\ #-i inputFiles/input.raw \\\n--aic-profiling-format latency --aic-profiling-out-dir ./BERT_LARGE_STATS \\\n--aic-profiling-start-iter 100 --aic-profiling-num-samples 99999 --time 20 \n</code></pre> Look at this Jupyter notebook example</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#conclusion","title":"Conclusion","text":"<p><code>qaic-runner</code> CLI tool is primarily intended for performance testing purposes. For actual inference tasks, it is recommended to utilize the Python or C++ APIs, depending on your preferred technology stack. </p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#2-use-python-apis","title":"2. Use Python APIs","text":"<p>Refer to Python API</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#3-use-c-apis","title":"3. Use C++ APIs","text":"<p>Refer to C++ API</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#4-use-onnxrt","title":"4. Use Onnxrt","text":"<p>Using Qualcomm Cloud AI 100 as execution provider in onnxrt.</p>"},{"location":"Getting-Started/Installation/","title":"Introduction","text":"<p>Developers can access Qualcomm Cloud AI hardware through cloud instances or by purchasing servers equipped with Qualcomm Cloud AI hardware. </p>"},{"location":"Getting-Started/Installation/#cloud-instances","title":"Cloud Instances","text":"<p>Cloud AI 100 cards are now available at 2 Cloud service providers - Amazon Web Services (AWS) and Cirrascale Cloud Services. The AI 100 accelerator SKUs and instance configurations offered at these providers can vary. </p> <ul> <li>Refer to Getting Started on AWS for more information on the instances available at AWS.</li> <li>Cirrascale Cloud Services have multiple configurations (from 1 to 8 Pro AI 100 accelerators per instance) for the user to choose from. </li> </ul> Note <p>Developers using cloud instances can skip the rest of the installation section. Click here to go to the next section, Inference Workflow</p>"},{"location":"Getting-Started/Installation/#on-premise-servers","title":"On-Premise Servers","text":"<p>Developers with on-premise servers need to work with system administators to ensure Cloud AI SDKs are installed and verified properly. It is recommended for developers and system admins to go through the installation section in its entirety. </p>"},{"location":"Getting-Started/Installation/#installation","title":"Installation","text":"<p>The Platform SDK (x86-64 and ARM64) and Apps SDK (x86-64 only) are targeted for Linux-based platforms. The SDKs can be installed natively on Linux operating systems. Container and orchestration are also supported through Docker and Kubernetes. Virtual machines, including KVM, ESXi, and Hyper-V, are also supported. This section covers:</p> <ul> <li>Installation of the SDKs across multiple Linux distributions</li> <li>Building a docker image with the SDKs and third-party packages for a seamless execution of QAic inference tools/workflow</li> <li>Setting up KVM, ESXi, and Hyper-V, and installation of SDKs</li> </ul>"},{"location":"Getting-Started/Installation/#compilation-and-execution-modes","title":"Compilation and Execution modes","text":"<p>Apps and Platform SDKs enable just-in-time(JIT) or ahead-of-time(AOT) compilation and execution on x86-64 platforms while only AOT compilation/execution is supported on ARM64. </p> <p>In JIT mode, compilation and execution are tightly coupled and require Apps and Platform SDKs to be installed on the same system/VM.</p> <p>In AOT mode, compilation and execution are decoupled. Networks can be compiled ahead-of-time on x86-64 (with Apps SDK only) and the compiled networks can be deployed  on x86-64 or ARM64 with Platform SDK.</p> <p>Both JIT and AOT are supported on x86-64 when Apps and Platform SDK are installed on the same server/VM. </p>"},{"location":"Getting-Started/Installation/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>CPU - Server grade x86 or ARM multi-core CPU </li> <li>RAM - minimum 512 GB (recommend 768GB or higher)<ul> <li>Required for the compilation of large models and corresponding configurations like batch size, context length etc. </li> </ul> </li> <li>Storage - minimum 1TB (recommend 4TB)<ul> <li>Large storage is recommended to store multiple models (and associated artifacts), serialized engines etc. </li> </ul> </li> </ul>"},{"location":"Getting-Started/Installation/#supported-operating-systems-hypervisors-and-platforms","title":"Supported Operating Systems, Hypervisors, and Platforms","text":"<p>The Cloud AI Platform SDK is compatible with the following operating systems (OS) and platforms.</p>"},{"location":"Getting-Started/Installation/#operating-systems","title":"Operating Systems","text":"Operating systems Kernel Ubuntu 20.04 Default Kernel (GA or HWE) Ubuntu 22.04 Default Kernel (GA or HWE) Ubuntu 24.04 Default Kernel (GA or HWE) Red Hat Enterprise Linux 8.9 Default Kernel Red Hat Enterprise Linux 9.3 Default Kernel AWS Linux 2 Amazon 5.10 Kernel AWS Linux 2023 Default Kernel <code>Note1</code>: Arm is a trademark of Arm Limited (or its subsidiaries) in the US and/or elsewhere. <code>Note2</code>: Apps SDK is available only for x86-64 platforms. <code>Note3</code>: Ultra cards consume 4 DRM/Accel device resources per card. Kernels prior to 6.2 are limited to 64 DRM device resources for the entire system. Using Ubuntu 22.04 with the HWE kernel is recommended as the resource limit is 256 Accel devices. Deployments with large numbers of Ultra cards that do not follow this recommendation may not be able to use all the combined hardware resources of the Ultra cards. <code>Note4</code>: AWS Linux 2 support available only for x86-64 platforms."},{"location":"Getting-Started/Installation/#hypervisors","title":"Hypervisors","text":"<p>Cloud AI only supports PCIe passthrough to a virtual machine. This means that the virtual machine completely owns the Cloud AI device. A single Cloud AI device cannot be shared between virtual machines or between a virtual machine and the native host. </p> Hypervisor x86-64 ARM64 KVM \u2714 \u2717 Hyper-V \u2714 \u2717 ESXi \u2714 \u2717 Xen \u2714 \u2717 <code>Note</code> Cloud AI SDKs are not required to be installed on the hypervisor. All the Cloud AI software/SDKs are installed on the guest VMs."},{"location":"Getting-Started/Installation/AWS/aws/","title":"Getting started on AWS instances","text":"<p>Qualcomm Cloud AI 100 inference accelerator instances are now available on the AWS Cloud. </p>"},{"location":"Getting-Started/Installation/AWS/aws/#dl2q","title":"DL2q","text":"<p>DL2q is an accelerated computing instance powered by Cloud AI 100 Standard accelerator cards that can be used to cost-efficiently deploy deep learning (DL) inference workloads. </p> <p>Details regarding the DL2q instance can be found here. </p> <p>The latest AMI from Qualcomm is <code>\u201cDeep Learning Base Qualcomm AMI (Amazon Linux 2) 1.12.0.88P 1.14.2.0A SDK\u201d ami-0d0b22ddccd54aec0</code>. Email your 12-digit AWS account number along with the intended use case (CV, NLP, GenAI etc) to <code>qualcomm_dl2q_ami@qti.qualcomm.com</code>. Qualcomm will provide access the latest AMI. </p> <p>We recommend users to attach 500GB of EBS GP3 storage to the instance to start with (especially for running LLMs).  </p> Note <p>Users of DL2q instance may need to request AWS to increase the vCPU limit to be able to spin up the first DL2q instance from an AWS account. </p> <p>SSH to the instance and add user to the <code>qaic</code> group to be able to run Cloud AI SDK tools without superuser (sudo) privileges.      <pre><code>sudo usermod -aG qaic $USER\n</code></pre> Close and re-open SSH session for the change to take effect. </p> <p>For CV, NLP and diffusion models, users should clone Cloud AI 100 GitHub repository containing model recipes, code samples and toolchain tutorials on the instance to get started. </p> <p>For LLMs, users should clone QEfficient Transformers Library on the instance to get started. </p>"},{"location":"Getting-Started/Installation/AWS/aws/#running-your-first-llm-on-dl2q","title":"Running your first LLM on DL2Q","text":"<p>QEfficient Transformers Library is the easiest way to get your first LLM running on your DL2q instance. </p> <p>Install the QEfficient library on your DL2q instance       <pre><code># Login to the Cloud AI 100 Server.\nssh -X username@hostname\n\npython3.8 -m venv qeff_env\nsource qeff_env/bin/activate\n\n# Clone the QEfficient Repo (use the 1.14 branch to match the SDK on the AMI)\ngit clone -b release/v1.14 --single-branch https://github.com/quic/efficient-transformers.git\n\n# Install the qefficient-library in Host machine (Used for CLI APIs) (Until we have docker integrated in Apps SDK)\npip install -e .\n</code></pre></p> Note <p>Some models (Llama, Mistral etc.) require custom ops which will need to be compiled prior to model compilation. Refer here for instructions on compiling the custom ops. Requires CMake 3+.</p> <p>Run your first LLM       <pre><code>python -m QEfficient.cloud.infer --model_name gpt2 --batch_size 1 --prompt_len 32 --ctx_len 128 --mxfp6 --num_cores 16 --device_group [0] --prompt \"My name is\" --mos 1 --aic_enable_depth_first\n</code></pre></p>"},{"location":"Getting-Started/Installation/Checklist/checklist/","title":"Installation Checklist","text":"<p>First, check Hardware requirements and supported operating environments.</p>"},{"location":"Getting-Started/Installation/Checklist/checklist/#local-server-installation","title":"Local Server Installation","text":"<ol> <li> <p>Set up Linux server/workstation environment (Pre-requisites)</p> <ul> <li>Enable 32 message signaled interrupts (MSI) in UEFI/BIOS setup (MSI instructions)</li> </ul> </li> <li> <p>Install Platform and Apps SDKs (Cloud AI SDK)</p> </li> <li> <p>Tip: To run Platform SDK tools without superuser (sudo) privilege, add yourself to the qaic group. <pre><code>sudo usermod -aG qaic $USER\n</code></pre></p> <ul> <li>Log in again for the changes to take effect, or run \u2018newgrp qaic\u2019.</li> </ul> </li> </ol>"},{"location":"Getting-Started/Installation/Checklist/checklist/#verify-card-healthfunction","title":"Verify Card Health/Function","text":"<ol> <li> <p>Check PCIe enumeration:   <pre><code>$ lspci | grep Qualcomm\n01:00.0 Unassigned class [ff00]: Qualcomm Device a100\n</code></pre></p> <ul> <li>A PCIe enumeration failure may indicate an unsecure connection. Try re-inserting the card, or troubleshoot with a different PCIe card.</li> </ul> </li> <li> <p>Check device nodes:   <pre><code>ls /dev/mhi*\n/dev/mhi0_QAIC_DIAG  /dev/mhi0_QAIC_TIMESYNC /dev/mhi0_QAIC_QDSS /dev/mhi1_QAIC_DIAG\n</code></pre></p> <ul> <li>For every card, QAIC_DIAG, QAIC_TIMESYNC, QAIC_QDSS and QAIC_DIAG nodes are created. </li> <li>If mhi* folders do not exist, double-check the MSI settings in UEFI/BIOS setup</li> </ul> </li> <li> <p>Check card health and status with qaic-util <pre><code>sudo /opt/qti-aic/tools/qaic-util -q | grep -e Status -e QID\n\n    QID 0\n      Status:Ready\n    QID 1\n      Status:Ready\n    QID 2\n      Status:Ready\n    QID 3\n      Status:Ready\n</code></pre></p> <ul> <li>\u2018Status: Ready\u2019 indicates card(s) is in good health. Shown here are a system with 4 Cloud AI SoCs (Each card may have one or more SoCs based on the SKU).</li> <li>\u2018Status: Error\u2019 indicates the respective card(s) has not booted up completely. <ul> <li>Card could still be booting. Wait for a few mins and retry above command.</li> <li>Card error could be due to several reasons - unsupported OS/Platform, secure boot etc. </li> </ul> </li> </ul> </li> <li> <p>Run a test inference workload with qaic-runner CLI to sanity check HW/SW function.  Example:   <pre><code>sudo /opt/qti-aic/exec/qaic-runner -t /opt/qti-aic/test-data/aic100/v2/1nsp/1nsp-conv-hmx/ -a 14 -n 5000 -d 0\n\nInput file: /opt/qti-aic/test-data/aic100/v2/1nsp/1nsp-conv-hmx//user_idx0_in.bin\n---- Stats ----\nInferenceCnt 5000 TotalDuration 17598us BatchSize 1 Inf/Sec 284123.196\n</code></pre></p> <ul> <li><code>-d</code> specify QID to run the workload on.</li> <li><code>-a</code> edit value based on no of NSPs in specified QID.</li> </ul> <p>A positive Inf/Sec indicates HW/SW is functioning correctly. The Inf/Sec reported depends on <code>-a</code> specified in the command. </p> </li> </ol>"},{"location":"Getting-Started/Installation/Checklist/checklist/#virtual-machine-setup","title":"Virtual Machine Setup","text":"<ol> <li>Configure PCIe pass-through for the supported Hypervisor</li> <li>Configure message signaled interrupts in UEFI/BIOS setup and Hypervisor</li> <li>Install and configure Hypervisor</li> <li>Configure and launch Virtual Machine instance with one of the supported Operating Systems and Pre-requisites</li> <li>Install Cloud AI SDKs</li> <li>Check Card Health/Function</li> </ol>"},{"location":"Getting-Started/Installation/Checklist/checklist/#docker-setup","title":"Docker Setup","text":"<ol> <li>Install Platform SDK on host system or virtual machine (Cloud AI SDK)</li> <li>Build Docker image and start the container (Docker)</li> </ol>"},{"location":"Getting-Started/Installation/Cloud-AI-SDK/Cloud-AI-SDK/","title":"Installation - Cloud AI SDK","text":""},{"location":"Getting-Started/Installation/Cloud-AI-SDK/Cloud-AI-SDK/#download-instructions","title":"Download Instructions","text":"<p>Platform and Apps SDKs are available on Qualcomm Package Manager. </p> <ol> <li>login to Qualcomm Package Manager. First time users need to register for a Qualcomm ID. </li> <li>Click on Tools </li> <li>In the Filter pane on the left, check Linux and uncheck Windows.  In the search box, type Cloud AI. Click on Qualcomm\u00ae Cloud AI Products to reveal the SDKs available. </li> <li>For Platform SDK, click Qualcomm\u00ae Cloud AI Platform SDK. For Apps SDK, click Qualcomm\u00ae Cloud AI Apps SDK. </li> <li>Two drop down lists are present, one for the OS and one for the version of the SDK. Select Linux and SDK Version from the drop down lists.  </li> <li>Click the Download button to download the SDK.</li> </ol>"},{"location":"Getting-Started/Installation/Cloud-AI-SDK/Cloud-AI-SDK/#platform-sdk","title":"Platform SDK","text":"<ul> <li>The downloaded Platform SDK file is named aic_platform.Core.<code>&lt;majorversion.minorversion.patchversion.buildversion&gt;</code>.Linux-AnyCPU.zip. For example: aic_platform.Core.1.12.2.0.Linux-AnyCPU.zip. </li> <li>On the host machine, log in as root or use <code>sudo</code> to have the right permissions to complete installation </li> <li>Copy the Platform SDK downloaded from the Qualcomm Portal to the host machine:<ul> <li>For networked x86-64 or ARM64 host:<ul> <li>Use scp, rsync, or samba to copy the Platform SDK zip file to the host machine</li> <li>Log in to the host machine (ssh or local terminal)</li> <li>Unzip the downloaded zip file to a working directory</li> <li><code>cd</code> to the working directory</li> </ul> </li> <li>For ARM64 hosts that support Google Android Debug Bridge (ADB):     <pre><code>adb push &lt;Platform SDK zip file&gt; /data\nadb shell\ncd /data\nUnzip the downloaded zip file to a working directory\ncd to the working directory\n</code></pre></li> </ul> </li> <li>unzip the downloaded file. </li> </ul> Info <p>The Platform SDK contains collaterals for x86-rpm, x86-deb, aarch64-rpm and aarch64-deb. Confirm the architecture and linux package format that works for your setup.    </p> <p>The Platform SDK (qaic-platform-sdk-<code>&lt;major.minor.patch.build&gt;</code>) is composed of the following tree structure.        <pre><code>\u251c\u2500\u2500 aarch64\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 deb\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 deb\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 deb-docker\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 deb-perf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 Workload\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 rpm\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 rpm\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 rpm-docker\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 rpm-perf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 Workload\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 test_suite\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 pcietool\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 powerstress\n\u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 qaic-test-data\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sectools\n\u2514\u2500\u2500 x86_64\n    \u251c\u2500\u2500 deb\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 deb\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 deb-docker\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 Workload\n    \u251c\u2500\u2500 rpm\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 rpm\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 rpm-docker\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 Workload\n    \u2514\u2500\u2500 test_suite\n        \u251c\u2500\u2500 pcietool\n        \u2514\u2500\u2500 powerstress\n</code></pre></p> <p>Uninstall existing Platform SDK     <pre><code>cd &lt;architecture&gt;/&lt;deb|rpm&gt;\nsudo ./uninstall.sh\nsync\n</code></pre></p> <p>Run the install.sh script as root or with sudo to install with superuser permissions. Installation may take up to 30 mins depending on the number of Cloud AI cards in the server/VM. Cloud AI cards undergo resets several times during the installation. </p> <p>Ugprading to SDK 1.16 is a 2-step process:</p> <ol> <li>In the first step we need to prepare each SoC to accept the 1.16 SBL bootloader firmware</li> <li>In the second step we upgrade to the 1.16 SBL bootloader firmware</li> </ol> <p>For Hybrid boot cards (PCIe CEM form factor cards), run:     <pre><code>cd &lt;architecture&gt;/&lt;deb/rpm&gt;\n\nsudo ./install.sh --no_auto_upgrade_sbl    # For VM on ESXi hypervisor, also add the --datapath_polling option\nsudo ./install.sh --ecc enable\n\n# start Qmonitor server in background\nsudo systemd-run --unit=qmonitor-proxy /opt/qti-aic/tools/qaic-monitor-grpc-server\n\n# Allow server to initialize all devices\nsleep 10\n\n# List QIDs in the system\nsudo /opt/qti-aic/tools/qaic-util -q | grep -e QID\n\n# Update SoC, repeat for all QIDs in the system.\nsudo /opt/qti-aic/tools/qaic-firmware-updater -d &lt;QID&gt; -f \n\n# Reset cards.\nsudo /opt/qti-aic/tools/qaic-util -s\n</code></pre></p> <p>For Flashless boot cards (less common), run:     <pre><code>sudo ./install.sh \u2013-ecc enable\n# For VM on ESXi hypervisor, run \nsudo ./install.sh --datapath_polling \u2013-ecc enable\n</code></pre></p> <p>On successful installation of the platform SDK, the contents shown below are stored in /opt/qti-aic:     <pre><code>config  dev  examples  exec  firmware  lib  services  test-data  tools  versions\n</code></pre></p> <p>Check Platform SDK version using      <pre><code>sudo /opt/qti-aic/tools/qaic-version-util --platform\n</code></pre>   Add user to the qaic group to allow command-line tools to run without sudo:     <pre><code>sudo usermod -a -G qaic $USER\n</code></pre></p>"},{"location":"Getting-Started/Installation/Cloud-AI-SDK/Cloud-AI-SDK/#verify-card-operation","title":"Verify card operation","text":"<p>Refer to Verify Card Operation</p>"},{"location":"Getting-Started/Installation/Cloud-AI-SDK/Cloud-AI-SDK/#apps-sdk","title":"Apps SDK","text":"<p>The Apps SDK is only available for x86-64 Linux-based hosts. For ARM64-based Qualcomm platforms, models are first compiled on x86 with the Apps SDK. The compiled binary (QPC) is transferred to the ARM64 host for loading and execution by the Platform SDK on Cloud AI hardware.</p> <ul> <li>The downloaded Apps SDK file is named aic_apps.Core.<code>&lt;majorversion.minorversion.patchversion.buildversion&gt;</code>.Linux-AnyCPU.zip. For example: aic_apps.Core.1.12.2.0.Linux-AnyCPU.zip. </li> <li>Copy the SDK over to the linux x86 machine. </li> <li>unzip the downloaded file. </li> <li>The Apps SDK (qaic-apps-<code>&lt;major.minor.patch.build version&gt;</code>) is composed of the following tree structure.  </li> </ul> <pre><code>\u251c\u2500\u2500 dev\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hexagon_docker_scripts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hexagon_tools\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 inc\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 lib\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 python\n\u251c\u2500\u2500 examples\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 apps\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 scripts\n\u251c\u2500\u2500 exec\n\u251c\u2500\u2500 integrations\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 kserve\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 qaic_onnxrt\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 triton\n\u251c\u2500\u2500 scripts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 qaic-model-configurator\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 qaic-prepare-model\n\u251c\u2500\u2500 tools\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 aic-manager\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 custom-ops\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 docker-build\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 graph-analysis-engine\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 k8s-device-plugin\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 opstats-profiling\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 package-generator\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 qaic-inference-optimizer\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 qaic-pytools\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 rcnn-exporter\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 smart-nms\n\u2514\u2500\u2500 versions\n</code></pre>"},{"location":"Getting-Started/Installation/Cloud-AI-SDK/Cloud-AI-SDK/#install-apps-sdk","title":"Install Apps SDK","text":"<ul> <li>Uninstall existing Apps SDK <code>sudo ./uninstall.sh</code></li> <li>Run the install.sh script as root or with sudo to install with root permissions. <code>sudo ./install.sh --enable-qaic-pytools</code></li> <li>On successful installation of the Apps SDK, the contents are stored to the /opt/qti-aic path under the dev and exec directories: <code>dev exec integrations scripts</code></li> <li>Check the Apps SDK version with the following command  <pre><code>sudo /opt/qti-aic/tools/qaic-version-util --apps\n</code></pre></li> <li> <p>Apply chmod commands </p> <pre><code>sudo chmod a+x /opt/qti-aic/dev/hexagon_tools/bin/*\nsudo chmod a+x /opt/qti-aic/exec/*\n</code></pre> </li> </ul>"},{"location":"Getting-Started/Installation/Docker/","title":"Containers with Cloud AI 100","text":"<p>This section provides resources to create, deploy and orchestrate Cloud AI 100 containers for inferencing workloads.</p> <ul> <li>Docker</li> <li>Kubernetes</li> <li>Triton inference server</li> </ul>"},{"location":"Getting-Started/Installation/Docker/Docker-rel-1.11/","title":"Introduction","text":"<p>Docker is a product that allows users to build, test, and deploy applications through software containers. Docker for Cloud AI 100 packages the Platform SDK, Apps SDK (x86-64 only), libraries, system tools, etc., which enables the user to navigate the inference workflow seamlessly. </p> <p>The Docker scripts are in the Apps SDK in the <code>tools/docker-build</code> folder. The scripts to build a QAic Docker image are composed of the following structure. <pre><code>\u251c\u2500\u2500 build_image.py\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 applications\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 base_image\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 build_type\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 python_version\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sdk\n\u251c\u2500\u2500 image_schema.json\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 sample_user_specs\n</code></pre></p>"},{"location":"Getting-Started/Installation/Docker/Docker-rel-1.11/#setup-and-system-pre-requisistes","title":"Setup and System Pre-requisistes","text":"<ul> <li>Packages: python3.8, docker v23+ (https://docs.docker.com/engine/install/)</li> <li>Download Apps SDK and Platform SDK from Qualcomm site.<ul> <li>Docker containers require the Cloud AI device drivers to communicate with the devices. Install the Platform SDK on the host bare metal OS or VM.</li> <li>Unzip Apps SDK and the build scripts are located under <code>/tools/docker-build/</code> <pre><code>unzip qaic-apps-1.11.0.&lt;&gt;.zip\ncd qaic-apps-1.11.0.&lt;&gt;/tools/docker-build/\n</code></pre></li> </ul> </li> <li>Install python modules listed in requirements.txt, preferably in a virtual environment <pre><code>[user@localhost qaic-docker]$ python3.8 -m venv /venv_directory/qaic_docker_venv\n[user@localhost qaic-docker]$ . /venv_directory/qaic_docker_venv/bin/activate\n(qaic_docker_venv) [user@localhost qaic-docker]$ pip install -r requirements.txt\n</code></pre></li> </ul>"},{"location":"Getting-Started/Installation/Docker/Docker-rel-1.11/#build-image","title":"Build Image","text":"<p>QAic docker build script is used to create new docker image with Cloud AI 100 Apps SDK and Cloud AI 100 Platform SDK with the supported OS.</p> <p>The command to build the docker image is:</p> <pre><code>python3 build_image.py [ --user_specification_file (Required) &lt;user-specification-json-file-path&gt; ] \\\n[ --mirror (Optional) &lt;docker-registry-mirror-location&gt; ] \\\n[ --apps_sdk (Optional) /qaic/apps/sdk/zip/path ] [ --platform_sdk (Optional) /qaic/platform/sdk/zip/path ] [ --qnn_sdk (Optional) ] \\\n[ --arch (Optional) &lt;x86_64|aarch64&gt; ] [ --external_dist_files (Optional) &lt;file-path-1 file-path-2 ... file-path-n&gt; ] \\\n[ --tag (Optional) &lt;tag-for-the-created-image&gt; ] [ --log_level (Optional) &lt;log-level-numeric-value&gt;] [ --no_cache (Optional) ]\n</code></pre> <p>The resulting docker image is named by the convention: qaic-\\-----\\:\\ <p>Description  user_specification_file: specifies path to user specification json file  mirror: specifies registry mirror location for base images in docker format (eg. host.domain/dir/)  arch: Pick from x86_64, aarch64. Default is host arch.  tag: specifies tag for the created image  apps_sdk: specifies path to the zip file of apps sdk. Overrides if corresponding path is already mentioned in user_specification_file.  platform_sdk: specifies path to the zip file of platform sdk. Overrides if corresponding path is already mentioned in user_specification_file.  qnn_sdk: specifies path to the zip file of qnn sdk. Overrides if corresponding path is already mentioned in user_specification_file.  external_dist_files: specify external files required in docker image build. Overrides any external_dist_files already mentioned in user_specification_file.  log_level: specifies numeric value corresponding to log level (debug:1, info:2, warning:3, error:4, critical:5)  no_cache: Do not use docker's internal layer cache (used for debug) </p> <p>For Example: <pre><code>python3 build_image.py --user_specification_file ./sample_user_specs/user_image_spec_qaic.json --apps_sdk ~/qaic-apps-1.11.0.&lt;&gt;.zip \\\n--platform_sdk ~/qaic-platform-sdk-1.11.0.&lt;&gt;.zip --tag 1.11.0.&lt;&gt;\n</code></pre></p> <p>To check the docker image created with above script:  <pre><code>$ docker images\nREPOSITORY                                                            TAG            IMAGE ID       CREATED        SIZE\nqaic-x86_64-ubuntu20-release-py38-qaic_platform-qaic_apps             1.11.0.&lt;&gt;      f784c37d7f18   2 hours ago    4.25GB\nqaic-x86_64-ubuntu20-release-py38-qaic_platform                       1.11.0.&lt;&gt;      a4f8193202db   2 hours ago    3.64GB\nqaic-x86_64-ubuntu20-release-py38                                     1.11.0.&lt;&gt;      78000059a5aa   2 hours ago    3.12GB\nqaic-x86_64-ubuntu20-release                                          1.11.0.&lt;&gt;      cbdd424f4338   2 hours ago    3.1GB\nqaic-x86_64-ubuntu20                                                  1.11.0.&lt;&gt;      a38215c10e0b   2 hours ago    3.1GB\n</code></pre></p>"},{"location":"Getting-Started/Installation/Docker/Docker-rel-1.11/#features","title":"Features","text":"<ul> <li>Incremental docker image build infrastructure optimizes disk usage and rebuilding time</li> <li>Flexibility to override specification options such as sdk paths, external dist files through cmd line options</li> <li>User provided image specification is resolved for dependencies and dumped as autocompleted_user_specification.json</li> <li>Supports multi-platform builds</li> </ul>"},{"location":"Getting-Started/Installation/Docker/Docker-rel-1.11/#incremental-build-layers","title":"Incremental build layers","text":"<p>The configurations are grouped as shown below to minimize the need for rebuild and to allow sharing layers. This enables reduction of disk usage and allows faster rebuild.</p> <p> </p>"},{"location":"Getting-Started/Installation/Docker/Docker-rel-1.11/#image-specification","title":"Image specification","text":"<p>User provides a specification json that defines the items to build in each group and list external file dependecies The build script auto-completes the configuration by resolving the dependencies for specified nodes. Sample user specification files are present in sample_user_specs directory within Apps SDK directory - tools/docker-build/</p> <p>Supported keys in json:      'build_type', 'applications', 'sdk', 'python_version', 'base_image', 'external_dist_files'</p> <p>Applications - Optional, accepts a list of values. SDK - Optional, accepts a dictionary of values (sdk type: file-path/boolean). Python Version - Optional, accepts a single value. Build Type - Required, accepts a single value. Base Image - Required, accepts a single value. External dist files - Optional, allows specifying external files required in docker image build. Accepts a list of values.</p> <p>Example -  <pre><code>{\n    \"applications\": [\"gae\", \"pytools\", \"aimet\", \"qinf\", \"pybase\"],\n    \"sdk\": {\n        \"qaic_apps\": \"/path/to/qaic-apps-1.11.0.&lt;&gt;.zip\",\n        \"qaic_platform\": \"/path/to/qaic-platform-sdk-1.11.0.&lt;&gt;.zip\"\n    },\n    \"python_version\": \"py38\",\n    \"base_image\": \"ubuntu20\",\n    \"external_dist_files\": [\"/path/to/aimetpro-1.26.0-RC7.torch-gpu-release.tar.gz\"]\n}\n</code></pre></p> <p>Minimal user specification to build the same image <pre><code>{\n    \"applications\": [\"gae\", \"aimet\", \"qinf\"],\n    \"sdk\": {\n        \"qaic_apps\": \"/path/to/qaic-apps-1.11.0.&lt;&gt;.zip\",\n        \"qaic_platform\": \"/path/to/qaic-platform-sdk-1.11.0.&lt;&gt;.zip\"\n    },\n    \"external_dist_files\": [\"/path/to/aimetpro-1.26.0-RC7.torch-gpu-release.tar.gz\"]\n}\n</code></pre> sdk, external_dist_files can be skipped here and provided through cmd line </p>"},{"location":"Getting-Started/Installation/Docker/Docker-rel-1.11/#overridable-options-in-specification","title":"Overridable options in specification","text":"<p>SDK, external dist files configuration rely on file paths. These keys can be specified either through specification file or cmd line options. cmd line specification takes precedence when specified through both.</p>"},{"location":"Getting-Started/Installation/Docker/Docker-rel-1.11/#auto-completion-of-user-provided-specification","title":"Auto-completion of user provided specification","text":"<p>The docker image may constitute base image + layers from other groups specified in the figure above. Resolution of dependencies is done top-down for the specified groups. </p> <p>Applications - Resolved as union of dependencies in same level.  SDK - Resolved as union of dependencies from higher levels and same level.  Python Version - Resolved as intersection of dependencies from higher levels of which preferred node is chosen.  Build Type - Resolved as intersection of dependencies from higher levels of which preferred node is chosen.  Base Image - Resolved as intersection of dependencies from higher levels of which preferred node is chosen. </p>"},{"location":"Getting-Started/Installation/Docker/Docker-rel-1.11/#multi-platform-image-build","title":"Multi-platform image build","text":"<p>Use the --arch command line option with the build command to configure building the image for the preferred architecture (x86_64/aarch64).</p>"},{"location":"Getting-Started/Installation/Docker/Docker-rel-1.11/#launch-container","title":"Launch container","text":""},{"location":"Getting-Started/Installation/Docker/Docker-rel-1.11/#command","title":"Command","text":"<p>Below is example for launching an x86_64 ubuntu20 image with python3.8 environment, apps and platform sdk. </p> <p>Run the container and map 1 or more qaic devices. (Or any other directory mapping using the -v option) <pre><code># Passing 1 device\ndocker run -dit --name qaic-ubuntu-test --device=/dev/qaic_aic100_0 qaic-x86_64-ubuntu20-release-py38-qaic_platform-qaic_apps:1.11.0.&lt;&gt;\n\n# Passing 1 device and mapping a local folder\ndocker run -dit --name qaic-ubuntu-test -v /data/test:/data/test --device=/dev/qaic_aic100_0 qaic-x86_64-ubuntu20-release-py38-qaic_platform-qaic_apps:1.11.0.&lt;&gt;\n</code></pre></p>"},{"location":"Getting-Started/Installation/Docker/Docker-rel-1.11/#architecture-selection","title":"Architecture selection","text":"<p>When passing --arch option where the host architecture is not the same as the requested architecture, setup the host for docker multiarch using qemu.</p> <p>Ubuntu Host <pre><code>[user@localhost qaic-docker]$ sudo apt install qemu binfmt-support qemu-user-static\n[user@localhost qaic-docker]$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n# Check the setup\n[user@localhost qaic-docker]$ docker run --rm -t --platform=linux/arm64 &lt;image name&gt; uname -m\n</code></pre> The command above should give the architecture as aarch64</p> <p>Centos Host <pre><code>[user@localhost qaic-docker]$ sudo yum install qemu qemu-kvm\n[user@localhost qaic-docker]$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n# Check the setup\n[user@localhost qaic-docker]$ docker run --rm -t --platform=linux/arm64 &lt;image name&gt; uname -m\n</code></pre> The command above should give the architecture as aarch64</p>"},{"location":"Getting-Started/Installation/Docker/Docker-rel-1.11/#run-tests-in-container","title":"Run tests in container","text":"<ul> <li> <p>Connect to the container and run tests:</p> <ul> <li>Run \"docker ps\" to get the container SHA</li> <li>Run docker exec -it  /bin/bash <li>Run \"/opt/qti-aic/tools/qaic_util -q\" and ensure that you can see your devices.</li> <li>Run an inference with recorded data. Eg: \"/opt/qti-aic/exec/qaic-runner -d 0 -t /opt/qti-aic/test-data/aic100/v2/1nsp/1nsp-quant-resnet50/\"</li> <li> <p>Activate the python enviornment and test Python HL API (valid if qaic_python: \"True\" specified in sdk group of user_specification_file while building docker image): <pre><code>source /opt/qti-aic/dev/python/py_env/bin/activate\ncd /opt/qti-aic/examples/apps/qaic-python-sdk/examples/resnet_example/\npython resnet_example.py\n</code></pre></p> </li>"},{"location":"Getting-Started/Installation/Docker/Docker/","title":"Docker","text":"<p>Docker allows users to build, test, and deploy applications through software containers. Docker for Cloud AI 100 packages the Platform SDK, Apps SDK (x86-64 only), libraries, system tools, etc., which enables the user to navigate the inference workflow seamlessly. </p> <p>The Docker scripts are in the Apps SDK in the <code>tools/docker-build</code> folder. The scripts to build a QAic Docker image are composed of the following structure. <pre><code>\u251c\u2500\u2500 build_image.py\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 applications\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 base_image\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 build_type\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 python_version\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sdk\n\u251c\u2500\u2500 image_schema.json\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 sample_user_specs\n</code></pre></p>"},{"location":"Getting-Started/Installation/Docker/Docker/#setup-and-system-pre-requisistes","title":"Setup and System Pre-requisistes","text":"<ul> <li>Packages: python3.8, docker v23+ (https://docs.docker.com/engine/install/)</li> <li>Download Apps SDK and Platform SDK from Qualcomm site.<ul> <li>Docker containers require the Cloud AI device drivers to communicate with the devices. Install the Platform SDK on the host bare metal OS or VM.</li> <li>Unzip Apps SDK and the build scripts are located under <code>/tools/docker-build/</code> <pre><code>unzip qaic-apps-1.16.1.&lt;&gt;.zip\ncd qaic-apps-1.16.1.&lt;&gt;/tools/docker-build/\n</code></pre></li> </ul> </li> <li>Install python modules listed in requirements.txt, preferably in a virtual environment <pre><code>[user@localhost qaic-docker]$ python3.8 -m venv /venv_directory/qaic_docker_venv\n[user@localhost qaic-docker]$ . /venv_directory/qaic_docker_venv/bin/activate\n(qaic_docker_venv) [user@localhost qaic-docker]$ pip install -r requirements.txt\n</code></pre></li> </ul>"},{"location":"Getting-Started/Installation/Docker/Docker/#build-image","title":"Build Image","text":"<p>QAic docker build script is used to create new docker image with Cloud AI 100 Apps SDK and Cloud AI 100 Platform SDK with the supported OS.</p> <p>The command to build the docker image is:</p> <pre><code>python3 build_image.py [ --user_specification_file (Required) &lt;user-specification-json-file-path&gt; ] \\\n[ --mirror (Optional) &lt;docker-registry-mirror-location&gt; ] \\\n[ --apps_sdk (Optional) /qaic/apps/sdk/zip/path ] [ --platform_sdk (Optional) /qaic/platform/sdk/zip/path ] [ --qnn_sdk (Optional) ] \\\n[ --arch (Optional) &lt;x86_64|aarch64&gt; ] [ --external_dist_files (Optional) &lt;file-path-1 file-path-2 ... file-path-n&gt; ] \\\n[ --tag (Optional) &lt;tag-for-the-created-image&gt; ] [ --log_level (Optional) &lt;log-level-numeric-value&gt;] [ --no_cache (Optional) ]\n</code></pre> <p>The resulting docker image is named by the convention: qaic-\\-----\\:\\ <p>Description  user_specification_file: specifies path to user specification json file  mirror: specifies registry mirror location for base images in docker format (eg. host.domain/dir/)  arch: Pick from x86_64, aarch64. Default is host arch.  tag: specifies tag for the created image  apps_sdk: specifies path to the zip file of apps sdk. Overrides if corresponding path is already mentioned in user_specification_file.  platform_sdk: specifies path to the zip file of platform sdk. Overrides if corresponding path is already mentioned in user_specification_file.  qnn_sdk: specifies path to the zip file of qnn sdk. Overrides if corresponding path is already mentioned in user_specification_file.  external_dist_files: specify external files required in docker image build. Overrides any external_dist_files already mentioned in user_specification_file.  log_level: specifies numeric value corresponding to log level (debug:1, info:2, warning:3, error:4, critical:5)  no_cache: Do not use docker's internal layer cache (used for debug) </p> <p>For Example: <pre><code>python3 build_image.py --user_specification_file ./sample_user_specs/user_image_spec_qaic.json --apps_sdk ~/qaic-apps-1.16.1.&lt;&gt;.zip \\\n--platform_sdk ~/qaic-platform-sdk-1.16.1.&lt;&gt;.zip --tag 1.16.1.&lt;&gt;\n</code></pre></p> <p>To check the docker image created with above script:  <pre><code>$ docker images\nREPOSITORY                                                            TAG            IMAGE ID       CREATED        SIZE\nqaic-x86_64-ubuntu20-release-py38-qaic_platform-qaic_apps             1.16.1.&lt;&gt;      f784c37d7f18   2 hours ago    4.25GB\nqaic-x86_64-ubuntu20-release-py38-qaic_platform                       1.16.1.&lt;&gt;      a4f8193202db   2 hours ago    3.64GB\nqaic-x86_64-ubuntu20-release-py38                                     1.16.1.&lt;&gt;      78000059a5aa   2 hours ago    3.12GB\nqaic-x86_64-ubuntu20-release                                          1.16.1.&lt;&gt;      cbdd424f4338   2 hours ago    3.1GB\nqaic-x86_64-ubuntu20                                                  1.16.1.&lt;&gt;      a38215c10e0b   2 hours ago    3.1GB\n</code></pre></p>"},{"location":"Getting-Started/Installation/Docker/Docker/#supported-operating-systems","title":"Supported Operating Systems","text":"<p>Docker container for Cloud AI 100 supports different operating system distributions. Below is the list of supported operating systems.</p>"},{"location":"Getting-Started/Installation/Docker/Docker/#operating-systems","title":"Operating Systems","text":"Operating systems Comment Alma Linux 8 Compatible with RHEL 8 Alma Linux 9 Compatible with RHEL 9 Amazon Linux 2 Amazon Linux 2023 CentOS Linux 7 Compatible with RHEL 7 Red Hat Universal Base Image 8 Red Hat Universal Base Image 9 Ubuntu 18 Ubuntu 20 Ubuntu 22 Ubuntu 24"},{"location":"Getting-Started/Installation/Docker/Docker/#features","title":"Features","text":"<ul> <li>Incremental docker image build infrastructure optimizes disk usage and rebuilding time</li> <li>Flexibility to override specification options such as sdk paths, external dist files through cmd line options</li> <li>User provided image specification is resolved for dependencies and dumped as autocompleted_user_specification.json</li> <li>Supports multi-platform builds</li> </ul>"},{"location":"Getting-Started/Installation/Docker/Docker/#incremental-build-layers","title":"Incremental build layers","text":"<p>The configurations are grouped as shown below to minimize the need for rebuild and to allow sharing layers. This enables reduction of disk usage and allows faster rebuild.</p> <p> </p>"},{"location":"Getting-Started/Installation/Docker/Docker/#image-specification","title":"Image specification","text":"<p>User provides a specification json that defines the items to build in each group and list external file dependecies The build script auto-completes the configuration by resolving the dependencies for specified nodes. Sample user specification files are present in sample_user_specs directory within Apps SDK directory - tools/docker-build/</p> <p>Supported keys in json:      'build_type', 'applications', 'sdk', 'python_version', 'base_image', 'external_dist_files'</p> <p>Applications - Optional, accepts a list of values. SDK - Optional, accepts a dictionary of values (sdk type: file-path/boolean). Python Version - Optional, accepts a single value. Build Type - Required, accepts a single value. Base Image - Required, accepts a single value. External dist files - Optional, allows specifying external files required in docker image build. Accepts a list of values.</p> <p>Example -  <pre><code>{\n    \"applications\": [\"gae\", \"pytools\", \"aimet\", \"qinf\", \"pybase\"],\n    \"sdk\": {\n        \"qaic_apps\": \"/path/to/qaic-apps-1.16.1.&lt;&gt;.zip\",\n        \"qaic_platform\": \"/path/to/qaic-platform-sdk-1.16.1.&lt;&gt;.zip\"\n    },\n    \"python_version\": \"py38\",\n    \"base_image\": \"ubuntu20\",\n    \"external_dist_files\": [\"/path/to/aimetpro-1.26.0-RC7.torch-gpu-release.tar.gz\"]\n}\n</code></pre></p> <p>Minimal user specification to build the same image <pre><code>{\n    \"applications\": [\"gae\", \"aimet\", \"qinf\"],\n    \"sdk\": {\n        \"qaic_apps\": \"/path/to/qaic-apps-1.16.1.&lt;&gt;.zip\",\n        \"qaic_platform\": \"/path/to/qaic-platform-sdk-1.16.1.&lt;&gt;.zip\"\n    },\n    \"external_dist_files\": [\"/path/to/aimetpro-1.26.0-RC7.torch-gpu-release.tar.gz\"]\n}\n</code></pre> sdk, external_dist_files can be skipped here and provided through cmd line </p>"},{"location":"Getting-Started/Installation/Docker/Docker/#overridable-options-in-specification","title":"Overridable options in specification","text":"<p>SDK, external dist files configuration rely on file paths. These keys can be specified either through specification file or cmd line options. cmd line specification takes precedence when specified through both.</p>"},{"location":"Getting-Started/Installation/Docker/Docker/#auto-completion-of-user-provided-specification","title":"Auto-completion of user provided specification","text":"<p>The docker image may constitute base image + layers from other groups specified in the figure above. Resolution of dependencies is done top-down for the specified groups. </p> <p>Applications - Resolved as union of dependencies in same level.  SDK - Resolved as union of dependencies from higher levels and same level.  Python Version - Resolved as intersection of dependencies from higher levels of which preferred node is chosen.  Build Type - Resolved as intersection of dependencies from higher levels of which preferred node is chosen.  Base Image - Resolved as intersection of dependencies from higher levels of which preferred node is chosen. </p>"},{"location":"Getting-Started/Installation/Docker/Docker/#multi-platform-image-build","title":"Multi-platform image build","text":"<p>Use the --arch command line option with the build command to configure building the image for the preferred architecture (x86_64/aarch64).</p>"},{"location":"Getting-Started/Installation/Docker/Docker/#launch-container","title":"Launch container","text":""},{"location":"Getting-Started/Installation/Docker/Docker/#command","title":"Command","text":"<p>Below is example for launching an x86_64 ubuntu20 image with python3.8 environment, apps and platform sdk. </p> <p>Run the container and map 1 or more qaic devices. (Or any other directory mapping using the -v option) <pre><code># Passing 1 device\ndocker run -dit --name qaic-ubuntu-test --device=/dev/accel/accel0 qaic-x86_64-ubuntu20-release-py38-qaic_platform-qaic_apps:1.16.1.&lt;&gt;\n\n# Passing 1 device and mapping a local folder\ndocker run -dit --name qaic-ubuntu-test -v /data/test:/data/test --device=/dev/accel/accel0 qaic-x86_64-ubuntu20-release-py38-qaic_platform-qaic_apps:1.16.1.&lt;&gt;\n</code></pre></p>"},{"location":"Getting-Started/Installation/Docker/Docker/#architecture-selection","title":"Architecture selection","text":"<p>When passing --arch option where the host architecture is not the same as the requested architecture, setup the host for docker multiarch using qemu.</p> <p>Ubuntu Host <pre><code>[user@localhost qaic-docker]$ sudo apt install qemu binfmt-support qemu-user-static\n[user@localhost qaic-docker]$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n# Check the setup\n[user@localhost qaic-docker]$ docker run --rm -t --platform=linux/arm64 &lt;image name&gt; uname -m\n</code></pre> The command above should give the architecture as aarch64</p> <p>CentOS Host <pre><code>[user@localhost qaic-docker]$ sudo yum install qemu qemu-kvm\n[user@localhost qaic-docker]$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n# Check the setup\n[user@localhost qaic-docker]$ docker run --rm -t --platform=linux/arm64 &lt;image name&gt; uname -m\n</code></pre> The command above should give the architecture as aarch64</p>"},{"location":"Getting-Started/Installation/Docker/Docker/#run-tests-in-container","title":"Run tests in container","text":"<ul> <li> <p>Connect to the container and run tests:</p> <ul> <li>Run \"docker ps\" to get the container SHA</li> <li>Run docker exec -it &lt;SHA&gt; /bin/bash</li> <li>Run \"/opt/qti-aic/tools/qaic_util -q\" and ensure that you can see your devices.</li> <li>Run an inference with recorded data. Eg: \"/opt/qti-aic/exec/qaic-runner -d 0 -t /opt/qti-aic/test-data/aic100/v2/1nsp/1nsp-quant-resnet50/\"</li> </ul> </li> <li> <p>Activate the python enviornment and test Python HL API (valid if qaic_python: \"True\" specified in sdk group of user_specification_file while building docker image): <pre><code>source /opt/qti-aic/dev/python/py_env/bin/activate\ncd /opt/qti-aic/examples/apps/qaic-python-sdk/examples/resnet_example/\npython resnet_example.py\n</code></pre></p> </li> </ul>"},{"location":"Getting-Started/Installation/Docker/k8s/","title":"Kubernetes","text":"<p>Machine learning applications built for the Cloud AI 100 accelerator can be containerized with Docker and deployed with Kubernetes. The following figure shows a sample Kubernetes deployment.</p> Kubernetes Cluster"},{"location":"Getting-Started/Installation/Docker/k8s/#k8s-device-plugin","title":"K8s device plugin","text":"<p>The Cloud AI 100 k8s device plugin can be found at qaic-apps-1.x.y.z/tools/k8s-device-plugin in the Cloud AI 100 Apps SDK. The qaic-k8s-device-plugin is composed of the following tree structure. </p> <pre><code>\u251c\u2500\u2500 Apache_License\n\u251c\u2500\u2500 build_image.sh\n\u251c\u2500\u2500 deploy-qaic-single.yaml\n\u251c\u2500\u2500 docker\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 aarch64\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 Dockerfile.ubuntu\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 x86_64\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 Dockerfile.ubuntu\n\u251c\u2500\u2500 examples\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pod-example.yml\n\u251c\u2500\u2500 go.mod\n\u251c\u2500\u2500 Gopkg.toml\n\u251c\u2500\u2500 go.sum\n\u251c\u2500\u2500 main.go\n\u251c\u2500\u2500 multi_soc_checks.go\n\u251c\u2500\u2500 multi_soc_checks_test.go\n\u251c\u2500\u2500 Notice.txt\n\u251c\u2500\u2500 qaic-device-plugin.yml\n\u251c\u2500\u2500 qaic.go\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 server.go\n\u251c\u2500\u2500 server_test.go\n\u251c\u2500\u2500 topology.go\n\u251c\u2500\u2500 topology_test.go\n\u2514\u2500\u2500 watcher.go\n</code></pre> <p>Contents of the qaic-k8s-device-plugin package:</p> <ul> <li>QAic K8s Device Plugin<ul> <li>Sends the kubelet the list of AI 100 devices it manages.</li> <li>Monitors AI 100 device health.</li> <li>Handles AI 100 device allocation and cleanup.</li> </ul> </li> <li>Qaic K8s Device Plugin Docker image build script</li> <li>Deployment scripts (YAML)<ul> <li>Device Plugin Deployment Script (deploys Qaic K8s Device Plugin as daemonset)</li> <li>Sample AI 100 Workload Deployment Script</li> </ul> </li> </ul> <p>Prerequisites for deployment:</p> <ul> <li>Platform SDK installed on Kubernetes Worker Node<ul> <li>Required for QAic Linux kernel drivers and firmware images.</li> </ul> </li> <li>QAic K8s Device Plugin Docker Image available through customer docker-hub or preloaded on Kubernetes Worker Node</li> <li>AI 100 Workload Docker Image available through customer docker-hub or preloaded on Kubernetes Worker Node</li> </ul>"},{"location":"Getting-Started/Installation/Hypervisors/hypervisor/","title":"Hypervisors","text":""},{"location":"Getting-Started/Installation/Hypervisors/hypervisor/#kvm","title":"KVM","text":"<p>Kernel-based Virtual Machine (KVM) is a module in the Linux kernel that allows the Linux OS to operate as a hypervisor. This enables a native install of Linux to act as the host for a virtual machine. In combination with QEMU and libVirt, a KVM virtual machine emulates a completely independent system that can be limited to a subset of the host hardware, and that can also run completely different operating systems from the host.</p> <p>An overview of KVM is at https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine.</p> <p>The benefit of a virtual machine is strong isolation. A virtual machine is isolated from other virtual machines and from the host. This isolation provides a high level of security because one application in a virtual machine may not be aware that it is in a virtual machine, much less that there may be other virtual machines with other applications. Also, a virtual machine provides separation from the host.</p> <p>Even if a driver in the virtual machine crashes the entire virtual machine, that crash will not take down the host and, therefore, will allow other services in other virtual machines to continue operating.</p> <p>The cost of these benefits is additional overhead to set up the system. Also, additional processing may be required at runtime between the virtual machine and the native hardware.</p> <p>AIC100 only supports PCIe passthrough to a virtual machine. This means that the virtual machine completely owns the AIC100 device. The AIC100 device cannot be shared between virtual machines, or between a virtual machine and the native host.</p> <p>The generic setup and operation of a KVM virtual machine is outside the scope of this document. This document assumes that the reader is familiar with those operations and, thus, only explains the elements directly related to assigning an AIC100 device to a KVM virtual machine.</p> <p>AIC100 supports only the operating systems listed here as the operating system running within the virtual machine as the guest OS.</p> <p>Within a virtual machine, AIC100 still requires the assignment of 32 message signal interrupts (MSI) to operate, which requires the virtual machine to emulate an IOMMU. During the creation of a virtual machine, the virtual machine must be configured to emulate a system that can emulate an IOMMU.</p> <p>One such system is the \u201cq35\u201d system. If using \u201cvirt-install\u201d, a q35 based virtual machine can be created by adding \u201c\u2014machine=q35\u201d to the virt-install command.</p> <p>The remainder of this section assumes that the virtual machine is created with virt-install and the \u2013machine=q35 option. Other systems may require different configurations than what is described to obtain the same end effect.</p> <p>After the virtual machine is created, it must be configured. This can be done with the \u201cvirsh edit\u201d command while the virtual machine is not running. See the virsh man page for more details.</p> <p>The virtual machine configuration must have the emulated IOMMU presented to the guest OS in the virtual machine, and the virtual machine must have the AIC100 device passed in.</p> <p>First, to present the IOMMU to the guest OS, add the following elements to the configuration XML:</p> <ol> <li>Configure the ioapic driver in split mode for interrupt remapping. This is done by adding \u201c\u201d under the \u201cfeatures\u201d section of the XML.</li> <li>Configure the emulated IOMMU to be present with the interrupt remapping functionality enabled. This is done by adding the following snippet under the \u201cfeatures\u201d section of the XML:   <pre><code>&lt;iommu model='intel'&gt;\n&lt;driver caching_mode='on' intremap='on'/&gt;\n&lt;/iommu&gt;\n</code></pre></li> </ol> <p>Second, configure what device to pass through to the virtual machine guest OS.</p> <ol> <li>Obtain the PCIe SBDF address of the AIC100 device using \u201clspci\u201d in the host.</li> <li>Add the device address obtained from Step 1 to the \u201cdevices\u201d section of the XML, as follows, but change the address tag values to match that of your specific system.</li> </ol> <p><pre><code>&lt;hostdev mode='subsystem' type='pci' managed='yes'&gt;\n&lt;source&gt;\n&lt;address domain='0x0' bus='0x0' slot='0x19' function='0x0'/&gt;\n&lt;/source&gt;\n&lt;/hostdev&gt;\n</code></pre> After making these edits, save the configuration. The next time the virtual machine is booted, you will observe the AIC100 device in lspci output in the virtual machine. It will have a different PCIE SBDF address than the host. Install the AIC100 Platform SDK and Apps SDK in the same manner as if you were operating a native system.</p>"},{"location":"Getting-Started/Installation/Hypervisors/hypervisor/#hyper-v","title":"Hyper-V","text":"<p>Hyper-V is a virtualization hypervisor commonly used with Microsoft Windows Server. This enables a native install of Windows Server to act as the host for a virtual machine. The virtual machine emulates a completely independent system that can be limited to a subset of the host hardware, and also can run completely different operating systems from the host.</p> <p>An overview of Hyper-V is at https://en.wikipedia.org/wiki/Hyper-V.</p> <p>The benefit of a virtual machine is strong isolation. A virtual machine is isolated from other virtual machines and also from the host. This isolation provides a high level of security because one application in a virtual machine may not be aware that it is in a virtual machine, much less that there may be other virtual machines with other applications. Also, a virtual machine provides separation from the host. Even if a driver in the virtual machine crashes the entire virtual machine, that crash will not take down the host and, therefore, will allow other services in other virtual machines to continue operating.</p> <p>The cost of these benefits is additional overhead to set up the system. Also, additional processing may be required at runtime between the virtual machine and the native hardware.</p> <p>AIC100 only supports PCIe passthrough to a virtual machine, which means that the virtual machine completely owns the AIC100 device. The AIC100 device cannot be shared between virtual machines, or between a virtual machine and the native host.</p> <p>The generic setup and operation of a Hyper-V virtual machine is outside the scope of this document. This document assumes that the reader is familiar with those operations and, thus, only explains the elements directly related to assigning an AIC100 device to a Hyper-V virtual machine.</p> <p>AIC100 supports only the operating systems listed in this table as the operating system running within the virtual machine as the guest OS.</p> <p>Within a virtual machine, AIC100 still requires the assignment of 32 message signal interrupts (MSI) to operate. Hyper-V does not support emulating an IOMMU in the guest OS of the virtual machine.</p> <p>However, Hyper-V supports a paravirtualized PCIe root controller that has a driver in the Linux kernel. This driver uses the Hyper-V hypervisor to configure the host IOMMU for the purposes of supplying MSIs for devices like AIC100.</p> <p>AIC100 requires the following fixes to the Hyper-V PCIe root controller driver within the Linux kernel to operate properly:</p> <ul> <li>https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v5.19-rc7&amp;id=08e61e861a0e47e5e1a3fb78406afd6b0cea6b6d</li> <li>https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v5.19-rc7&amp;id=455880dfe292a2bdd3b4ad6a107299fce610e64b</li> <li>https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v5.19-rc7&amp;id=b4b77778ecc5bfbd4e77de1b2fd5c1dd3c655f1f</li> <li>https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v5.19-rc7&amp;id=a2bad844a67b1c7740bda63e87453baf63c3a7f7</li> </ul> <p>Consult the provider of your Linux distribution to confirm that those fixes are present in the Linux kernel used for your Linux distribution. Once the Hyper-V virtual machine is created, the AIC100 device must be assigned to the virtual machine. Hyper-V calls this direct device assignment (DDA) and it is the same as PCIe passthrough. Assign the AIC100 device to the Hyper-V virtual machine as follows:</p> <ol> <li>Configure the virtual machine stop action.</li> <li>Disable the device in Windows.</li> <li>Unmount the device from Windows.       This step requires the \u201c-force\u201d option to be used. AIC100 currently does not have a partitioning driver. This may be provided in the future.</li> <li>Add the device to the virtual machine.</li> </ol> <p>Details on these steps can be found at the following DDA resources:</p> <ul> <li>https://docs.microsoft.com/en-us/windows-server/virtualization/hyper-v/deploy/deploying-graphics-devices-using-dda</li> <li>https://docs.microsoft.com/en-us/windows-server/virtualization/hyper-v/deploy/deploying-storage-devices-using-dda</li> </ul> <p>To identify the AIC100 device to disable and dismount, look for a device that has the following in the \u201cHardware Ids\u201d property: <pre><code>PCI\\VEN_17CB&amp;DEV_A100&amp;SUBSYS_A10017CB&amp;REV_00\n</code></pre> Once the AIC100 device is assigned to the virtual machine, it should appear in lspci output in the virtual machine the next time the virtual machine is booted. It will have a different PCIE SBDF address than the host. Install the AIC100 software in the same manner as if you were operating a native system.</p>"},{"location":"Getting-Started/Installation/Hypervisors/hypervisor/#esxi","title":"ESXi","text":"<p>ESXi is the virtualization hypervisor from VMWare. Refer to ESXi documentation for instructions on how to create a virtual machine (VM).</p> <p>Before powering-on the VM and installing the guest OS, a few setting changes are required to assign one or more AI 100 cards to the VM. To activate passthrough, follow these steps.</p> <ol> <li>Go to \u201cHost\u201d -&gt; \u2018Manage\u2019 tab on the left bar. Then click on the \u2018Hardware\u2019 tab.</li> <li>Search for \u201cQualcomm\u201d. All the installed AI 100 cards should show up.</li> <li>Check the cards and click \u201cToggle Passthrough\u201d. Each card should then list \u201cActive\u201d under the Passthrough attribute.</li> <li>Create a VM per instructions in the ESXi documentation. Under \"virtual hardware\", \"add other device\", and select \"pci device\". This will add a new entry for the PCI device. Verify that the correct AI 100 card is selected here. Repeat this process for every AI 100 card that should be assigned to the VM.</li> <li>Setup is now complete and the VM can be powered ON. It should automatically boot the guest OS ISO and start the installer. A preview of the console is shown in the virtual machine tab when the concerned VM is selected. The preview can be clicked to be expanded and used as an interface for the VM. Walk through the OS installer like any other system.</li> </ol>"},{"location":"Getting-Started/Installation/Hypervisors/hypervisor/#xen","title":"Xen","text":"<p>Xen is an open source virtualization hypervisor that is maintained by the Xen community. Please refer to the community documentation for Xen hypervisor and virtual machine setup/operation instructions.</p> <p>Only HVM type virtual machines are supported with Qualcomm Cloud AI products. PV type virtual machines are not supported.</p> <p>The desired Cloud AI device must be assigned to the desired VM via PCI passthrough. The Xen documentation details this process, but for reference:</p> <p>Assume the Cloud AI device is at PCI address 0000:12:00.0.  A driver (xen-pciback) to support device assignment needs to be loaded in Dom0 every time Dom0 is booted - modprobe xen-pciback The desired PCI device needs to be configured for passthrough in Dom0 every time Dom0 is booted - xl pci-assignable-add 12:00.0 The passthrough configuration can be verified - xl pci-assignable-list The VM config needs to be edited to assign the device to the VM - pci         = [ '0000:12:00.0,permissive=1' ] The VM will need to be booted after the config change. The device will appear in lspci output within the VM.</p>"},{"location":"Getting-Started/Installation/Hypervisors/hypervisor/#_1","title":"Hypervisors","text":""},{"location":"Getting-Started/Installation/Pre-requisites/pre-requisites/","title":"Pre-requisites","text":""},{"location":"Getting-Started/Installation/Pre-requisites/pre-requisites/#message-signaled-interrupts","title":"Message Signaled Interrupts","text":"<p>The QAic Kernel Driver for Linux requires 32 message signaled interrupts (MSI) for best performance. QAic kernel driver does support single MSI configuration but is not recommended. On x86-based host systems, Intel VT-d or IOMMU features must be enabled in the BIOS to enable the required number of MSIs.</p> <p>For host systems using Intel chipsets, ensure that Intel Virtualization (VT-d) is enabled in the BIOS. For host systems using AMD chipsets, ensure that the IOMMU feature is enabled in the BIOS.</p>"},{"location":"Getting-Started/Installation/Pre-requisites/pre-requisites/#ubuntu-on-x86-64","title":"Ubuntu on x86-64","text":"<ul> <li>Install Ubuntu 18.04, 20.04, 22.04 or 24.04 with default kernel and minimum options</li> <li>Install the following packages:</li> </ul> Note <p>Python 3.8 64-bit is the only supported Python version by Cloud AI SDK.</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y software-properties-common \nsudo add-apt-repository ppa:ubuntu-toolchain-r/test -y\nsudo add-apt-repository ppa:deadsnakes/ppa -y\nsudo apt-get update &amp;&amp; sudo apt-get install -y build-essential git less vim-tiny nano libpci-dev libudev-dev libatomic1 python3-pip python3-setuptools python3-wheel python3.8 python3.8-dev python3.8-venv\nsudo apt-get update &amp;&amp; sudo apt-get install -y unzip zip wget ca-certificates sudo pciutils libglib2.0-dev libssl-dev snap snapd openssh-server pkg-config clang-format libpng-dev gpg\nsudo apt-get install -y libstdc++6\nsudo apt-get install -y libncurses5-dev\npython3.8 -m pip install --upgrade pip\npython3.8 -m pip install wheel numpy opencv-python onnx\n</code></pre>"},{"location":"Getting-Started/Installation/Pre-requisites/pre-requisites/#addupdate-environment-variables","title":"Add/update environment variables:","text":"<pre><code>export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/opt/qti-aic/dev/lib/x86_64\"\nexport PATH=\"/usr/local/bin:${PATH}\"\nexport PATH=\"${PATH}:/opt/qti-aic/tools:/opt/qti-aic/exec:/opt/qti-aic/scripts\"\nexport QAIC_EXAMPLES=\"/opt/qti-aic/examples\"\nexport QAIC_APPS=\"/opt/qti-aic/examples/apps\"\nexport QAIC_LIB=\"/opt/qti-aic/dev/lib/x86_64/libQAic.so\"\nexport QAIC_COMPILER_LIB=\"/opt/qti-aic/dev/lib/x86_64/libQAicCompiler.so\"\n</code></pre>"},{"location":"Getting-Started/Installation/Pre-requisites/pre-requisites/#centos-7-rhel-on-x86-64","title":"CentOS 7 / RHEL on x86-64","text":"<ul> <li>CentOS 7 / RHEL with default kernel installed</li> <li>Run all commands as 'root'.</li> <li>Install the dkms package for the kernel.<ul> <li>CentOS 7 \u2013 yum -y install dkms</li> <li>RHEL \u2013 Refer to this link on how to install EPEL packages</li> </ul> </li> <li>Install the appropriate linux-headers package for the kernel.</li> <li>Install the following packages:</li> </ul> Note <p>Python 3.8 64-bit is the only supported Python version by Cloud AI SDK.</p> <pre><code>yum update \u2013y\n[Optional] update-ca-trust force-enable\nyum install -y epel-release centos-release-scl centos-release-scl-rh\nyum install -y devtoolset-9 automake\nyum install -y git vim-minimal nano cmake pciutils-devel rpm-build systemd-devel libudev-devel python3-pip python3-setuptools python3-wheel python3-devel rh-python38-python\nyum install -y unzip zip wget ca-certificates findutils gpg openssh-server scl-utils sudo tar which pciutils git less libatomic ncurses-devel libasan glib2-devel mesa-libGL libpng-devel\n\n# Add the following lines to end of /etc/bashrc:\nsource /opt/rh/devtoolset-9/enable\nsource /opt/rh/rh-python38/enable\n\n# Start new bash shell then:\nwget https://bootstrap.pypa.io/get-pip.py -O /tmp/get-pip.py\npython3.8 /tmp/get-pip.py\npython3.8 -m pip install wheel numpy opencv-python onnx\n</code></pre>"},{"location":"Getting-Started/Installation/Pre-requisites/pre-requisites/#addupdate-environment-variables_1","title":"Add/update environment variables:","text":"<pre><code>export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/opt/qti-aic/dev/lib/x86_64\"\nexport PATH=\"/usr/local/bin:${PATH}\"\nexport PATH=\"${PATH}:/opt/qti-aic/tools:/opt/qti-aic/exec:/opt/qti-aic/scripts\"\nexport QAIC_EXAMPLES=\"/opt/qti-aic/examples\"\nexport QAIC_APPS=\"/opt/qti-aic/examples/apps\"\nexport QAIC_LIB=\"/opt/qti-aic/dev/lib/x86_64/libQAic.so\"\nexport QAIC_COMPILER_LIB=\"/opt/qti-aic/dev/lib/x86_64/libQAicCompiler.so\"\n</code></pre>"},{"location":"Getting-Started/Model-Architecture-Support/","title":"Model Architecture Support","text":"<p>The Cloud AI 100 family of accelerators supports a comprehensive range of model architectures and use-cases. </p> <ul> <li>Transformer Encoders </li> <li>Transformer Decoders</li> <li>Tranformer Encoder - Decoder (coming soon)</li> <li>Computer vision - CNN, R-CNN, vision transformers  </li> <li>Diffusion </li> </ul> <p>Multiple AI 100 SoCs with dedicated DDR memory, stacked on a accelerator card (SKUs) and/or on the server can be used to run very large models.</p>"},{"location":"Getting-Started/Model-Architecture-Support/#model-architecture-fit-guidelines","title":"Model Architecture Fit Guidelines","text":"<p>The architecture of the Cloud AI 100 accelerators is described here. The image below provides a block diagram of the Cloud AI 100 Ultra card. The only change for the Std and Pro SKUs would be that the number of SoCs per card is 1 instead of 4. </p> <p></p> <ul> <li>AI core - This is the smallest compute unit on which a neural network can be executed. </li> <li>SoC - This is the System-on-Chip that contains up to 16 AI cores. </li> <li>Accelerator Card - This is a single width PCIe form-factor card that contains one or more SoCs. </li> </ul> <p>Cloud AI 100 accelerator architecture is flexible and provides knobs to tune for highest throughput or lowest latency or a balance of both. The table below describes the categories of models that are supported across SKUs. Based on the model size, batch size, input/output sizes and data types used for activations/weights, one or more cards may be required to execute the inference.   </p> Model Family Standard Pro Ultra Transformer Encoders Yes Yes Yes Transformer Decoders Yes Yes Yes Transformer Encoder-Decoder Yes Yes Yes Computer Vision (CNN, R-CNN etc) Yes Yes Yes Diffusion Yes Yes Yes <p>Refer to model recipes in the <code>cloud-ai-sdk</code> repo for details on the number of cores used for all categories of models for highest throughput vs lowest latency. </p> <p>Models (2B parameters and below) are performant in a single SoC. </p> <p>For LLMs (7B parameters and above), the least latency is achieved in most cases through model sharding where the model is tensor-sliced and run across multiple SoCs (or cards). Refer to Model sharding feature for more information. </p> <p>Refer to performance tuning overview for some more details on the knobs used to tune performance.   </p>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/","title":"Large Language Models (LLMs)","text":"<p>Large Language Models (LLMs) represent a significant advancement in artificial intelligence, particularly in the domain of natural language processing (NLP). These models, often based on transformer architectures, have revolutionized the way machines understand and generate human-like text.</p> <p>LLMs understand context, syntactic structures, and semantic nuances within textual data.</p> <p>The transformer architecture, introduced in the paper \"Attention is All You Need\" has become the foundation for many state-of-the-art language models. This paper discusses Encoder-Decoder architecture. However, many language models use Encoder only or Decoder only architectures as well.</p> <p>In the following section, we will discuss the end-to-end workflow (from onboarding to execution of models) of decoder architecture models.</p>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#decoder-only-models","title":"Decoder-only Models","text":"<p>This architecture is particularly suitable for autoregressive tasks where sequence generation involves processing one token at a time, and contextual information from earlier tokens is crucial for predicting the next token. The inclusion of a kV cache enhances the efficiency of the decoding process, making it more computationally efficient.</p> <p>The image below provides a high level overview of the Torch vs AI 100 execution of LLMs.  </p> <p></p> <p>The ahead-of-time AI 100 compiler pre-allocates the resources on the device based on the prompt length, generation length, KV cache, batch size etc. Hardware resources are allocated as mentioned in the QPC, when the compiled model binary aka QPC is loaded on the device. The entire KV Cache (across batch size and layers) is managed in the device memory.  </p>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#llm-execution-on-cloud-ai-100-accelerators","title":"LLM execution on Cloud AI 100 Accelerators","text":"<p>To align with the autoregressive nature of the transformer decoder based LLMs, processing is divided into two stages - Prefill and Decode. </p>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#prefill-stage","title":"Prefill Stage","text":"<p>The prefill stage is a one-time process that prepares the model's context for autoregressive generation. It involves encoding the input prompt/sequence and establishing the initial state of the model. During this stage, the input prompt up to the sequence length is processed and the KV Cache is stored in the on-board DDR of Cloud AI 100.</p>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#decode-stage","title":"Decode Stage","text":"<p>The decoding stage is where the model generates the output sequence token by token. It operates in an autoregressive manner, where each token is predicted based on the context of the preceding tokens. As each token is generated, it is appended to the existing context, dynamically updating the input for subsequent predictions. This allows the model to consider its own generated output as part of the context.</p> <p>The decoding stage is an iterative process that continues until a stopping criterion is met, such as reaching a maximum length or generating an end-of-sequence token. The following figure illustrates the prefill and decode stages.</p> <p></p>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#network-specialization-overview","title":"Network Specialization Overview","text":"<p>The ONNX model used in both Prefill stage and Decode stage is the same except for model input shapes. The model used in Prefill stage is compiled for a pre-defined prompt/sequence length while the model used in decode stage is always compiled for input length of one token, as only the last generated token is provided as input to the model.</p> <p>Since the models used in Prefill stage and Decode stage are essentially the same, both the models can share the same weights during execution. Hence a feature called \"Network Specialization\" is used to pack both the ONNX files into a single QPC file while sharing the same weights. This minimizes the time required for switching between models and the memory requirements. The following figure illustrates network specialization.</p> <p></p>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#steps-to-run-the-model","title":"Steps to run the Model","text":"<ol> <li>Modify the model to make use of performance optimization features offered by Cloud AI 100.</li> <li>Generate the ONNX version of the modified model.</li> <li>Compile the modified model for Cloud AI 100 using the AIC SDK.</li> <li>Run inference on Cloud AI 100.</li> </ol>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#modify-the-model-overview","title":"Modify the Model - Overview","text":"<p>Cloud AI 100 offers various features to optimize LLM processing. Developers are required to modify the model accordingly for the best performance. The key features which would benefit from model changes are as follows:</p> <ul> <li>Static Input Shapes - Cloud AI 100 supports only static input shapes to make use of various efficiencies this approach offers. The open source LLM models generally support dynamic input shapes. So, these models need to be modified to support static input shapes.</li> <li>KV Cache storage on on-board DDR - Cloud AI 100 supports storing the KV Cache generated by the model on the on-board DDR memory. This avoids unnecessary KV cache movements between CPU and Cloud AI 100</li> <li>Automatic Attention_mask generation - The model can be modified to generate the attention_mask on Cloud AI 100 and store it in the on-board DDR of Cloud AI 100, thus removing the need to move attention_masks between CPU and Cloud AI 100 every iteration.</li> </ul>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#modify-the-model-detailed","title":"Modify the Model - Detailed","text":"<p>The Cloud-AI-SDK GitHub repo provides the modifications required and end-to-end recipes for several decoder based LLMs. </p>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#definitions","title":"Definitions","text":"<ul> <li>maximum_context_length - This the maximum number of tokens supported by the model.</li> <li>prompt_length <ul> <li>Prefill stage: This parameter represents the maximum length of the initial input or instruction that can be given to the model. This length must be fixed to support the static input shapes expected by Cloud AI 100. This is also referred to as sequence length</li> <li>Decode stage: In this stage the prompt length is always one. This is because only the last generated token is provided as input for generating the next token.  Generation length = Maximum number of tokens generated = (maximum_context_length) - (prompt_length of Prefill stage). </li> </ul> </li> <li>no_of_heads: number of Attention heads in the Transformer model.</li> <li>hidden_size: Size of the hidden state of the Transformer model.</li> <li>vocabulary_size: Vocabulary size of the Transformer model.</li> </ul>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#required-model-interface","title":"Required Model Interface","text":"<p>To use the features mentioned above, the interface of the model should be modified as follows:</p> <p>Model Inputs</p> <ul> <li>input_ids (batch_size, prompt_length, int)</li> <li>position_ids (batch_size, prompt_length, int)</li> <li>attention_mask (batch_size, maximum_context_length, bool)</li> <li>past_key_values (batch_size, no_of_heads, maximum_context_length, hidden_size, float)</li> <li>cache_index (0, int)</li> </ul> <p>Model Outputs</p> <ul> <li>logits (batch_size, 1, vocabulary_size, float)</li> <li>attention_mask_RetainedState (batch_size, maximum_context_length, bool)</li> <li>past_key_values_RetainedState (batch_size, no_of_heads, maximum_context_length, hidden_size, float)</li> </ul>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#model-changes-for-inputs","title":"Model Changes for Inputs","text":""},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#input_ids","title":"Input_Ids","text":"<ul> <li> <p>Prefill stage - The prompt needs to be of fixed shape. So, the input_ids to the model should be generated with padding. It is required to use left padded tokens during prefill stage.</p> <p>For example, if the model is compiled with a prompt length of 32 and is required to process the prompt \"Qualcomm is a San Diego based Technology company.\" The tokenization may look like:</p> <p></p> <p>So, left padding the input_ids shall look like the following:</p> <p></p> </li> <li> <p>Decode stage - The input to the model will be only one token which is the last generated token. So, the prompt_length is always 1.</p> </li> </ul>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#past_key_values","title":"past_key_values","text":"<p>Cloud AI 100 supports storing the KV Cache on the on-board DDR of Cloud AI 100. If a zero-size past_key_values input is passed to the model, the KV cache will be picked from the on-board DDR of Cloud AI 100. This avoids data transfers of KV cache between CPU and Cloud AI 100.</p> <p>Each layer of decoder in the network will have its own KV data. So, if a model has <code>n</code> layers of decoders, <code>n</code> pairs of key and value inputs will be created.</p> <p>It is recommended to use a separate tensor for KV data of each layer to attain a better performance. So, the past_key_value input of the model shall look like this, (past_value.1, past_key.1),(past_value.2, past_key.2) ... (past_value.n,past_key.n)</p>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#cache_index","title":"cache_index","text":"<p>As the input sizes are fixed, the memory required for KV Cache of entire \"maximum_context_length\" is allocated during initialization. After each iteration, KV cache needs to be stored at the correct index, similar to a scatter operation. This is done with the help of \"Cache_index\" input.</p> <p>Let's understand this with an example. Assuming the \"prompt_length\" is 32 and \"maximum_context_length\" is 256. This would create a memory pool with 256 memory slots sufficient to save 256 past key values during initialization.</p> <ul> <li> <p>Prefill stage - Since the data is yet to be processed, the cache_index in prefill stage can be set to \"0\". After the prefill stage, the first 32 slots (size of prompt_length in example) of 256 slots (Size of maximum_context_length in example) are filled with the KV data of \"input prompt\".</p> </li> <li> <p>Decode stage - The KV data of only the newly generated token is stored in this stage. In the first iteration of Decode stage the cache_index will be 33 in the example. cache_index needs to be specifically passed as input only in the first iteration of Decoder stage. The cache_index is then incremented automatically by the compiler in case of zero size input. However, the flexibility to store the data at any valid index is given to the developer. Following is an example of GPT model where the code of the model is modified to copy the KV cache of the newly generated token at correct index:</p> <pre><code>Transformers/src/transformers/models/gpt2/modeling_gpt2.py\n\nclass GPT2Attention (nn.Module):\n     if layer_past is not None:\n         past_key = layer_past[0]\n         past_value = layer_past[1]\n-        key = torch.cat((past_key, key), dim=-2)\n-        value = torch.cat((past_value, value), dim=-2)\n+        seq_length = key.shape[2]\n+        assert value.shape[2] == seq_length\n+        kv_indices = torch.arange(seq_length) + cache_index\n+        past_key[:, :, kv_indices] = key\n+        past_value[:, :, kv_indices] = value\n+        key = past_key\n+        value = past_value\n</code></pre> </li> </ul> <p>The original code would \"torch.cat\" the key and value tensors to the existing tensor creating dynamic shaped tensors. So, this part of the code must be modified.</p> <p>Key.shape[2] gives us the number of tokens processed in the current iteration. So for Prefill stage, Key.shape[2] would be maximum input prompt length, while in case of Decode stage, it would be 1. So the code change suggested above would store the newly generated key and values at the cache_index specified.</p>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#attention_mask","title":"attention_mask","text":"<ul> <li>Prefill stage - The Attention mask needs to be passed as usual.</li> <li> <p>Decode stage - The Attention mask can be generated in the model based on cache_index and saved on the on-board DDR. This avoids the need to transfer attention_mask from host to Cloud AI 100 during every iteration. This reduces the memory bandwidth requirements. In order to use the  attention_mask stored on the on-board DDR, a zero-size input is fed to the model.</p> <p>Following is the code changes needed to generate attention_mask on Cloud AI 100 for decode stage: <pre><code>Transformers/src/transformers/models/gpt2/modeling_gpt2.py\n\nclass GPT2Model(GPT2PreTrainedModel):\n+        if cache_index is not None:\n+            attention_mask[:, cache_index + seq_length - 1] = True\n+            attention_mask_retained = attention_mask\n</code></pre></p> </li> </ul>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#position_ids","title":"Position_ids","text":"<ul> <li> <p>Prefill stage - If the position IDs of the sentence \"Qualcomm is a San Diego based Technology company.\" looks like this:</p> <p></p> <p>If prompt_length is 32, the position IDs should be left padded. So, input <code>position_ids</code> tensor would look like the following:</p> <p></p> <p>In the above tensor, the first 19 elements can be any number, as they would be discarded with the help of attention_mask.</p> </li> <li> <p>Decode stage - In case of the above example, the <code>position_ids</code> of first iteration would be 13, second iteration would be 14 and so on.</p> </li> </ul>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#model-changes-for-outputs","title":"Model Changes for Outputs","text":"<p>The outputs that needs to be saved on device should end with suffix \"_RetainedState\". For example, in order to retrieve the \"attention_mask\" from on-board DDR, the output should have \"_RetainedState\" appended. So output will be \"attention_mask_RetainedState\"</p> <ul> <li>Logits Logits is the raw output of the model.</li> <li>attention_mask_RetainedState - The attention_mask generated will be stored on the on-board DDR</li> <li>past_key_values_RetainedState - The KV Cache generated by the model will be saved on the on-board DDR at the appropriate index using the \"cache_index\" input.</li> </ul>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#compile-the-model","title":"Compile the Model","text":"<p>Following is a sample compile script using <code>qaic-exec</code>. Compilation uses the Network Specialization and Custom I/O features.  Two data precision formats - FP16 or MX6 (Shared Micro-exponents), are recommended. MX6 enables smaller memory footprint by storing the model weights in MX6 (uses 6 bits vs 16 bits for FP16). MX6 will help developers run models on Cloud AI 100 SKUs in scenarios where FP16 exceeds the memory footprint and hence, cannot be used.  </p> <pre><code>``` \n#!/bin/bash\nset -e\nif [ -z \"$1\" ]; then\n  echo \"Usage: $0 &lt;model_name&gt;\"\n  exit 1\nfi\nmodel_name=\"$1\"\nprompt_len=$(grep seq_len specializations.json | head -n1 | grep -Eo '[[:digit:]]+')\nctx_len=$(grep ctx_len specializations.json | head -n1 | grep -Eo '[[:digit:]]+')\nnum_cores=16\nnum_blocks=$(grep 'value.' ${model_name}/custom_io.yaml | tail -n1 | grep -Eo '[[:digit:]]+')\n# Create qpc directory\nmkdir -p qpc\nmodel_path=\"${model_name}/generatedModels/${model_name}_simplified.onnx\"\nif [ ! -f \"$model_path\" ]; then\n  model_path=\"${model_name}/generatedModels/${model_name}.onnx\"\nfi\n\n# Compile the model\n/opt/qti-aic/exec/qaic-exec \\\n-m=$model_path \\\n  -aic-hw \\\n  -aic-hw-version=2.0 \\\n  -network-specialization-config=specializations.json \\\n  -convert-to-fp16 \\\n  -retained-state=true \\\n  -aic-num-cores=${num_cores} \\\n  -custom-IO-list-file=${model_name}/custom_io.yaml \\\n  -compile-only \\\n  -aic-binary-dir=qpc/${model_name}-kv-${prompt_len}pl-${ctx_len}cl-${num_cores}c\n```\n</code></pre>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#network-specialization","title":"Network Specialization","text":"<p>Using the \"Network Specialization\" feature, the models used for Prefill and Decode stages can be packed into a single QPC and both models will be sharing the same weights. The input shapes for prefill and decode stages are defined in a .json file for enabling \"Network Specialization\" feature. Following is an example JSON file:</p> <pre><code>``` \n \"specializations\": [\n         {\n                 \"batch_size\": \"1\",\n                 \"seq_len\": \"32\",\n                 \"ctx_len\": \"128\"\n         },\n         {\n                 \"batch_size\": \"1\",\n                 \"seq_len\": \"1\",\n                 \"ctx_len\": \"128\"\n         }\n ]\n}\n```\n</code></pre>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#custom-io","title":"Custom I/O","text":"<p>Custom I/O feature can be used to store the KV Cache in a precision needed by the model. For example, if the model is compiled for FP16 precision, The past_key_values can also be stored on on-board DDR in FP16 format. This would reduce the size of the KV cache and avoid unnecessary cast operations of the FP16 data to FP32.</p> <p>The precision for each I/O can be configured in a .yaml file and passed as an input during compile time. An example of the custom_io.yaml file is shown below.</p> <pre><code>``` \n# Model Inputs\n - IOName: past_key.0\n   Precision: float16\n - IOName: past_value.0\n   Precision: float16\n - IOName: past_key.1\n   Precision: float16\n - IOName: past_value.1\n   Precision: float16\n.\n.\n.\n.\n - IOName: past_key.26\n   Precision: float16\n - IOName: past_value.26\n   Precision: float16\n - IOName: past_key.27\n   Precision: float16\n - IOName: past_value.27\n   Precision: float16\n\n\n# Model Outputs\n - IOName: past_key.0_RetainedState\n   Precision: float16\n - IOName: past_value.0_RetainedState\n   Precision: float16\n - IOName: past_key.1_RetainedState\n   Precision: float16\n - IOName: past_value.1_RetainedState\n   Precision: float16\n   .\n   .\n   .\n - IOName: past_key.26_RetainedState\n   Precision: float16\n - IOName: past_value.26_RetainedState\n   Precision: float16\n - IOName: past_key.27_RetainedState\n   Precision: float16\n - IOName: past_value.27_RetainedState\n   Precision: float16\n```\n</code></pre>"},{"location":"Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#inference","title":"Inference","text":"<p>During inference, the prefill model is executed first and the KV cache is stored at appropriate index with left padding. The Decode stage is then executed generating one token at a time and is stored with right padding. Refer to the Cloud-AI-SDK GitHub repo for LLM model recipes. </p>"},{"location":"Getting-Started/Quick-Start-Guide/","title":"Quick Start Guide","text":"<p>This section illustrates the ease of running inference on Cloud AI platforms using a vision transformers model for classifying images. </p>"},{"location":"Getting-Started/Quick-Start-Guide/#device-status","title":"Device Status","text":"<p>Follow the checklist to make sure device is ready for use.</p>"},{"location":"Getting-Started/Quick-Start-Guide/#steps-to-run-a-sample-model-on-qualcomm-cloud-ai-platforms","title":"Steps to run a sample model on Qualcomm Cloud AI Platforms","text":""},{"location":"Getting-Started/Quick-Start-Guide/#1-import-libraries","title":"1. Import libraries","text":"<pre><code>import os, sys, requests, torch, numpy, PIL\nfrom transformers import ViTForImageClassification, ViTImageProcessor\n\nsys.path.append('/opt/qti-aic/examples/apps/qaic-python-sdk/qaic')\nimport qaic\n</code></pre>"},{"location":"Getting-Started/Quick-Start-Guide/#2-pick-a-model-from-hf","title":"2. Pick a model from HF","text":"Choose the Vision Transformers model for classifying images and its image input preprocessor<pre><code>model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n</code></pre>"},{"location":"Getting-Started/Quick-Start-Guide/#3-convert-to-onnx","title":"3. Convert to ONNX","text":"<pre><code>dummy_input = torch.randn(1, 3, 224, 224)       # Batch, channels, height, width\n\ntorch.onnx.export(model,                        # PyTorch model\n                     dummy_input,               # Input tensor\n                     'model.onnx',              # Output file\n                     export_params = True,      # Export the model parameters\n                     input_names   = ['input'], # Input tensor names\n                     output_names  = ['output'] # Output tensor names\n)\n</code></pre>"},{"location":"Getting-Started/Quick-Start-Guide/#4-compile-the-model","title":"4. Compile the model","text":"<p>Compile the model with the <code>qaic-exec</code> CLI tool. You can find more details about its usage here. This quickstart issues the command from Python.</p> <pre><code>aic_binary_dir = 'aic-binary-dir'\ncmd = '/opt/qti-aic/exec/qaic-exec -aic-hw -aic-hw-version=2.0 -compile-only -convert-to-fp16 \\\n-aic-num-cores=4 -m=model.onnx -onnx-define-symbol=batch_size,4 -aic-binary-dir=' + aic_binary_dir\n\nos.system(cmd)\n</code></pre>"},{"location":"Getting-Started/Quick-Start-Guide/#5-get-example-input","title":"5. Get example input","text":"<pre><code>url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = PIL.Image.open(requests.get(url, stream=True).raw)\n</code></pre>"},{"location":"Getting-Started/Quick-Start-Guide/#6-run-the-model","title":"6. Run the model","text":"Create the AIC100 session and prepare inputs and outputs<pre><code>vit_sess = qaic.Session(model_path= aic_binary_dir+'/programqpc.bin',\\\n   num_activations=3) # (1)\n\ninputs = processor(images=image, return_tensors='pt')\ninput_shape, input_type = vit_sess.model_input_shape_dict['input']\ninput_data = inputs['pixel_values'].numpy().astype(input_type) \ninput_dict = {'input': input_data}\n\noutput_shape, output_type = vit_sess.model_output_shape_dict['output']\n</code></pre> <ol> <li>Make sure qpcPath matches where the output is generated in <code>qaic-exec</code> above. We are activating 3 instances on network. More options.</li> </ol> Run model on AIC100<pre><code>vit_sess.setup() # Load the model to the device.\noutput = vit_sess.run(input_dict) # Execute on AIC100 now.\n</code></pre> Obtain the prediction by finding the highest probability among all classes<pre><code>logits = numpy.frombuffer(output['output'], dtype=output_type).reshape(output_shape)\npredicted_class_idx = logits.argmax(-1).item()\nprint('Predicted class:', model.config.id2label[predicted_class_idx]) \n</code></pre>"},{"location":"Getting-Started/Quick-Start-Guide/#next-steps","title":"Next Steps","text":"<p>We showed the ease of onboarding and running inference on Cloud AI platforms in this section. Refer to the User Guide for details on SDK installation, inference workflow, system management etc. </p>"},{"location":"Getting-Started/Quick-Start-Guide/#appendix","title":"Appendix","text":"<p>Input image link</p>"},{"location":"Getting-Started/Quick-Start-Guide/#full-quickstart-code","title":"Full quickstart code","text":"quickstart.py <pre><code># Copyright (c) 2023 Qualcomm Innovation Center, Inc. All rights reserved.\n# SPDX-License-Identifier: BSD-3-Clause-Clear\n# Import relevant libraries\nimport os, sys, requests, torch, numpy, PIL\nfrom transformers import ViTForImageClassification, ViTImageProcessor\n\nsys.path.append('/opt/qti-aic/examples/apps/qaic-python-sdk/qaic')\nimport qaic\n\n# Choose the Vision Transformers model for classifying images and its image input preprocessor\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n\n# Convert to ONNX\ndummy_input = torch.randn(1, 3, 224, 224)     # Batch, channels, height, width\n\ntorch.onnx.export(model,                      # PyTorch model\n                    dummy_input,              # Input tensor\n                    'model.onnx',             # Output file\n                    export_params=True,       # Export the model parameters\n                    input_names  =['input'],  # Input tensor names\n                    output_names =['output']  # Output tensor names\n)\n\n# Compile the model \naic_binary_dir = 'aic-binary-dir'\ncmd = '/opt/qti-aic/exec/qaic-exec -aic-hw -aic-hw-version=2.0 -compile-only -convert-to-fp16 \\\n-aic-num-cores=4 -m=model.onnx -onnx-define-symbol=batch_size,4 -aic-binary-dir=' + aic_binary_dir\n\nos.system(cmd)\n\n# Get example Egyptian cat image for classification\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = PIL.Image.open(requests.get(url, stream=True).raw)\n\n# Run the model\n\n## Create the AIC100 session and prepare inputs and outputs\nvit_sess = qaic.Session(model_path= aic_binary_dir+'/programqpc.bin',\\\n   num_activations=3)\n\ninputs = processor(images=image, return_tensors='pt')\ninput_shape, input_type = vit_sess.model_input_shape_dict['input']\ninput_data = inputs['pixel_values'].numpy().astype(input_type) \ninput_dict = {'input': input_data}\n\noutput_shape, output_type = vit_sess.model_output_shape_dict['output']\n\n## Access the hardware\nvit_sess.setup() # Load the model to the device.\noutput = vit_sess.run(input_dict) # Execute on AIC100 now.\n\n## Obtain the prediction by finding the highest probability among all classes.\nlogits = numpy.frombuffer(output['output'], dtype=output_type).reshape(output_shape)\npredicted_class_idx = logits.argmax(-1).item()\nprint('Predicted class:', model.config.id2label[predicted_class_idx]) \n</code></pre>"},{"location":"Getting-Started/Quick-Start-Guide/#output","title":"Output","text":"Output <pre><code>sudo python quickstart.py                  \n/usr/local/lib/python3.8/dist-packages/transformers/models/vit/modeling_vit.py:170: TracerWarning: Converting a  tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python  values, so this value will be treated as a constant in the future. This means that the trace might not  generalize to other inputs!\nif num_channels != self.num_channels:\n/usr/local/lib/python3.8/dist-packages/transformers/models/vit/modeling_vit.py:176: TracerWarning: Converting a  tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python  values, so this value will be treated as a constant in the future. This means that the trace might not  generalize to other inputs!\nif height != self.image_size[0] or width != self.image_size[1]:\n============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\nverbose: False, log level: Level.ERROR\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n\nReading ONNX Model from model.onnx\nCompile started ............... \nCompiling model with FP16 precision.\nGenerated binary is present at aic-binary-dir\nPredicted class: Egyptian cat\n</code></pre>"},{"location":"Getting-Started/System-Management/system-management/","title":"System Management","text":"<p><code>qaic-util</code> command line utility enables developers to query </p> <ul> <li>card and SoC(s) health</li> <li>firmware version</li> <li>compute/memory/IO resources available vs in-use</li> <li>card power and temperature </li> <li>status of certain device capabilities like ECC etc</li> </ul> <p>Cloud AI Platform SDK installation is required for <code>qaic-util</code> usage. </p> <p><code>QID</code> a.k.a <code>deviceID</code> are indentifiers (integers) assigned to each AI 100 SoC present in the system. Note that certain SKUs may contain more than one AI 100 SoC per Card. </p> <p><code>qaic-util</code> displays information in 2 formats:</p> <ul> <li>vertical format where cards/SoCs are queried once and the parameters are listed one per line.  <pre><code>sudo /opt/qti-aic/tools/qaic-util -q \nsudo /opt/qti-aic/tools/qaic-util -q  -d &lt;QID#&gt; #To display information for a specific `QID`\n</code></pre></li> <li>tabular format where certain parameters (compute, IO, power, temperature etc) are listed in a tabular format, refreshed every 'n' seconds (user input) <pre><code>sudo /opt/qti-aic/tools/qaic-util -q -t 1 \n</code></pre></li> </ul> <p><code>-d</code> flag can be used to display information for a specific <code>QID</code></p> <p><code>qaic-util</code>, provides --filter(-f) option along with -q and -t options, which can filter by certain device properties. Also to dump output to the .json file by using -j option.</p> <p>Examples:  To display information for a specific <code>Card</code>. <pre><code>sudo /opt/qti-aic/tools/qaic-util -q -f \"Board serial==&lt;BOARD_SERIAL_OF_CARD&gt;\"\n</code></pre> To display information for a specific <code>Card</code> in tabular format. <pre><code>sudo /opt/qti-aic/tools/qaic-util -t 1 -f \"Board serial==&lt;BOARD_SERIAL_OF_CARD&gt;\"\n</code></pre> To dump output from the qaic-util, option -j can be used, <pre><code>sudo /opt/qti-aic/tools/qaic-util -j &lt;output-file-name&gt;.json -f \"Board serial==&lt;BOARD_SERIAL_OF_CARD&gt;\"\n</code></pre></p> <p>Developers can <code>grep</code> for keywords like <code>Status</code>, <code>Capabilities</code>, <code>Nsp</code>, <code>temperature</code>, <code>power</code> to get specific information from the cards/SoCs.</p>"},{"location":"Getting-Started/System-Management/system-management/#health","title":"Health","text":"<p><code>Status</code> field indicates the health of the card. </p> <ul> <li><code>Ready</code> indicates card is in good health.</li> <li><code>Error</code> indicates card is in error condition or user lacks permissions (use <code>sudo</code>). </li> </ul> <pre><code>sudo /opt/qti-aic/tools/qaic-util -q | grep -e Status -e QID\nQID 0\n        Status:Ready\nQID 1\n        Status:Ready\nQID 2\n        Status:Ready\nQID 3\n        Status:Ready\n</code></pre> <p>Verify the function steps can be used to run a sample workload on <code>QIDs</code> to ensure HW/SW is funtioning correctly. </p>"},{"location":"Getting-Started/System-Management/system-management/#soc-reset","title":"SoC Reset","text":"<p>Developers can reset the <code>QIDs</code> using <code>soc_reset</code> sysfs node to recover the SoCs if they are in <code>Error</code> condition. These are the steps to issue a <code>soc_reset</code>. </p> <ol> <li> <p>Identify the <code>MHI ID</code> associated with the <code>QID</code></p> <p><pre><code>sudo /opt/qti-aic/tools/qaic-util -q | grep -e MHI -e QID\n</code></pre> In the sample output below, <code>MHI ID:0</code> is associated with <code>QID 0</code> and so on. </p> Note <p>MHI and QID do not always map to the same integer. It is imperative for developers to identify the mapping first before issuing the <code>soc_reset</code></p> <pre><code>sudo /opt/qti-aic/tools/qaic-util -q | grep -e MHI -e QID\nQID 0\n        MHI ID:0\nQID 1\n        MHI ID:1\nQID 2\n        MHI ID:2\nQID 3\n        MHI ID:3\n</code></pre> </li> <li> <p>Issue <code>soc_reset</code> to the <code>MHI ID</code> identified in step 1. </p> <pre><code>sudo su \necho 1 &gt; /sys/bus/mhi/devices/mhi&lt;MHI ID&gt;/soc_reset  #MHI ID is 0,1,2...  \n</code></pre> <p>Verify the health/function of the SoCs/Cards after a <code>soc_reset</code>. </p> </li> </ol>"},{"location":"Getting-Started/System-Management/system-management/#advanced-system-management","title":"Advanced System Management","text":"<p>For advanced system management details, refer to Cloud AI Card Management </p> <p>This document is shared with System Integrators and covers the following topics. </p> <ul> <li>Boot and firmware management</li> <li>Security - Secure boot enablement and attestation </li> <li>BMC integration</li> <li>Platform validation tools </li> <li>Platform error management </li> </ul>"},{"location":"Getting-Started/System-Management/system-management/#python-apis","title":"Python APIs","text":"<p>Python APIs also provide the abilty to monitor the health and resources of the cards/SoCs. Refer to Util class</p>"},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/","title":"QAIC execution provider","text":"<p>QAIC Execution Provider for ONNX Runtime enables hardware accelerated execution on Qualcomm AIC100 chipset. It leverages AIC compiler and runtime API packaged in apps, platform SDKs.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#setup","title":"Setup","text":"<ul> <li>Create a json specification file for onnxruntime as shown below     <pre><code>{\n    \"base_image\": \"ubuntu20\",\n    \"applications\": [\"onnxruntime\"],\n    \"python_version\": \"py38\",\n    \"sdk\": {\n        \"qaic_apps\": \"/path/to/apps/sdk.zip\",\n        \"qaic_platform\": \"/path/to/platform/sdk.zip\"\n    }\n}\n</code></pre></li> <li>Launch a docker container for an image built with this specification, following instructions in Docker</li> <li>In docker container, onnxruntime_qaic build (onnxruntime version 1.13.1 with qaic EP integration) will be available at /opt/qti-aic/integrations/qaic_onnxrt/onnxruntime_qaic.</li> </ul>"},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#configuration-options","title":"Configuration Options","text":"option type description config str [Required] Path to the model-settings YAML file. Contains AIC configuration parameters used by QAic execution provider of ONNX Runtime. The configuration for best performance and accuracy can be generated using model configurator tool. aic_device_id int [Optional] AIC device ID, auto-picked when not configured"},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#parameters-supported-in-model-settings-yaml-file","title":"Parameters supported in model settings yaml file","text":"Option Description Default Relevance Runtime parameters aic-binary-dir Absolute path or relative path ( wrt model settings file parent directory) to dir with programqpc.bin \"\" Required to skip compilation. device-id AIC device ID 0 Optional set-size Set Size for inference loop execution 10 Optional aic-num-of-activations Number of activations 1 Optional qaicRegisterCustomOp - Compiler C API register-custom-op Register custom op using this configuration file Required if model has AIC custom ops; vector of string Graph Config - Compiler API aic-depth-first-mem Sets DFS memory size Set by compiler Optional. Used in compilation with aic-enable-depth-first aic-enable-depth-first Enables DFS with default memory size; \"True\", \"False\" Set by compiler Optional. Used in compilation. aic-num-cores Number of aic cores to be used for inference on 1 Optional. Used in compilation. allocator-dealloc-delay Option to increase buffer lifetime 0 - 10, e.g 1 Set by compiler Optional. Used in compilation. batchsize Sets the number of batches to be used for execution 1 Optional. Used in compilation. convert-to-fp16 Run all floating-point in fp16; \"True\", \"False\" \"False\" Optional. Used in compilation. enable-channelwise Enable channelwise quantization of Convolution op; \"True\", \"False\" Set by compiler Optional. Used in compilation with pgq-profile. enable-rowwise Enable rowwise quantization of FullyConnected and SparseLengthsSum ops; \"True\", \"False\" Set by compiler Optional. Used in compilation with pgq-profile. execute-nodes-in-fp16 Run all insances of the operators in this list with FP16; \"True\", \"False\" Set by compiler Optional. Used in compilation with pgq-profile for mixed precision. hwVersion HW version of AI QAIC_HW_V2_0 Cannot be configured, set to QAIC_HW_V2_0. keep-original-precision-for-nodes Run operators in this list with original precision at generation Optional. Used in compilation with pgq-profile for mixed precision. mos Effort level to reduce the on-chip memory; eg: \"1\" Set by compiler Optional. Used in compilation. multicast-weights Reduce DDR bandwidth by loading weights used on multiple-cores only once and multicasting to other cores ols Factor to increasing splitting of network for parallelism Set by compiler Optional. Used in compilation. quantization-calibration Specify quantization calibration -\"None\", \"KLMinimization\", \"Percentile\", \"MSE\", \"SQNR\", \"KLMinimizationV2\" \"None\" Optional. Used in compilation with pgq-profile. quantization-schema-activations Specify quantization schema - \"asymmetric\", \"symmetric\", \"symmetric_with_uint8\", \"symmetric_with_power2_scale\" \"symmetric_with_uint8\" Optional. Used in compilation with pgq-profile. quantization-schema-constants Specify quantization schema -\"asymmetric\", \"symmetric\", \"symmetric_with_uint8\", \"symmetric_with_power2_scale\" \"symmetric_with_uint8\" Optional. Used in compilation with pgq-profile. size-split-granularity To set max tile size, KiB between 512 - 2048, e.g 1024 Set by compiler Optional. Used in compilation. aic-hw To set the target to QAIC_SIM or QAIC_HW; \"True\", \"False\" \"True\" Optional. Model Params - Compiler API model-path Path to model file Required. Used in compilation, OnnxRT framework. onnx-define-symbol Define an onnx symbol with its value. pairs of onnx symbol key,value separated by space. Required. Used in compilation, OnnxRT framework. external-quantization Path to load the externally generated quantization profile Optional node-precision-info Path to load model loader precision file for setting node instances to FP16 or FP32 Optional. Used in compilation with pgq-profile for mixed precision. Common relative-path aic-binary-dir absolute path will be constructed using base-path of model-settings file; \"True\", \"False\" \"False\" Optional. Set to true, to allow relative-path for aic-binary-dir."},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#usage","title":"Usage","text":""},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#python","title":"Python","text":"<p>Here are few basic commands you can use with ONNX Runtime and QAIC.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#load-a-model","title":"Load a model","text":"<pre><code>import onnxruntime as ort\nprovider_options = []  \nqaic_provider_options = {} \nqaic_provider_options['config'] = '/path/to/yaml/file' \nqaic_provider_options['device_id'] = aic_device_id \nprovider_options.append(qaic_provider_options) \nsession=onnxruntime.InferenceSession('/path/to/onnx/model', sess_options,                                                \n                                           providers = providers, provider_options = providers_options)\n</code></pre> <p>This will bind your model to AIC100 chip, with qaic exectuion provider.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#perform-inference","title":"Perform Inference","text":"<pre><code># Perform inference using OnnxRuntime\nresults = sess.run(None, {'input_name': input_data})\n</code></pre> <p>In the above code replace <code>'input_name'</code> with name of model input node and input_data with the actual input data.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#c","title":"C++","text":""},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#load-a-model_1","title":"Load a Model","text":"<pre><code>#include &lt;onnxruntime_cxx_api.h&gt;\n#include &lt;qaic_provider_factory.h&gt;\n\n\n// Set environment as required\nOrt::Env env(ORT_LOGGING_LEVEL_ERROR, \"test\");\n// Initialize session options, create session\nOrt::SessionOptions session_options;\nsession_options.SetIntraOpNumThreads(1);\nsession_options.SetGraphOptimizationLevel(\n    GraphOptimizationLevel::0);\nauto s = OrtSessionOptionsAppendExecutionProvider_QAic(\n        session_options, \"/path/to/yaml/file\", aic_device_id);\n\nOrt::Session session(env, \"/path/to/onnx/model\", session_options);\n</code></pre>"},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#perform-inference_1","title":"Perform inference","text":"<pre><code>// Run the model\n\nauto output_tensors = session.Run(Ort::RunOptions{nullptr},\ninput_names.data(), &amp;input_tensor, 1, output_names.data(), 1);\n</code></pre> <p>In the above code, replace <code>\"/path/to/onnx/model/\"</code> to the path for your onnx file. Also ensure data and shape of your input tensor match the requirements of your model.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#end-to-end-examples","title":"End-to-end examples","text":"<p>Install additional packages in the container for running End-to-end examples <pre><code>apt-get update\napt-get install -y python-yaml libpng-dev\n\npip3 install --upgrade pip\npip3 install opencv-python pyyaml scipy\n</code></pre> End to end examples (cpp and python) for resnet50 are available at - <code>/opt/qti-aic/integrations/qaic_onnxrt/tests/</code>.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#running-the-resnet-c-sample","title":"Running the ResNet C++ sample","text":"<p>Compile the Sample Resnet C++ test using build_tests.sh script. By default, test is built using libs from onnxruntime_qaic release build. To enable debugging, re-build onnxruntime_qaic project in Debug configuration and run ./build_test.sh with debug flag.  </p> <pre><code>build_tests.sh [--release|--debug]    \n</code></pre> <p>Run the executable. The commands below set the environment and run the ResNet-50 model with the provided image on QAic or CPU backend. The program outputs the most probable prediction class index for each iteration.   </p> <pre><code>cd build/release \n./qaic-onnxrt-resnet50-test -i &lt;path/to/input/png/image&gt;  \n                            -m  ../../resnet50/resnet50.yaml \n</code></pre> <p>Test options   </p> Option Description -m, --model-config [Required] Path to the model-setting yaml file -i, --input-path [Required]  Path to the input PNG image file -b, --backend [Optional] Default='qaic', Specify qaic/cpu as backend -d, --device-id [Optional]  Default=0 Specify qaic device ID -n, --num-iter [Optional]"},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#running-the-resnet-python-sample","title":"Running the ResNet Python sample","text":"<pre><code>Run test_resnet.py at /opt/qti-aic/integrations/qaic_onnxrt/tests/resnet50  \npython test_resnet.py --model_config ./resnet50/resnet50.yaml   \n                      --input_file &lt;/path/to/png/image&gt;  \n</code></pre> <p>Test options</p> Option Description --model_config [Required] Path to the model-setting yaml file --input_file [Required]  Path to the input PNG image file --backend [Optional] Default='qaic', Specify qaic/cpu as backend --device_id [Optional]  Default=0 Specify qaic device ID --num_iter [Optional]"},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#running-models-with-generic-qaic-ep-test","title":"Running models with generic QAic EP test","text":"<p><code>test_qaic_ep.py</code> is a generic test runner for compilation, execution on AIC100.</p> <p>Run <code>test_qaic_ep.py</code> at <code>/opt/qti-aic/integrations/qaic_onnxrt/tests/</code> -</p> <pre><code>python test_qaic_ep.py --model_config ./resnet50/resnet50.yaml   \n                           --input_file_list &lt;/path/to/input/list&gt;   \n</code></pre> <p>Test options    </p> Option Description --model_config [Required] Path to the model-setting yaml file --input_file_list [Required]  Path of the file (.txt) containing list of batched inputs in .raw format --backend [Optional] Default='qaic', Specify qaic/cpu as backend --device_id [Optional]  Default=0 Specify qaic device ID --num_iter [Optional] --max_threads [Optional]  Default=1000 Maximum no. of threads to run inferences --log_level [Optional]"},{"location":"ONNXRT%20QAIC/onnxruntime-rel-1.11/#execution-through-onnxruntime-test-framework","title":"Execution through Onnxruntime test framework","text":"<ul> <li> <p>QAic EP is enabled for execution with onnx_test_runner, onnxruntime_perf_test.</p> </li> <li> <p>For model directory requirements and comprehensive list of options supported, refer to onnxruntime perf test documentation.</p> </li> <li> <p>onnx_test_runner documentation.</p> </li> <li> <p>Sample testdata can be downloaded here.</p> </li> </ul> <pre><code>cd /opt/qti-aic/integrations/qaic_onnxrt/onnxruntime_qaic/build/Release\n\n./onnxruntime_perf_test -e qaic -i 'config|/path/to/resnet50.yaml aic_device_id|0' -m times -r 1000 /path/to/model.onnx\n</code></pre> <pre><code>./onnx_test_runner -e qaic /path/to/model/dir\n</code></pre>"},{"location":"ONNXRT%20QAIC/onnxruntime/","title":"QAIC execution provider","text":"<p>QAIC Execution Provider for ONNX Runtime enables hardware accelerated execution on Qualcomm AIC100 chipset. It leverages AIC compiler and runtime API packaged in apps, platform SDKs.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime/#setup","title":"Setup","text":"<ul> <li>Create a json specification file for onnxruntime as shown below     <pre><code>{\n    \"base_image\": \"ubuntu20\",\n    \"applications\": [\"onnxruntime\"],\n    \"python_version\": \"py38\",\n    \"sdk\": {\n        \"qaic_apps\": \"/path/to/apps/sdk.zip\",\n        \"qaic_platform\": \"/path/to/platform/sdk.zip\"\n    }\n}\n</code></pre></li> <li>Launch a docker container for an image built with this specification, following instructions in Docker</li> <li>In docker container, onnxruntime_qaic build (onnxruntime version 1.13.1 with qaic EP integration) will be available at /opt/qti-aic/integrations/qaic_onnxrt/onnxruntime_qaic.</li> </ul>"},{"location":"ONNXRT%20QAIC/onnxruntime/#configuration-options","title":"Configuration Options","text":"option type description config str [Required] Path to the model-settings YAML file. Contains AIC configuration parameters used by QAic execution provider of ONNX Runtime. The configuration for best performance and accuracy can be generated using model configurator tool. aic_device_id int [Optional] AIC device ID, auto-picked when not configured"},{"location":"ONNXRT%20QAIC/onnxruntime/#parameters-supported-in-model-settings-yaml-file","title":"Parameters supported in model settings yaml file","text":"Option Description Default Relevance Runtime parameters aic-binary-dir Absolute path or relative path ( wrt model settings file parent directory) to dir with programqpc.bin \"\" Required to skip compilation. device-id AIC device ID 0 Optional set-size Set Size for inference loop execution 10 Optional aic-num-of-activations Number of activations 1 Optional qaicRegisterCustomOp - Compiler C API register-custom-op Register custom op using this configuration file Required if model has AIC custom ops; vector of string Graph Config - Compiler API aic-depth-first-mem Sets DFS memory size Set by compiler Optional. Used in compilation with aic-enable-depth-first aic-enable-depth-first Enables DFS with default memory size; \"True\", \"False\" Set by compiler Optional. Used in compilation. aic-num-cores Number of aic cores to be used for inference on 1 Optional. Used in compilation. allocator-dealloc-delay Option to increase buffer lifetime 0 - 10, e.g 1 Set by compiler Optional. Used in compilation. batchsize Sets the number of batches to be used for execution 1 Optional. Used in compilation. convert-to-fp16 Run all floating-point in fp16; \"True\", \"False\" \"False\" Optional. Used in compilation. enable-channelwise Enable channelwise quantization of Convolution op; \"True\", \"False\" Set by compiler Optional. Used in compilation with pgq-profile. enable-rowwise Enable rowwise quantization of FullyConnected and SparseLengthsSum ops; \"True\", \"False\" Set by compiler Optional. Used in compilation with pgq-profile. execute-nodes-in-fp16 Run all insances of the operators in this list with FP16; \"True\", \"False\" Set by compiler Optional. Used in compilation with pgq-profile for mixed precision. hwVersion HW version of AI QAIC_HW_V2_0 Cannot be configured, set to QAIC_HW_V2_0. keep-original-precision-for-nodes Run operators in this list with original precision at generation Optional. Used in compilation with pgq-profile for mixed precision. mos Effort level to reduce the on-chip memory; eg: \"1\" Set by compiler Optional. Used in compilation. multicast-weights Reduce DDR bandwidth by loading weights used on multiple-cores only once and multicasting to other cores ols Factor to increasing splitting of network for parallelism Set by compiler Optional. Used in compilation. quantization-calibration Specify quantization calibration -\"None\", \"KLMinimization\", \"Percentile\", \"MSE\", \"SQNR\", \"KLMinimizationV2\" \"None\" Optional. Used in compilation with pgq-profile. quantization-schema-activations Specify quantization schema - \"asymmetric\", \"symmetric\", \"symmetric_with_uint8\", \"symmetric_with_power2_scale\" \"symmetric_with_uint8\" Optional. Used in compilation with pgq-profile. quantization-schema-constants Specify quantization schema -\"asymmetric\", \"symmetric\", \"symmetric_with_uint8\", \"symmetric_with_power2_scale\" \"symmetric_with_uint8\" Optional. Used in compilation with pgq-profile. size-split-granularity To set max tile size, KiB between 512 - 2048, e.g 1024 Set by compiler Optional. Used in compilation. aic-hw To set the target to QAIC_SIM or QAIC_HW; \"True\", \"False\" \"True\" Optional. Model Params - Compiler API model-path Path to model file Required. Used in compilation, OnnxRT framework. onnx-define-symbol Define an onnx symbol with its value. pairs of onnx symbol key,value separated by space. Required. Used in compilation, OnnxRT framework. external-quantization Path to load the externally generated quantization profile Optional node-precision-info Path to load model loader precision file for setting node instances to FP16 or FP32 Optional. Used in compilation with pgq-profile for mixed precision. Common relative-path aic-binary-dir absolute path will be constructed using base-path of model-settings file; \"True\", \"False\" \"False\" Optional. Set to true, to allow relative-path for aic-binary-dir."},{"location":"ONNXRT%20QAIC/onnxruntime/#usage","title":"Usage","text":""},{"location":"ONNXRT%20QAIC/onnxruntime/#python","title":"Python","text":"<p>Here are few basic commands you can use with ONNX Runtime and QAIC.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime/#load-a-model","title":"Load a model","text":"<pre><code>import onnxruntime as ort\nprovider_options = []  \nqaic_provider_options = {} \nqaic_provider_options['config'] = '/path/to/yaml/file' \nqaic_provider_options['device_id'] = aic_device_id \nprovider_options.append(qaic_provider_options) \nsession=onnxruntime.InferenceSession('/path/to/onnx/model', sess_options,                                                \n                                           providers = providers, provider_options = providers_options)\n</code></pre> <p>This will bind your model to AIC100 chip, with qaic exectuion provider.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime/#perform-inference","title":"Perform Inference","text":"<pre><code># Perform inference using OnnxRuntime\nresults = sess.run(None, {'input_name': input_data})\n</code></pre> <p>In the above code replace <code>'input_name'</code> with name of model input node and input_data with the actual input data.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime/#c","title":"C++","text":""},{"location":"ONNXRT%20QAIC/onnxruntime/#load-a-model_1","title":"Load a Model","text":"<pre><code>#include &lt;onnxruntime_cxx_api.h&gt;\n#include &lt;qaic_provider_factory.h&gt;\n\n\n// Set environment as required\nOrt::Env env(ORT_LOGGING_LEVEL_ERROR, \"test\");\n// Initialize session options, create session\nOrt::SessionOptions session_options;\nsession_options.SetIntraOpNumThreads(1);\nsession_options.SetGraphOptimizationLevel(\n    GraphOptimizationLevel::0);\nauto s = OrtSessionOptionsAppendExecutionProvider_QAic(\n        session_options, \"/path/to/yaml/file\", aic_device_id);\n\nOrt::Session session(env, \"/path/to/onnx/model\", session_options);\n</code></pre>"},{"location":"ONNXRT%20QAIC/onnxruntime/#perform-inference_1","title":"Perform inference","text":"<pre><code>// Run the model\n\nauto output_tensors = session.Run(Ort::RunOptions{nullptr},\ninput_names.data(), &amp;input_tensor, 1, output_names.data(), 1);\n</code></pre> <p>In the above code, replace <code>\"/path/to/onnx/model/\"</code> to the path for your onnx file. Also ensure data and shape of your input tensor match the requirements of your model.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime/#end-to-end-examples","title":"End-to-end examples","text":"<p>Install additional packages in the container for running End-to-end examples <pre><code>apt-get update\napt-get install -y python-yaml libpng-dev\n\npip3 install --upgrade pip\npip3 install opencv-python pyyaml scipy\n</code></pre> End to end examples (cpp and python) for resnet50 are available at - <code>/opt/qti-aic/integrations/qaic_onnxrt/tests/</code>.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime/#running-the-resnet-c-sample","title":"Running the ResNet C++ sample","text":"<p>Compile the Sample Resnet C++ test using build_tests.sh script. By default, test is built using libs from onnxruntime_qaic release build. To enable debugging, re-build onnxruntime_qaic project in Debug configuration and run ./build_test.sh with debug flag.  </p> <pre><code>build_tests.sh [--release|--debug]    \n</code></pre> <p>Run the executable. The commands below set the environment and run the ResNet-50 model with the provided image on QAic or CPU backend. The program outputs the most probable prediction class index for each iteration.   </p> <pre><code>cd build/release \n./qaic-onnxrt-resnet50-test -i &lt;path/to/input/png/image&gt;  \n                            -m  ../../resnet50/resnet50.yaml \n</code></pre> <p>Test options   </p> Option Description -m, --model-config [Required] Path to the model-setting yaml file -i, --input-path [Required]  Path to the input PNG image file -b, --backend [Optional] Default='qaic', Specify qaic/cpu as backend -d, --device-id [Optional]  Default=0 Specify qaic device ID -n, --num-iter [Optional]"},{"location":"ONNXRT%20QAIC/onnxruntime/#running-the-resnet-python-sample","title":"Running the ResNet Python sample","text":"<pre><code>Run test_resnet.py at /opt/qti-aic/integrations/qaic_onnxrt/tests/resnet50  \npython test_resnet.py --model_config ./resnet50/resnet50.yaml   \n                      --input_file &lt;/path/to/png/image&gt;  \n</code></pre> <p>Test options</p> Option Description --model_config [Required] Path to the model-setting yaml file --input_file [Required]  Path to the input PNG image file --backend [Optional] Default='qaic', Specify qaic/cpu as backend --device_id [Optional]  Default=0 Specify qaic device ID --num_iter [Optional]"},{"location":"ONNXRT%20QAIC/onnxruntime/#running-models-with-generic-qaic-ep-test","title":"Running models with generic QAic EP test","text":"<p><code>test_qaic_ep.py</code> is a generic test runner for compilation, execution on AIC100.</p> <p>Run <code>test_qaic_ep.py</code> at <code>/opt/qti-aic/integrations/qaic_onnxrt/tests/</code> -</p> <pre><code>python test_qaic_ep.py --model_config ./resnet50/resnet50.yaml   \n                           --input_file_list &lt;/path/to/input/list&gt;   \n</code></pre> <p>Test options    </p> Option Description --model_config [Required] Path to the model-setting yaml file --input_file_list [Required]  Path of the file (.txt) containing list of batched inputs in .raw format --backend [Optional] Default='qaic', Specify qaic/cpu as backend --device_id [Optional]  Default=0 Specify qaic device ID --num_iter [Optional] --max_threads [Optional]  Default=1000 Maximum no. of threads to run inferences --log_level [Optional]"},{"location":"ONNXRT%20QAIC/onnxruntime/#execution-through-onnxruntime-test-framework","title":"Execution through Onnxruntime test framework","text":"<ul> <li> <p>QAic EP is enabled for execution with onnx_test_runner, onnxruntime_perf_test.</p> </li> <li> <p>For model directory requirements and comprehensive list of options supported, refer to onnxruntime perf test documentation.</p> </li> <li> <p>onnx_test_runner documentation.</p> </li> <li> <p>Sample testdata can be downloaded here.</p> </li> </ul> <pre><code>cd /opt/qti-aic/integrations/qaic_onnxrt/onnxruntime_qaic/build/Release\n\n./onnxruntime_perf_test -e qaic -i 'config|/path/to/resnet50.yaml aic_device_id|0' -m times -r 1000 /path/to/model.onnx\n</code></pre> <pre><code>./onnx_test_runner -e qaic /path/to/model/dir\n</code></pre>"},{"location":"Python-API/qaic/qaic/","title":"qaic package","text":"<p><code>qaic</code> - qaic package provides a way to run inference on Qualcomm Cloud AI 100 card.</p>"},{"location":"Python-API/qaic/qaic/#description","title":"Description","text":"<p>A user can create a session object by passing in </p> <ol> <li> <p>with a <code>.onnx</code> file.</p> </li> <li> <p>with a precompiled qpc as model_path, in case a user already has compiled qpc. Full path to qpc.bin should be passed while using precompiled binary.</p> </li> </ol> Note <p>QPC : Qualcomm Program Container</p>"},{"location":"Python-API/qaic/qaic/#example-on-how-to-run-an-inference","title":"Example on how to run an inference","text":"option 1 : Compiles the onnx file to generate QPC and sets up session for inference<pre><code>import qaic\nimport numpy as np\nsess = qaic.Session('/path/to/model/model.onnx') \ninput_dict = {'input_name': input_data}\noutput = sess.run(input_dict)\n</code></pre> option 2 : Uses generate QPC and sets up session for inference<pre><code>import qaic\nimport numpy as np\nsess = qaic.Session('/path/to/model/qpc.bin') # option 2 : Session uses compiled QPC file to \ninput_dict = {'input_name': input_data}\noutput = sess.run(input_dict)\n</code></pre>"},{"location":"Python-API/qaic/qaic/#example-for-benchmarking","title":"Example for benchmarking","text":"<pre><code>import qaic\nsess = qaic.Session(model_path='/path/to/model', backend='aic', options_path = '/path/to/yaml') # model_path can be either onnx or precompiled qpc\ninf_completed, inf_rate, inf_time, batch_size = sess.run_benchmark()\n</code></pre>"},{"location":"Python-API/qaic/qaic/#limitations","title":"Limitations","text":"<ul> <li>Currently only QAic backend is supported. We plan support for QNN backend in future releases. </li> <li>APIs are compatible with only Python 3.8 </li> <li>These APIs are supported only on x86-64 platforms </li> </ul>"},{"location":"Python-API/qaic/qaic/#class-session","title":"class Session","text":"<p>Session is the entry point of these APIs. Session is a factory method which user needs to call to create an instance of session. A model is compiled by default when creating a session.</p> <pre><code>Session(model_path, **kwargs)\n</code></pre> <p>Session creates a session object based on the qpc provided.</p>"},{"location":"Python-API/qaic/qaic/#parameters","title":"Parameters","text":"Parameter Type Description <code>model_path</code> <code>str</code> path to .onnx file or .bin file i.e. the compiled model QPC <code>**kwargs</code> refer to the keyword arguments listed below Keyword Arguments Type Description <code>dev_id</code> <code>int</code> Device on which to run the inference. Default is 0 <code>num_activations</code> <code>int</code> Number of instances on network to be activated. <code>set_size</code> <code>int</code> Number of ExecObj to be created. <code>mos</code> <code>int</code> Effort level to reduce the on-chip memory. <code>ols</code> <code>int</code> Factor to increasing splitting of network for parallelism <code>aic_num_cores</code> <code>int</code> Number of aic cores to be used for inference <code>convert_to_fp16</code> <code>bool</code> Run all floating-point in fp16 <code>onnx_define_symbol</code> <code>(list[tuple(str, int)])</code> Define an onnx symbol with its value <code>output_dir</code> <code>str</code> Stores model binaries at directory location provided <code>output_node_names</code> <code>(list[str])</code> Output node names should be in the order as present in model file. This option is mandatory for TF models <code>model_inputs list</code> <code>dict</code> `Provide input node name with its data type and shape. Dict must contain keys 'input_name', 'input_type','input_shape'.  This is mandatory for pytorch models <code>allocator_dealloac_delay</code> <code>int</code> Option to increase the lifetime of buffers to reduce false dependencies <code>size_split_granularity</code> <code>int</code> Option to specify a maximum tile size target for operations that may be too large to execute out of fast memory. Tile size in KiB between 512 - 2048 <code>vtcm_working_set_limit_ratio</code> <code>float</code> Option to Specify the maximum ratio amount of fast memory to DDR any single instruction is allowed use of all available value between <code>execute_nodes_in_fp16</code> <code>(list[str])</code> Run all insances of the operators in this list with FP16 <code>node_precision_info_file</code> <code>str</code> Load model loader precision file which contains first output name of operator instances required to be executed in FP16 or FP32. <code>keep_original_precision_for_nodes</code> <code>(list[str])</code> Run all insances of the operators in this list with original precision during generation of quantized precision model even if the operator is supported in Int8 precision <code>custom_io_list_file</code> <code>str</code> Custom I/O config file in yaml format containing layout, precision scale and offset for each input and output of the model. <code>dump_custom_io_config_template_file</code> <code>str</code> Dumps the yaml template for Custom I/O configuration <code>external_quantization_file</code> <code>str</code> Load the externally generated quantization profile <code>quantization_schema_activations</code> <code>str</code> Specify which quantization schema to use for activations. Valid options: asymmetric, symmetric, symmetric_with_uint8 (default), symmetric_with_power2_scale <code>quantization_schema_constants</code> <code>str</code> Specify which quantization schema to use for constants. Valid options: asymmetric, symmetric, symmetric_with_uint8 <code>quantization_calibration</code> <code>str</code> Specify which quantization calibration to use Default is None (MinMax calibration is applied). Valid options: None (default), KLMinimization, KLMinimizationV2, Percentile, MSE and SQNR. <code>percentile_calibration_value</code> <code>float</code> Specify the percentile value to be used with Percentile calibration method. The specified float value must lie within 90 and 100, default: 100. <code>num_histogram_bins</code> <code>int</code> Sets the num of histogram bins that will be used in profiling every node. Default value is 512 <code>quantization_precision</code> <code>str</code> Specify which quantization precision to use. Int8(default) is only supported precision for now. <code>quantization_precision_bias</code> <code>str</code> Specify which quantization precision to use. Value options: Int8, Int32 (default) <code>enable_rowwise</code> <code>bool</code> Enable rowwise quantization of FullyConnected and SparseLengthsSum ops. <code>enable_channelwise</code> <code>bool</code> Enable channelwise quantization of Convolution op. <code>dump_profile</code> <code>str</code> Perform quantization profiling for a given graph and dump result to the file. Compilation will be done after dumping profile unlike qaic-exec <code>load_profile</code> <code>str</code> Load quantization profile file and quantize the graph. The profile file to be loaded is the one which is dumped through option -dump-profile. <code>convert_to_quantize</code> <code>bool</code> If -load-profile option is not provided then input data is profiled and run in quantized mode. Default is off. Also set-quantization-* options as per requirement. Do not use this option along with -dump-profile or -load-profile. <code>load_embedding_tables</code> <code>str</code> Load embedding tables from this zip file for DLRM and RecSys models. <code>dump_embedding_tables</code> <code>str</code> Extract embedding tables from pytorch model and dump them in the zip file specified. <code>mdp_load_partition_config</code> <code>str</code> Load config file for partitioning a graph across devices. <code>mdp_dump_partition_config</code> <code>str</code> Dump config file for partitioning a graph across devices. <code>host_preproc</code> <code>bool</code> Enable all pre-/post-processing on host <code>aic_preproc</code> <code>bool</code> Disable all pre-/post-processing on host. Operations are performed on AI 100 instead. <code>aic_enable_depth_first</code> <code>bool</code> Enables DFS with default memory size <code>aic_depth_first_mem</code> <code>int</code> Sets DFS memory size. number must be choosen from [8,32] <code>stats_batchsize</code> <code>int</code> This option is used to normalize performance statistics to be per inference <code>always_expand_onnx_functions</code> <code>bool</code> This option forces the expansion ONNX functions. <code>enable_debug</code> <code>bool</code> Enables debug mode during model compilation <code>time_passes</code> <code>bool</code> Enables printing of compile-time statistics <code>io_crc</code> <code>bool</code> Enables CRC check for inputs and outputs of the network. <code>io_crc_stride</code> <code>int</code> Specifies size of stride to calculate CRC in the stride section <code>sdp_cluster_sizes</code> <code>(list[int])</code> Enables single device partitioning and sets the cluster configuration <code>profiling_threads</code> <code>int</code> This option is used to assign the number of threads to use for for quantization profile generation <code>compile_threads</code> <code>int</code> Sets the number of parallel threads used for compilation. <code>use_producer_dma</code> <code>bool</code> Initiate NSP DMAs from the thread that produces data being transferred <code>aic_perf_warnings</code> <code>bool</code> Print performance warning messages <code>aic_perf_metrics</code> <code>bool</code> Print compiler performance metrics <code>aic_pmu_recipe</code> <code>str</code> Enable the PMU selection based on built-in recipe: AxiRd, AxiWr, AxiRdWr, KernelUtil, HmxMacs <code>aic_pmu_events</code> <code>str</code> Track events in NSP cores. Up to 8 events are supported <code>dynamic_shape_input</code> <code>(list[str])</code> Inform the compiler which inputs should be treated as having dynamic shape <code>multicast_weights</code> <code>bool</code> Reduce DDR bandwidth by loading weights used on multiple-cores only once and multicasting to other cores. <code>ddr_stats</code> <code>bool</code> Used to collect DDR traffic details at per core level. <code>combine_inputs</code> <code>bool</code> When enabled combines inputs into fewer buffers for  transfer to device. <code>combine_outputs</code> <code>bool</code> When enabled combines outputs into a single buffer for transfer to host. <code>enable_metrics</code> <code>bool</code> Set value to True if you are interested in getting performance metrics for inference runs on a session. (Can not be used if enable_profiling is set to True.) <code>enable_profiling</code> <code>bool</code> Set value to True if you want to profile the inferences and get performance metrics for inference runs on a session. (Can not be used if enable_metrics is set to True.)"},{"location":"Python-API/qaic/qaic/#returns","title":"Returns","text":"<p>Session object.</p>"},{"location":"Python-API/qaic/qaic/#example","title":"Example","text":"using options_path yaml file<pre><code>sess = qaic.Session('/path/to/model', options_path = '/path/to/options.yaml') \ninput_dict = {'input_name': input_data}\noutput = sess.run(input_dict)\n</code></pre> sample contents of yaml file<pre><code>aic_num_cores: 4\nnum_activations: 1\nconvert_to_fp16: true\nonnx_define_symbol:\n  batch: 1\noutput_dir: './resnet_qpc'\n</code></pre> using keyword args<pre><code>    sess = qaic.Session('/path/to/model_qpc/*.bin', num_activations=4, set_size=10)\n    input_dict = {'input_name': input_data}\n    output = sess.run(input_dict)\n</code></pre>"},{"location":"Python-API/qaic/qaic/#api-list-function-variables-for-session-object","title":"API List (Function variables for session object)","text":"<p>Session class has following methods.</p>"},{"location":"Python-API/qaic/qaic/#backend_options","title":"backend_options()","text":"<p>Returns</p> <p>A dict of options that can be configured after creating session </p> Usage example<pre><code>backend_options_dict = session.backend_options()\n</code></pre>"},{"location":"Python-API/qaic/qaic/#get_metrics","title":"get_metrics()","text":"<p>Returns</p> <p>A dictionary containing the following metrics:</p> <pre><code>- num_of_inferences (int): The number of inferences.\n- min_latency (float): The minimum inference time.\n- max_latency (float): The maximum inference time.\n- P25 (float): The 25th percentile latency.\n- P50 (float): The 50th percentile latency (median).\n- P75 (float): The 75th percentile latency.\n- P90 (float): The 90th percentile latency.\n- P99 (float): The 99th percentile latency.\n- P999 (float): The 99.9th percentile latency.\n- total_inference_time (float): The sum of individual insference times.\n- avg_latency (float): The average latency.\n</code></pre> Usage example<pre><code>metrics_dict = session.get_metrics()\n</code></pre>"},{"location":"Python-API/qaic/qaic/#model_input_shape_dict","title":"model_input_shape_dict()","text":"<p>Returns</p> <p>A dict with input_name as key and input_shape, input_type as values     </p> Usage example<pre><code>input_shape_dict = session.model_input_shape_dict() \n</code></pre>"},{"location":"Python-API/qaic/qaic/#model_output_shape_dict","title":"model_output_shape_dict()","text":"<p>Returns</p> <p>A dict with output_name as key and output_shape, output_type as values  </p> Usage example<pre><code>output_shape_dict = session.model_output_shape_dict()\n</code></pre>"},{"location":"Python-API/qaic/qaic/#print_metrics","title":"print_metrics()","text":"<p>Returns</p> <p><code>None</code></p> Usage example<pre><code>session.print_metrics() \n</code></pre> Note <p>This method assumes that either the 'enable_profiling' or 'enable_metrics' attribute is set to True.</p> <p>Sample Output:</p> <pre><code>Number of inferences utilized for calculation are 999\nMinimum latency observed 0.0009578340000000001 s\nMaximum latency observed 0.002209001 s\nAverage latency / inference time observed is 0.0012380756316316324 s\nP25 / 25% of inferences observed latency less than 0.001095435 s\nP50 / 50% of inferences observed latency less than 0.0012522870000000001 s\nP75 / 75% of inferences observed latency less than 0.001299786 s\nP90 / 90% of inferences observed latency less than 0.002209001 s\nP99 / 99% of inferences observed latency less than 0.0016082370000000002 s\nSum of all the inference times 1.2368375560000007 s\nAverage latency / inference time observed is 0.0012380756316316324 s\n</code></pre>"},{"location":"Python-API/qaic/qaic/#print_profile_data","title":"print_profile_data","text":"<p>Returns</p> <p><code>None</code></p> Usage example<pre><code>session.print_profile_data(n)   \n</code></pre> <p>Print profiling data for the first n iterations</p> Note <p>This function only works when 'enable_profiling' is set to True for the Session.</p> <ul> <li>This method assumes that the 'enable_profiling' attribute is set to True, and the 'profiling_results' attribute contains the profiling data for each iteration.</li> <li>The method prints the profiling data in a tabular format, including the file, line, function, number of calls, function time (seconds), and total time (seconds) for each function.</li> </ul> <p>Sample Output:</p> <pre><code>|  File-Line-Function  | |  num calls  | |  func time  | |  tot time  |\n\n('~', 0, \"&lt;method 'astype' of 'numpy.ndarray' objects&gt;\") 1 0.000149101 0.000149101\n\n('~', 0, '&lt;built-in method numpy.empty&gt;') 1 2.38e-06 2.38e-06\n\n('~', 0, '&lt;built-in method numpy.frombuffer&gt;') 1 4.22e-06 4.22e-06\n</code></pre>"},{"location":"Python-API/qaic/qaic/#reset","title":"reset()","text":"<p>Returns</p> <p><code>None</code> </p> Usage example<pre><code>session.reset() \n</code></pre> <p>Releases all the device resources acquired by session </p>"},{"location":"Python-API/qaic/qaic/#setup","title":"setup()","text":"<p>Returns</p> <p><code>None</code> </p> Usage example<pre><code>session.setup() \n</code></pre> <p>Loads the network to the device.</p> <p>Network is usually loaded during first call of run. If this is called before that, network will be already loaded when first run is called.</p>"},{"location":"Python-API/qaic/qaic/#runinput_dict","title":"run(input_dict)","text":"<p>Returns</p> <p>A dict with output_name and output_value of inference   </p> Usage example<pre><code>output = session.run(input_dict)    \n</code></pre> <p>input_dict should have input_name as key and value should be a numpy array</p>"},{"location":"Python-API/qaic/qaic/#run_benchmark","title":"run_benchmark()","text":"<p>Returns</p> <p>inf_completed: Total number of inferences run inf_rate: Inf/Sec of the model inf_time: total time taken to run inferences batch_size: Batch Size used by model</p> Usage example<pre><code>inf_completed, inf_rate, inf_time, batch_size = session.run_benchmark() \n</code></pre> <p>It accepts following args:</p> <p>num_inferences: num of inferences to run in benchmarking. Default 40 inf_time: duration for which inference is to be run in seconds. Default None input_dict: Input to be used in inference. Default random</p> Note <p>num_inferences and time cannot be used together.</p> <p>This API uses C++ benchmarking APIs and doesn't take into account python overheads</p>"},{"location":"Python-API/qaic/qaic/#update_backend_optionskwargs","title":"update_backend_options(**kwargs)","text":"<p>Returns</p> <p><code>None</code> </p> Usage example<pre><code>session.update_backend_options(num_activations = 2)     \n</code></pre> <p>Update option specified in kwargs</p> <p>For example:</p> <p><code>num_activation</code>, <code>dev_id</code>, <code>set_size</code> can be configured with this API.</p>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/","title":"class InferenceBenchmarkSet","text":"<p>Help on class InferenceBenchmarkSet in module <code>qaicrt</code>:</p>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#inferencebenchmarksetlogger","title":"InferenceBenchmarkSet(Logger)","text":"<p><pre><code>InferenceBenchmarkSet(context, qpc, devID, setSize, numActivations, inferenceDataVector, properties) -&gt; infrenceBenchmarkSet\n</code></pre> Creates and returns a new object for running benchmarks.</p> <p>Parameters</p> Parameter Description <code>context</code> A previously created context. <code>qpc</code> A previously created Qpc Object. <code>devID</code> [optional] Device on which benchmark is to be run. By default device will be auto-picked <code>setSize</code> Number of ExecObj per program. <code>numActivations</code> Number of separate activations. Set this value to 1 to have a single instance of the program on device Set this value to a specific number of activations. <code>inferenceDataVector</code> A vector of QBuffers for initial input and output data. This is the first set of data to be use for the first submission during benchmark. If new data is needed for each inference, the user can register a callback which will return the ExecObj pointer, which can be used to  setData for the next inference. Alternatively, the same data can be used repeatedly <code>properties</code> [optional] Properties for the program and queues to created by the benchmark set. One of the key properties is number of threads per queue which can be changed in the queue properties <p>Once the Benchmark object is created, the specified program will be initialized. The inferenceDataVector must be compatible with the program. Depending on the <code>setSize</code> and <code>numActivations</code> configured, <code>ExecObj(s)</code>  will be created per program. <code>numInferencesToRun</code> will be completed each time the start signal is given through <code>run()</code> method. </p> <p>Methods defined here:</p>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#init","title":"init","text":"<pre><code>__init__(self: qaicrt.InferenceBenchmarkSet, context: qaicrt.Context, qpc: qaicrt.Qpc, dev: Optional[int] = None, setSize: int, numActivations: int, inferenceDataVector: List[qaicrt.QBuffer], properties: qaicrt.BenchmarkProperties = None) -&gt; None\n</code></pre>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#disable","title":"disable","text":"<p><pre><code>disable(self: qaicrt.InferenceBenchmarkSet) -&gt; qaicrt.QStatus\n</code></pre> Description</p> <p>Disable InferenceBenchmarkSet.  This will deactivate and release all resources associated with this benchmark.</p> <p>Returns</p> <ul> <li>Status:<ul> <li>qaicrt.QStatus.QS_SUCCESS Successful completion</li> <li>qaicrt.QStatus.QS_ERROR Failed to release benchmark object programs and resources</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#disableexecobjcompletioncallback","title":"disableExecObjCompletionCallback","text":"<pre><code>disableExecObjCompletionCallback(self: qaicrt.InferenceBenchmarkSet) -&gt; None\n</code></pre> <p>Description: </p> <p>Disable ExecObj Completion Callback</p>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#disableopstatscallback","title":"disableOpStatsCallback","text":"<pre><code>disableOpStatsCallback(self: qaicrt.InferenceBenchmarkSet) -&gt; qaicrt.QStatus\n</code></pre> <p>Returns:</p> <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#enable","title":"enable","text":"<pre><code>enable(self: qaicrt.InferenceBenchmarkSet) -&gt; qaicrt.QStatus\n</code></pre> <p>Description: </p> <p>Enable InferenceBenchmarkSet. This will trigger the load and activation of all the program instances on device.  This may fail if resources are not available.</p> <p>Returns: </p> <p>Operational status</p> <ul> <li>Operational status:<ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Failed to setup benchmark object programs and resources</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#enableexecobjcompletioncallback","title":"enableExecObjCompletionCallback","text":"<pre><code>enableExecObjCompletionCallback(self: qaicrt.InferenceBenchmarkSet, callback: Callable[[qaicrt.ExecObj, qaicrt.ExecObjInfo], None]) -&gt; None\n</code></pre> <p>Description</p> <p>Enable ExecObj Completion Callback. When enabled, each time an ExecObj completes, this callback will be called with the ExecObj that completed. This can be used to perform data validation or to extract the inference results.</p> <p>Parameters</p> Parameter Description <code>callback</code> Callback function <p>Returns</p> <p>Operational status.</p> <ul> <li>Operational Status: <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#enableopstatscallback","title":"enableOpStatsCallback","text":"<pre><code>enableOpStatsCallback(self: qaicrt.InferenceBenchmarkSet, callback: Callable[[qaicrt.vector_char, int, qaicrt.QAicOpStatsFormatEnum, int, qaicrt.ExecObjInfo], None], initialSampleIndex: int, numSamplesPerProgramInstance: int, format: qaicrt.QAicOpStatsFormatEnum) -&gt; qaicrt.QStatus\n</code></pre> <p>Description</p> <p>Enable Operational Statistics Callback. OpStats are data collected by the AIC neural network core during execution.</p> <p>Parameters </p> Parameter Description <code>callback</code> Callback function <code>initialSampleIndex</code> The index from 0 to the first ExecObj completion the callback should be called. For example, if set to 3, starting at the 3rd ExecObj completion the callback will be called function <code>format</code> The format of the data to be collected. This may be ASCII or JSON <p>Returns</p> <p>Operational status.</p> <ul> <li>Operational Status: <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Invalid parameter format</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#getinfcompleted","title":"getInfCompleted","text":"<pre><code>getInfCompleted(self: qaicrt.InferenceBenchmarkSet) -&gt; List[int]\n</code></pre> <p>Description</p> <p>Get the number of inferences completed.</p> <p>Returns</p> <p>The number of executions completed, not accounting for the batchSize, and for all activations.</p>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#getlastrunstats","title":"getLastRunStats","text":"<pre><code>getLastRunStats(self: qaicrt.InferenceBenchmarkSet) -&gt; Tuple[int, float, int, int]\n</code></pre> <p>Description</p> <p>Get stats on the last run of the InferenceBenchmarkSet</p> <p>Returns</p> <ul> <li>Tuple of <code>infCompleted</code>, <code>infRate</code>, <code>runtimeUs</code> and <code>batchSize</code>.<ul> <li><code>infCompleted</code>   : Number of inferences completed for all activations.</li> <li><code>infRate</code>        : Inference Rate in inferences per second for all activations.</li> <li><code>runtimeUs</code>      : Duration in microseconds of the last run.</li> <li><code>batchSize</code>      : Number of individual inferences completed in one execution of ExecObj.                    When the program is compiled it may be configured to have a batchsize                    of 1 or larger. The effective inference rate is the infRate multiplied                    by the batchSize.</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#run","title":"run","text":"<pre><code>run(self: qaicrt.InferenceBenchmarkSet, numInferencesToRun: int) -&gt; qaicrt.QStatus\n</code></pre> <p>Description</p> <p>Run the benchmark. This call will block until the numInferencesToRun are completed.</p> <p>Parameters</p> Parameter Description <code>numInferencesToRun</code> Number of inferences to run <p>Returns</p> <ul> <li>Operational Status.<ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Invalid param <code>numInferencesToRun</code></li> <li><code>qaicrt.QStatus.QS_ERROR</code> Inference run failed</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#runforduration","title":"runForDuration","text":"<pre><code>runForDuration(self: qaicrt.InferenceBenchmarkSet, duration: int) -&gt; qaicrt.QStatus\n</code></pre> <p>Description</p> <p>Run the benchmark. This call will block until all the submitted inferences are completed. </p> <p>Parameters</p> Parameter Description <code>duration</code> Duration for which to keep submitting new inference <p>Returns</p> <ul> <li>Operational Status.<ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Invalid param duration</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Inference run failed</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#setliveprofilinglevel","title":"setLiveProfilingLevel","text":"<pre><code>setLiveProfilingLevel(self: qaicrt.InferenceBenchmarkSet, level: qaicrt.LiveReportingLevelEnum) -&gt; qaicrt.QStatus\n</code></pre> <p>Description</p> <p>Set the reporting level once liveReporting is enabled. Will retain the same period of reporting, but will use the new reporting level specified.</p> <p>Parameters</p> Parameter Description <code>level</code> New reporting level.  levels not defined <p>Returns</p> <ul> <li>Operational Status.<ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Live reporting is not enabled, level change fails</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#setlivereporting","title":"setLiveReporting","text":"<pre><code>setLiveReporting(self: qaicrt.InferenceBenchmarkSet, level: qaicrt.LiveReportingLevelEnum = &lt;LiveReportingLevelEnum.LIVE_REPORTING_SUMMARY: 1&gt;, reportingPeriodMs: int = 1000) -&gt; qaicrt.QStatus\n</code></pre> <p>Description</p> <p>Enable live reporting. Live reporting will print to sys.stdout a summary of the  inferences completed at the given period. The data printed may be detailed or summarized, depending on the level selected.</p> <p>Parameters</p> Parameter Description <code>level</code> Level of detail requested for periodic live reporting.  levels not defined <code>reportingPeriodMs</code> The period at which the report will be generated, in milliseconds. <p>Returns</p> <ul> <li>Operational Status.<ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Invalid param level</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#static-methods","title":"Static methods","text":"<pre><code>initDefaultProperties from builtins.PyCapsule\ninitDefaultProperties(properties: qaicrt.BenchmarkProperties) -&gt; None\n</code></pre> <p>Initialize properties for InferenceBenchmarkSet to default.</p> <p>Parameters</p> Parameter Description properties Properties to initialize."},{"location":"Python-API/qaicrt/class_util/","title":"class Util","text":"<p>Help on class Util in module <code>qaicrt</code>:</p> <p>Utility class to get information on the AIC device. Supports basic operations on device such as getting device ID, device information, resource information, and so on.</p>"},{"location":"Python-API/qaicrt/class_util/#util","title":"Util()","text":"<p><code>Util() -&gt; util</code></p> <p>Creates and returns a new util object.</p> <p>Methods defined here:</p>"},{"location":"Python-API/qaicrt/class_util/#init","title":"init","text":"<pre><code>__init__(self: qaicrt.Util) -&gt; None\n</code></pre>"},{"location":"Python-API/qaicrt/class_util/#checklibraryversion","title":"checkLibraryVersion","text":"<pre><code>checkLibraryVersion(self: qaicrt.Util) -&gt; qaicrt.QStatus\n</code></pre> <p>Description: </p> <p>Checks Library Version with AicApi header.</p> <ul> <li>Major library version should be equal to LRT_LIB_MAJOR_VERSION.</li> <li>Minor library version should be less than LRT_LIB_MINOR_VERSION.</li> </ul> <p>Returns:</p> <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Driver init failed</li> </ul>"},{"location":"Python-API/qaicrt/class_util/#getaicversion","title":"getAicVersion","text":"<pre><code>getAicVersion(self: qaicrt.Util) -&gt; Tuple[qaicrt.QStatus, int, int, str, str]\n</code></pre> <p>Description: </p> <p>Gets Aic version of the library.</p> <p>Returns: </p> <p>Tuple of operational status, major, minor, patch, variant</p> <ul> <li>Operational status:<ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Internal error in getting aic version</li> </ul> </li> <li>major: Major version of the library</li> <li>minor: Minor version of the library</li> <li>patch: Patch being used by the library</li> <li>variant: Variant of the library being used</li> </ul>"},{"location":"Python-API/qaicrt/class_util/#getdeviceids","title":"getDeviceIds","text":"<pre><code>getDeviceIds(self: qaicrt.Util, deviceType: qaicrt.QAicDeviceType = &lt;QAicDeviceType.QAIC_DEVICE_TYPE_DEFAULT: 1&gt;) -&gt; Tuple[qaicrt.QStatus, qaicrt.QIDList]\n</code></pre> <p>Description</p> <p>Get the list of AIC devices. Optional argument deviceType specifies the type of the device.</p> <p>Parameters</p> Parameter Description <code>deviceType</code> [Optional] Type of device <p>Returns</p> <p>Tuple of operational status and the list of AIC devices.</p> <ul> <li>Operational Status: <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Internal error in getting device list</li> <li><code>qaicrt.QStatus.QS_NODEV</code> No valid device</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Driver init failed or Bad device AddrInfo</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_util/#getdeviceinfo","title":"getDeviceInfo","text":"<p><pre><code>getDeviceInfo(self: qaicrt.Util, devID: int) -&gt; Tuple[qaicrt.QStatus, qaicrt.QDevInfo]\n</code></pre> Description</p> <p>Get device information for the device specified. </p> <p>Parameters </p> Parameter Description <code>devID</code> A valid device ID returned from getDeviceIds() <p>Returns</p> <p>Tuple of operational status and Device info.</p> <ul> <li>Operational Status: <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Internal error in getting device info</li> <li><code>qaicrt.QStatus.QS_NODEV</code> No valid device</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Driver init failed</li> <li><code>qaicrt.QStatus.QS_AGAIN</code> Error on reload Device Id</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_util/#getperformanceinfo","title":"getPerformanceInfo","text":"<p><pre><code>getPerformanceInfo(self: qaicrt.Util, devID: int) -&gt; Tuple[qaicrt.QStatus, qaicrt.QPerformanceInfo]\n</code></pre> Description</p> <p>Get device performance information, this is a simple query that returns performance info. Note the same information is availablethrough GetDeviceInfo, however this is a more lightweight ap allowing to retrieve only the performance info.</p> <p>Parameters</p> Parameter Description <code>devID</code> A valid device ID returned from getDeviceIds() <p>Returns</p> <p>Tuple of operational status and Performance info.</p> <ul> <li>Operational Status: <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Internal error in getting performance info</li> <li><code>qaicrt.QStatus.QS_NODEV</code> Device not found, the device ID provided is not available</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Driver init failed</li> <li><code>qaicrt.QStatus.QS_DEV_ERROR</code> Device validity error</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_util/#getresourceinfo","title":"getResourceInfo","text":"<pre><code>getResourceInfo(self: qaicrt.Util, devID: int) -&gt; Tuple[qaicrt.QStatus, qaicrt.QResourceInfo]\n</code></pre> <p>Description</p> <p>Get device performance information. This is a simple query that returns status of dynamic resources such as free memory, etc..Note that the same information is available through GetDeviceInfo,however this is a more lightweight api allowing to retrieve only the resourceinfo.</p> <p>Parameters</p> Parameter Description <code>devID</code> A valid device ID returned from getDeviceIds() <p>Returns</p> <p>Tuple of operational status  and Resource info.</p> <ul> <li>Operational Status: <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Internal error in getting resource info</li> <li><code>qaicrt.QStatus.QS_NODEV</code> Device not found, the device ID provided is not available</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Driver init failed</li> <li><code>qaicrt.QStatus.QS_DEV_ERROR</code> Device validity error</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_util/#lockdevice","title":"lockDevice","text":"<pre><code>lockDevice(self: qaicrt.Util, devID: int, lock: bool, block: bool) -&gt; qaicrt.QStatus\n</code></pre> <p>Description</p> <p>Lock a device for exclusive access. This is an advisory lock, so if an uncooperative app accesses a locked device the operations are not blocked. This functionality is enabled by setting env var QAIC_SERIALIZE_DEVICE to 1.If the env var is unset, the call returns QS_UNSUPPORTED.</p> <p>Parameters</p> Parameter Description <code>devID</code> A valid device ID returned from getDeviceIds() <code>lock</code> locks or unlocks the device <code>block</code> Blocking or non-blocking. No-op if lock is false. <p>Returns</p> <p>Operational Status.</p> <ul> <li>Operational Status<ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_NODEV</code> Device not found</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Driver init failed</li> <li><code>qaicrt.QStatus.QS_UNSUPPORTED</code> Env var for advisory lock not set</li> <li><code>qaicrt.QStatus.QS_BUSY</code> Device locked</li> </ul> </li> </ul>"},{"location":"blogs/AmberChat/","title":"Accelerate Inference of Fully Transparent Open-Source LLMs from LLM360 on Qualcomm\u00ae Cloud AI 100 DL2q Instances","text":"<p>Posted By Ravi Sivalingam</p> <p>Many popular Large Language Models (LLMs) are closed and/or limited by their licensing to specific use-cases, which limits the democratization of AI progress. LLM360 is a joint effort of Petuum, MBZUAI, and Cerebras, with a focus on providing fully accessible and open-source LLMs. Amber and AmberChat are two such open-source LLMs released under the LLM360 initiative. Amber-7B is a foundation English language model, following the Llama2-7B architecture, and AmberChat-7B is the instruction fine-tuned version for chat applications.</p> <p>As part of Qualcomm Technologies' \"Developer First\" strategy and in general support of open-source initiatives, the Qualcomm Cloud AI 100 inference accelerator now supports the Amber and AmberChat models, which are accelerated using microscaling formats and on-device caching of key-value tensors. You can run these models on Qualcomm Cloud AI 100 Standard cards using the DL2q instances on AWS EC2 in a few simple steps. Check out the model recipe on our Qualcomm Cloud AI 100 Github page to run the Amber or AmberChat models.</p>"},{"location":"blogs/AmberChat/#qualcomm-cloud-ai-100-optimizations-for-llm-inference","title":"Qualcomm Cloud AI 100 Optimizations for LLM Inference:","text":"<p>A variety of compute and memory optimizations in both the hardware and software deliver the best-in-class performance-per-TCO$ for LLMs on Qualcomm Cloud AI 100. While these are covered in earlier articles [2] [3], two key optimizations to highlight specifically for these models are:</p> <ul> <li> <p>Microscaling Format: The model weights are compressed to MXFP6     using the new microscaling formats (Open Compute Project MX     Specifications     [4])     to reduce weight memory footprint and to boost the effective memory     bandwidth of the system. The computations are performed in FP16. For     more details, check out our blog on Microscaling Formats     [5]</p> </li> <li> <p>On-device Caching of key-value tensors for past tokens over the     model's entire context length to:</p> <ul> <li>Avoid re-computation during the autoregressive generation in LLMs and</li> <li>Avoid movement of numerous large key-value tensors back and forth between host and device.</li> </ul> </li> </ul> <p>As LLM inference especially in the decode phase is primarily memory-bound, these optimizations significantly boost the LLM performance and improve the end-user experience in an inference application deployed using these LLMs.</p>"},{"location":"blogs/AmberChat/#performance-numbers-on-qualcomm-cloud-ai-100-standard-sku","title":"Performance numbers on Qualcomm Cloud AI 100 Standard SKU:","text":"<p>The performance of AmberChat-7B on a single Cloud AI 100 Standard SKU for a batchsize of 1 is listed in the table below. Note that Amber and AmberChat have the same architecture, so their performance should be identical. A single DL2q instance contains 8 Standard SKU cards, so we can run 8 replicas of these models on a single instance, 1 on each card. We can further increase the throughput performance by increasing the batchsize.</p> Model Device Precisionof Weights Replicas Batch Size CompiledPrompt Length MaximumContext Length PrefillLatency (sec) DecodeThroughput (tok/s) AmberChat-7B 1x STD SKU MXFP6 1 1 128 2048 1.33 14.9 8x STD SKU DL2q MXFP6 8 1 128 2048 1.33 119.2"},{"location":"blogs/AmberChat/#sample-prompts-and-generated-responses","title":"Sample Prompts and Generated Responses:","text":"<p>Here are some sample prompts and the corresponding responses generated by AmberChat-7B. We used the prompt template from the model's HuggingFace Model Card:</p> <p>Prompt Template:</p> <p>A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.</p> <p>### Human: {prompt}</p> <p>### Assistant:</p> <p>Prompt: How do I mount a TV to drywall safely?</p> <p>Response: To mount a TV to dry wall safely, you will need the following tools:</p> <ol> <li>A drill</li> <li>Wall anchors</li> <li>Screws</li> <li>A stud finder</li> <li>A level</li> <li>A screwdriver</li> <li>A stud mounting bracket</li> <li>A TV mounting bracket</li> </ol> <p>Here are the steps to mount a TV to dry wall safely:</p> <ol> <li>Use a stud finder to locate the studs in the wall where you want to mount the TV.</li> <li>Use a level to make sure the TV is level before mounting it.</li> <li>Drill holes in the dry wall where the wall anchors will go.</li> <li>Insert the wall anchors into the holes and screw them into the studs.</li> <li>Use screws to secure the TV mounting bracket to the wall anchors.</li> <li>Place the TV on the mounting bracket and adjust the level until it is straight.</li> <li>Secure the TV to the mounting bracket with screws.</li> <li>Use a screwdriver to tighten the screws and make sure the TV is secure.</li> </ol> <p>It is important to make sure the TV is securely mounted to the wall to prevent it from falling and causing injury or damage.</p> <p></p> <p>Prompt: Happy is to sad as calm is to _.</p> <p>Response: Angry.</p> <p></p> <p>Prompt: Got any creative ideas for a 10 year old's birthday?</p> <p>Response: Sure, how about a DIY spa day at home? The birthday girl can invite her friends over and each girl can create their own spa treatment using items like face masks, bath salts, and nail polish. You could even set up a DIY manicure and pedicure station with nail polish and nail art supplies. The girls can also play relaxing music and enjoy some snacks while they pamper themselves. It's a fun and creative way to celebrate a birthday!</p>"},{"location":"blogs/AmberChat/#references","title":"References:","text":"<p>[1] Z. Liu, et al. \"LLM360: Towards Fully Transparent Open-Source LLMs\", arXiv preprint arXiv:2312.06550, 2023.</p> <p>[2] \"Power-efficient acceleration for large language models -- Qualcomm Cloud AI SDK\", https://developer.qualcomm.com/blog/power-efficient-acceleration-large-language-models-qualcomm-cloud-ai-sdk</p> <p>[3] \"Train anywhere, Infer on Qualcomm Cloud AI 100\", https://developer.qualcomm.com/blog/train-anywhere-infer-qualcomm-cloud-ai-100</p> <p>[4] Open Compute Project (OCP) Microscaling Formats (MX) Specifications. https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf</p> <p>[5] \"Qualcomm Cloud AI 100 Accelerates Large Language Model Inference by ~2x Using Microscaling (Mx) Formats\", https://developer.qualcomm.com/blog/qualcomm-cloud-ai-100-accelerates-large-language-model-inference-2x-using-microscaling-mx</p> <p>Qualcomm branded products are products of Qualcomm Technologies, Inc. and/or its subsidiaries.</p>"},{"location":"blogs/Text_Embeddings/","title":"Extremely Low-Cost Text Embeddings on Qualcomm\u00ae Cloud AI 100 DL2q Instances","text":"<p>Posted By Ravi Sivalingam</p> <p>Text embedding models map text into dense vectors for use in tasks such as semantic similarity search, document retrieval, text classification and clustering. Hence, they are well-suited for use with vector databases for RAG-type applications. The two BGE (BAAI General Embedding) models, BAAI/bge-large-en-v1.5 and BAAI/bge-base-en-v1.5, are recent and popular English-language text embedding models, ranked in the top 10 of the HuggingFace MTEB Leaderboard [1] (as of 30 Jan 2024). A recent article [2] showcased how one can deploy the bge-base-en-v1.5 embedding model on an inf2.xlarge EC2 instance for the cost of $0.006 per 1 million tokens.</p> <p>With an industry-leading performance per Watt, Qualcomm Cloud AI 100 can run the BGE embedding models at an extremely low cost-per-token. Using the recipe listed on Cloud AI SDK Github, we can export, compile, and benchmark bge-base-en-v1.5 and bge-large-en-v1.5 (and a whole line-up of other transformer encoder-type models) with a single command. You can run inference on these compiled models via command line or using our Python or C++ APIs.</p> <p>Qualcomm Cloud AI 100 Standard inference accelerators are now generally available via the DL2q instances on AWS EC2. Each dl2q.24xlarge instance has 8 such accelerators, with a staggering 1.4 PetaFLOPS of FP16 performance, or 2.8 PetaOPS of INT8 performance. Each accelerator has 14 AI cores. For best throughput performance, we can compile each BGE embedding model for 2 cores with a batchsize of 1, and serve 7 replicas per card, or 56 independent replicas per dl2q.24xlarge instance. This simplifies and provides immense flexibility for deployment at scale, since we don't have to deal with batching of requests to achieve the highest performance.</p> <p>The throughput and latency numbers for the two BGE embedding models are shown in the table below, when compiled for the full sequence length of 512 defined by these models, without network overhead. The calculations of price in $/1M tokens is shown for both on-demand and 3-year reserved, all upfront, pricing, based on the AWS EC2 US-West (Oregon) region (as of 30 Jan 2024).</p> Pricing Model Price ($/hr) Embedding Model Sequence Length (tokens) Latency (ms) Throughput (inferences/sec) Total Throughput (tokens/sec) M Tokens per $ $ / 1M tokens $ / 1B toeksn 3 yr, all up front $3.3537 BAAI/bge-base-en-v1.5 512 15.2 457 1,871,872 2,009 $0.000498 $0.498 on-demand $8.9194 BAAI/bge-base-en-v1.5 512 15.2 457 1,871,872 756 $0.001324 $1.324 3 yr, all up front $3.3537 BAAI/bge-large-en-v1.5 512 62.6 111 455,885 489 $0.002043 $2.043 on-demand $8.9194 BAAI/bge-large-en-v1.5 512 62.6 111 455,885 184 $0.005435 $5.435 <p>Even with on-demand pricing, Qualcomm Cloud AI 100 offers the extremely low-cost performance at $0.0013 per 1M tokens for the bge-base-en-v1.5 model, and $0.0054 per 1M tokens for the bge-large-en-v1.5 model. For large-scale deployments, using the 3-year reserved instances, we can get as low as $0.0005 per 1M tokens and $0.002 per 1M tokens, respectively. In other words, for less than 50 cents, you can embed 1 billion tokens worth of text data!</p>"},{"location":"blogs/Text_Embeddings/#conclusion","title":"Conclusion","text":"<p>Embedding text at scale is crucial for many practical applications of Generative AI, and the industry leading performance/Watt of Qualcomm Cloud AI 100 delivers extremely low-cost text embedding with top embedding models such as bge-base-en-v1.5 and bge-large-en-v1.5, offering as low as ~$0.0005 per 1M tokens and ~$0.002 per 1M tokens, respectively.</p>"},{"location":"blogs/Text_Embeddings/#references","title":"References:","text":"<p>[1] HuggingFace Massive Text Embedding Benchmark (MTEB) Leaderboard</p> <p>[2] \\\"Deploy Embedding Models on AWS inferentia2 with Amazon SageMaker\\\"</p>"},{"location":"blogs/Microscaling/microscaling/","title":"Accelerate Large Language Model Inference by ~2x Using Microscaling (Mx) Formats","text":"<p>Posted By Colin Verrilli</p> <p>MxFP, defined by the Microscaling Formats (Mx) Alliance, is enabled on DL2q instance of AWS EC2 and is evaluated on several large language models.</p>"},{"location":"blogs/Microscaling/microscaling/#large-language-model-challenges","title":"Large Language Model Challenges","text":"<p>When performing inference on large language models (LLMs), the generation of tokens (beyond the first) requires DRAM memory bandwidth that far exceeds the required compute. Each subsequent token that is generated requires reading all the model weights from DRAM. In addition, the remembered state (KV cache) of the prompt processing and of all previously generated tokens must be read from DRAM. For this reason, these workloads are severely memory bandwidth constrained in the subsequent token generation phase. The more memory bandwidth available, the faster the generation can occur.</p> <p>In addition to bandwidth, memory capacity can also be a constraining factor in deploying LLMs. The largest of these models have hundreds of billions of parameters which easily exceeds the capacity of inference accelerators. These large models are typically split across several accelerator cards in such a way that each card will hold a portion of the model weights. The storing of KV cache also requires more DRAM capacity. The capacity requirements increase linearly with model size (d_model), context length and batch size.</p> <p>A third performance limiter with LLM inference is communication bandwidth. Since these workloads are split across many cards, intermediate activations must be broadcast between the cards several times during the execution of each decoder block of every token inference. The size of the data to be communicated is proportional to the model size (d_model and number of decoder blocks), the batch size and the sequence length. The communication impact is especially important during prompt processing where the data for many tokens need to be communicated. In this case, the ratio of communication to DRAM access is much higher.</p> <p>The three above performance constraints can all be addressed with data compression techniques. Weight compression reduces the required bandwidth to read the weights. It also can reduce the footprint of the weights in DRAM. Activation compression can reduce the amount of actual data that needs to be communicated over the inter-card links.</p>"},{"location":"blogs/Microscaling/microscaling/#microscaling-formats","title":"Microscaling Formats","text":"<p>Earlier this year, AMD, Arm, Intel, Meta, Microsoft, NVIDIA, and Qualcomm Technologies, Inc. formed the Microscaling Formats (MX) Alliance with the goal of creating and standardizing next-generation 6- and 4-bit data types for AI training and inferencing. The key enabling technology that enables sub 8-bit formats to work, referred to as microscaling, builds on a foundation of years of design space exploration and research. MX enhances the robustness and ease-of-use of existing 8-bit formats such as FP8 and INT8, thus lowering the barrier for broader adoption of single digit bit training and inference.</p> <p>The initial MX specification introduces four concrete floating point and integer-based data formats (MXFP8, MXFP6, MXFP4, and MXINT8) that are compatible with current AI stacks, support implementation flexibility across both hardware and software, and enable fine-grain microscaling at the hardware level. Extensive studies demonstrate that MX formats can be easily deployed for many diverse real-world cases such as large language models, computer vision, and recommender systems. MX technology also enables LLM pre-training at 6- and 4-bit precisions without any modifications to conventional training recipes.</p> <p>One of the primary intentions of MX is to allow vendors to build more power and area efficient hardware for both inference and training accelerators. This is a long-term goal, however, a short-term benefit of this standard and the associated data science is that weights and activations can be \"direct cast\" (compressed / quantized) into these formats post-training to address the issues described above.</p>"},{"location":"blogs/Microscaling/microscaling/#qualcomm-cloud-ai-100-and-mx","title":"Qualcomm Cloud AI 100 and MX","text":"<p>Qualcomm's Cloud AI 100 inference accelerator provides flexible programming that permits adaptation to a wide range of new workloads and new acceleration techniques. One such technique recently implemented is the use of MXFP6 as a weight compression format. When the user selects this compilation option, the compiler will automatically compress the weights from FP32 or FP16 into MXFP6 format during the offline compilation phase. Compressing in this form saves 61% of the size of the weights and thus reduces the pressure on DRAM capacity.</p> <p>At inference run time, the Qualcomm Cloud AI 100 performs on-the-fly decompression in software using its vector engine with an optimized decompression kernel. decompression can be performed in parallel with the weight fetching and computations, so the overhead is mostly hidden. Again, up to 61% of the DRAM bandwidth can be saved. After decompression, the calculations are performed as before in FP16 precision. The use of FP16 is acceptable since the LLMs still remain DRAM constrained so that the compute is not a bottleneck. FP16 also allows to retain the higher precision activations which overcomes loss of accuracy from the quantization.</p> <p>Not yet available, but in-plan is the use of MXINT8 for compression and decompression of activations that need to be communicated over inter-card links. MXINT8 has the property that FP16 can generally be \"direct cast\" to MXINT8 without overall loss in accuracy of the workload (see referenced white paper). In cases where the link bandwidth is constrained, the benefit gained in reduced transfer latencies overcomes the overhead of compressing and decompressing in software. This feature is expected to make a significant improvement in time-to-first-token latencies for large models and for large prompts.</p>"},{"location":"blogs/Microscaling/microscaling/#experimental-results","title":"Experimental Results","text":"<p>Use of MXFP6 for weight compression allows the Qualcomm Cloud AI 100 to have all the benefits of reduced precision (like FP8 or INT8) without the need for special hardware, without the loss of workload accuracy and without the need to modify the training process. It is entirely transparent to the user.</p> <p>Table 1 shows experimental results for various workloads on the Qualcomm Cloud AI 100 Standard card. The throughput improvement when using MXFP6 weights is given. For the 13B model, without MXFP6 the network will not fit, but with MXFP6, the model fits and benefits from the performance improvements.</p> <p>Table 1 - Qualcomm Cloud AI 100 STD Throughput Improvement</p> Model BS PL GL CL FP16 (tok/sec) MXFP6 (tok/sec) MXFP6 % Gain over FP16 gpt-j-6b 1 1024 1024 2048 8.08 13.9 72.03 opt-2.7b 1 1024 1024 2048 15.44 27.72 79.53 opt-13b 1 1024 1024 2048 n/a 6.98 n/a <p></p> <p>Table 2 shows experimental results for various workloads on the Qualcomm Cloud AI 100 Ultra card. The throughput improvement when using MXFP6 weights is given. </p> <p>Table 2 - Qualcomm Cloud AI 100 Ultra Throughput Improvement</p> Model BS PL GL CL FP16 (tok/sec) MXFP6 (tok/sec) MXFP6 % Gain over FP16 opt-6.7b 1 1024 1024 2048 19.49 41.29 111.85 Llama2-7b 1 1024 1024 2048 26.49 43.78 65.27 xgen-7b 1 1024 1024 2048 26.5 43.46 64 gpt-j-6b 1 1024 1024 2048 31.14 50.99 63.74 opt-2.7b 1 1024 1024 2048 64.17 104.25 62.46 <p>The MXFP6 weight compression feature is available in the 1.12 release of the Qualcomm Cloud AI 100 software and runs on a DL2q instance in AWS EC2 today. MxFP delivers up to 79% increase in tokens per second for 2K context length and enables models such as OPT-13B to fit on a single card in the DLq2 instance. Each DL2q instance can run 8 instances of OPT-13B.</p> <p>Visit Qualcomm\u00ae Cloud AI 100 Github to learn more and download SDK.</p> <p></p> <p>Snapdragon and Qualcomm branded products are products of Qualcomm Technologies, Inc. and/or its subsidiaries.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/","title":"Quadruple LLM Decoding Performance with Speculative Decoding (SpD) and Microscaling (MX) Formats","text":"<p>Posted by Natarajan \"Raj\" Vaidhyanathan   Co-written with Apoorva Gokhale</p> <p> Figure 1: Generation on Qualcomm Cloud AI 100 Ultra (left) Baseline with FP16 weights (center) Acceleration with MX6 (right) Acceleration with MX6 and SpD Note: Highlighted text in the right column indicate speculated tokens accepted.</p> <p>Speculative Sampling (SpS), also known as Speculative Decoding (SpD),  and weight compression through  MXFP6 microscaling format, are two advanced techniques that significantly enhance large language model (LLM) decoding speeds for LLM inference AI workloads. Both techniques are available for LLM acceleration on Qualcomm Technologies' data center AI accelerators. To achieve a significant inference performance speedup, start exploring Qualcomm Cloud AI 100 instances available on Amazon AWS EC2 and Cirrascale Cloud Services.</p> <p>This post explores the application of these advanced techniques on two large language models, CodeGen 1-7B and Llama 2-7B-Chat-FT, showcasing the potential for accelerated AI processing and efficiency. Join us as we unravel the details of this advancement and be sure to try out the speculative decoding feature documentation in  Qualcomm Cloud AI Github.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#language-models-lms-and-autoregressive-generation","title":"Language Models (LMs) and Autoregressive Generation","text":"<p>This section introduces the basics of LLM decoding based on traditional autoregressive  decoding and points out its inherent sequential nature of multi-token generation. Text completion is the common task for LMs: Given a prompt of text, the LMs generate plausible completions. LMs process text in the granularity of atomic units of text called tokens. Informally, tokens can be thought of as sub words in a word. The set of tokens that a LM can process is referred to as its vocabulary and denoted as \\(\\Sigma\\). Language Models take a sequence of tokens as input and produce a probability distribution function (pdf) over its vocabulary. Sampling from this pdf gives one plausible token following the input. During implementation, a pdf over \\(\\Sigma\\) is represented as a vector of length \\(|\\Sigma|\\) with elements in \\(\\lbrack 0,1\\rbrack\\) that sum up to one.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#autoregressive-ar-generation","title":"Autoregressive (AR) Generation","text":"<p>Typically, we want to generate multiple following tokens, not just one. Given a prompt of \\(m\\) tokens \\(u_{1},\\ldots,\\ u_{m}\\) generation of \\(n\\) tokens \\(v_{1},\\ldots,\\ v_{n}\\) requires \\(n\\) invocations of the LM (implemented as a decoder-only transformer model) as shown below:</p> <pre><code>input = u1,...,um   # the prompt\n\nfor (i = 1; i &lt;= n; i++):\n\n    // Invoke the LM with the current input\n\n    logits  = LM(input)   # logits is a vector of length |Sigma|\n\n    P = top-k/top-p/softmax warp(logits)    # P is a pdf over vocabulary\n\n    v[i] = sample a token from the vocabulary pdf P\n\n    input = input &amp; v[i]    # Append the generated token to the input\n\nOutput v[1],...,v[n] as a completion\n</code></pre> <p>Figure 2: Outline of Autoregressive (AR) Generation</p> <p>Observe the following about the above completion generation process in Figure 2:</p> <ol> <li> <p>Logits and Warping: The LM produces a vocabulary-sized vector of quantities called logits. Normalizing it via softmax gives a pdf over \\(\\Sigma\\). Sampling from this pdf is generally referred to as multinomial sampling.</p> <ul> <li> <p>Such a pdf can also be warped to redistribute the probability mass over a small subset of \\(\\Sigma\\).</p> </li> <li> <p>A commonly used special form of multinomial sampling is greedy  sampling. This arises from top-1 warping. It will put all the  probability mass on the token with the highest logit value and 0  on others. Sampling from such a distribution will always result  in the token with the highest probability/logit value.</p> </li> </ul> </li> <li> <p>Multiple Invocations: LM is invoked $n $times each time -- once  for each generated token. Each invocation makes use of all the parameters of the model.</p> </li> <li> <p>Autoregressive: The output of the LM is concatenated with the input to the same LM in the following iteration.</p> </li> <li> <p>Causality: It is not possible to generate \\(v_{i}\\) without     generating all the previous tokens making the generation as a serial     process - the generation of \\(v_{i}\\) and \\(v_{j}\\ (i \\neq j)\\)     cannot be parallelized.</p> </li> <li> <p>Repeated calculation: The LM invocations are independent, and     there is no internal state maintained within the LM across     invocations. This results in the same input being processed multiple     times, generating the same set of Key-Value vectors. For example,     the prompt itself is processed \\(n\\) times and the generated token     \\(v_{i}\\) is processed \\(n - i\\) times.</p> </li> </ol>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#accelerating-autoregressive-generation-by-reducing-the-computation-with-kv-cache-kv","title":"Accelerating Autoregressive Generation by reducing the computation with KV Cache (KV$)","text":"<p>The common approach to avoid re-computation is to maintain internal state in the LM referred to as Key-Value Cache (KV$) between invocations. Such a model, LM_with_KV$, functions the same as the LM described above with the following operational differences:</p> <ul> <li> <p>when invoked for the first time on the prompt     \\(u_{1},\\ldots,u_{m}\\), it will create KV$ and write the Key-Value vectors of \\(m\\) prompt tokens to it.</p> </li> <li> <p>when invoked for subsequent token generations:</p> <ul> <li> <p>It takes as input only the single token generated in the     previous iteration instead of the growing input of tokens.</p> </li> <li> <p>It calculates the Key-Query-Value vectors of the single input     token and appends the Key-Values to the KV$.</p> </li> <li> <p>It processes only the single token through all layers of the LM but     calculates the causal attention of the single token with all the     Key-Value vectors in KV$.</p> </li> </ul> </li> </ul> <p>The text completion with LM_with_KV$() can be rewritten as follows:</p> <pre><code>input = u1,...,um    #initialize with the prompt\n\nfor (i = 1; i &lt;= n; i++):\n\n    logits = LM_with_KV$(input)    # Append to KV\\$\n\n    P = top-k/top-p/softmax warp(logits)    # P is a pdf over vocabulary\n\n    v[i] = Sample a token from the vocabulary using the pdf P\n\n    input = v[i]    # Single token that was just generated\n\nOutput v[i],...,v[n] as a completion.\n</code></pre> <p>It is worth emphasizing that LM_with_KV$() with appropriate KV$ contents and LM() are the same function - both calculate the same conditional probability distribution over the vocabulary that can be used to predict the next token following \\(v_{i}.\\)</p> <p>\\(LM\\left( u_{1}..u_{m}, v_{1}\\ldots v_{i} \\right)\\  \\equiv LM\\_ with\\_ KV\\$\\ (v_{i}) \\equiv P\\)( . |\\(u_{1},\\ u_{2},\\ldots,\\ u_{m},\\ v_{1},\\ldots v_{i})\\)</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#performance-profiles-of-lm-completion-generation","title":"Performance Profiles of LM Completion Generation","text":"<p>The execution phase of LM_with_KV$() that takes a prompt as input and generates the first token is called the Prefill Phase. The execution phase that generates the subsequent tokens autoregressively, one token at a time, is called the Decode Phase.</p> <p>These phases have quite different performance profiles. The Prefill Phase requires just one invocation of the LM, requiring the fetch of all the parameters of the model once from the DRAM, and reuses it \\(m\\) times to process all the \\(m\\) tokens in the prompt. With sufficiently large value of \\(m\\), the Prefill Phase is more compute constrained than memory-bandwidth constrained.</p> <p>On the other hand, the Decode Phase is more DRAM-bandwidth constrained than computation constrained, because all the parameters of the model need to be fetched to process just one input token. Further, utilization of computational resources in the accelerator is quite low, as only one token is processed at a time. In other words, the decode phase can afford to do additional computation without impacting the latency.</p> <p>To a first order approximation, the number of tokens that an LM can generate during the decode phase can be calculated as DRAM-Read-Bandwidth (bytes/sec) divided by the capacity of the model in bytes.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#speculative-decoding","title":"Speculative Decoding","text":"<p>It appears that the AR decoding cannot be parallelized because of its causality-induced serial nature: The token \\(v_{i + 1}\\) can only be generated after the token \\(v_{i}\\) has been generated. Let's observe how SpD overcomes this hurdle.</p> <p>SpD leverages the insight that it's faster to check whether a sequence of \\(K ( &gt; 1)\\) tokens is a plausible continuation for a prompt rather than generating such a K-token continuation - as the former just requires one invocation of the LM. In SpD, the plausible completion is generated by another LM called Draft Language Model (DLM) with lower capacity. In this context, the original LM by contrast is referred to as Target LM (TLM).</p> <p>To check whether \\(v_{1},\\ldots,v_{K}\\) is a plausible completion of \\(u_{1}\\ldots u_{m}\\), we need to check the following \\(K\\) conditions:  </p> <p></p> <ul> <li>\\(v_{1},\\ldots ,v_{i}\\) is a plausible completion of \\(u_{1},\\ldots ,u_{m}\\) for \\(i = 1,..,K\\) </li> </ul> <p></p> <p>Due to the parallelism inherent in checking the above K conditions, it can be achieved with one invocation of LM while generating K tokens requires K sequential invocations.</p> <p>For the overall speculative decoding scheme to be faster, the following parameters need to be chosen appropriately:</p> <ul> <li> <p>Relative capacity of DLM compared to the TLM needs be small enough     to generate fast speculation autoregressively and large enough to     generate plausible completion for the TLM.</p> </li> <li> <p>The length of speculation \\(K\\) needs to be small enough to ensure     that both the single invocation of the TLM to check completion and     the time for the DLM to generate do not become too expensive     computationally.</p> </li> </ul> <p>More formally, given a prompt of \\(u_{1}\\ldots u_{m}\\) and a potential completion \\(v_{1},\\ldots ,v_{K}\\), it is possible to efficiently evaluate the following conditional pdfs:</p> <p></p> <ul> <li> <p>\\(P_{LM}(\\ .\\ |u_{1}\\ldots u_{m})\\)</p> </li> <li> <p>\\(P_{LM}(\\ .\\ |u_{1}\\ldots u_{m},\\ v_{1})\\)</p> </li> <li> <p>\\(P_{LM}(\\ .\\ |u_{1}\\ldots u_{m},\\ v_{1},\\ldots v_{i - 1})\\) for \\(i = 2\\ldots K\\)</p> </li> </ul> <p></p> <p>The above denotes one invocation of LM_with_KV$() with input of \\(K\\) tokens \\(v_{1},\\ldots,v_{K}\\) and KV$ holding the Key-Value vectors of \\(u_{1},\\ldots ,u_{m}\\). This invocation is also referred to as scoring the sequence of \\(K\\) tokens \\(v_{1},\\ldots,v_{K}\\).</p> <p>In summary, SpD relies on the fact that for small values of \\(K\\), the latency to score \\(K\\) tokens is no different, or minimally higher, compared to autoregressively generating a single token because the computational units are underutilized during the AR decode stage.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#components-of-speculative-decoding","title":"Components of Speculative Decoding","text":"<p>There are three functional components in SpD to produce \\(n\\) subsequent tokens given a prompt of m tokens \\(u_{1},\\ldots ,u_{m}\\).</p> <ol> <li> <p>TLM: The LM from which we need to generate completion called     Target or True LM (TLM).</p> </li> <li> <p>DLM: A smaller LM used to guess a K-token completion called     Draft LM (DLM), generating those tokens autoregressively one at a     time.</p> <ul> <li> <p>K is a smaller number compared to \\(n.\\) Since the DLM is small, the resulting increase in latency is expected to be minimal.</p> </li> <li> <p>The sequence generated by the DLM is the speculated completion for TLM.</p> </li> </ul> </li> <li> <p>MRS: A probabilistic acceptance scheme to decide whether TLM     should accept the speculated completion or a prefix of it is called     Modified Rejection Sampling (MRS).</p> <ul> <li>This is the heart of the decoding process that guarantees the correctness of Speculative Sampling. Note that the speculated completion is likely to be a plausible completion from the perspective of DLM but may not be so for the TLM. So, the TLM may accept only a prefix of the speculation or even none of it. The acceptance decision is probabilistic in nature. The probabilistic reasoning underpinning the MRS is to ensure that the probability of acceptance of \\(u_{1},.. ,u_{m},t_{1},..,t_{i}\\) indeed matches the probability TLM assigns to it \\(P_{TLM}(u_{1},\\ldots ,u_{m},t_{1}\\ldots ,t_{i})\\).</li> </ul> </li> </ol>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#process-of-speculative-decoding","title":"Process of Speculative Decoding","text":"<p>First, we begin with a simpler case where DLM and TLM use greedy sampling instead of the general multinomial sampling. This is simpler because all the probabilistic reasoning can be reduced to deterministic reasoning as the pdfs become 1-hot pdfs.</p> <p> Figure 3: AR generation of K tokens from DLM</p> <ol> <li> <p>For the given prompt of \\(m\\) tokens, the DLM will autoregressively generate \\(K\\) tokens \\(t_{1},\\ldots ,t_{K}\\), one at a time, with K invocations (as shown in Figure 3 above).</p> </li> <li> <p>The TLM will process sequence \\(u_{1}, u_{2},\\ldots u_{m}, t_{1}\\ldots ,t_{K}\\) with one invocation of TLM and calculate  \\(v_{1},\\ldots ,v_{K+1}\\) (as shown in Figure 4 below). </p> <ul> <li>\\(v_{1}\\) is the next token predicted by TLM on \\(u_{1}\\ldots ,u_{m}\\).</li> <li>\\(v_{i}\\) is the next token predicted by TLM on \\(u_{1},u_{2},\\ldots ,u_{m},t_{1},\\ldots ,t_{i - 1}\\) for \\(1 &lt; i \\leq K + 1\\).</li> </ul> </li> </ol> <p> Figure 4: Single invocation of TLM on speculation</p> <ol> <li> <p>Modified Rejection Sampling (MRS) Algorithm: This component takes the \\(K\\) speculated tokens from DLM \\(t_{1},\\ldots ,t_{K}\\) and the \\(K + 1\\) tokens \\(v_{1},\\ldots ,v_{K + 1}\\) from TLM via the scoring process. The output is a desired plausible completion for the prompt \\(u_{1}\\ldots ,u_{m}\\). It scans the speculated tokens left to right, one token at a time, to see whether it matches the corresponding token sampled by TLM during scoring. If it does, it is accepted as a plausible completion. If it does not, the process stops,  the token that came from TLM is taken as the alternative to the rejected one and the remaining speculation is rejected. If all are accepted, then an additional token is accepted. See Figure 7 for the possible acceptances when \\(K = 3\\).  </p> <pre><code>for (i=1, i &lt;= K; i++) {\n    # Sequentially scan the speculated tokens\n    if t[i] == v[i]:\n        accept(t[i]);    # matches what TLM predicted\n                         # same as accept v[i]\n                         # Idea is to accept what DLM speculated\n    else:\n        accept(v[i]);    # does not match. Accept what TLM predicted!\n        Break;           # exit loop\nif all t[i]'s are accepted then accept v[K+1]\n</code></pre> <p>Figure 5: MRS with Greedy Sampling on DLM and TLM</p> </li> </ol> <p> Figure 6: Potential completions from Speculation</p> <ol> <li> <p>At this point in time, we would have generated at least 1 and at     most \\(K + 1\\) tokens for the TLM with just 1 invocation of TLM and \\(K\\)     invocations of DLM.</p> </li> <li> <p>The above speculate-accept interaction is repeated until the required     number of \\(n\\) tokens are generated.</p> </li> <li> <p>An important implementation detail which we do not elaborate here is     that the KV$ of DLM and TLM should be purged of the tokens that     were rejected.</p> </li> </ol>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#multinomial-sampling","title":"Multinomial Sampling","text":"<p>Qualcomm Cloud AI 100 supports the general case when DLM and TLM do multinomial sampling. In this case, when TLM scores the input sequence and outputs conditional pdfs, the MRS scheme needs to make probabilistic decisions with appropriate probabilities, so that the completions are sampled from the desired target distributions. For further explanation on Multinomial Sampling, please see the Appendix.</p> <p>In summary, SpD involves the use of a smaller capacity model (DLM) running autoregressively to speculate a multi-token completion. Since DLM is small, it can be executed much faster than TLM. The MRS algorithm validates the speculation as a plausible completion from TLM by invoking the TLM just once and accepting a prefix of the speculation. There is a theoretical guarantee that there is no accuracy loss due to the speculate-accept handshake process between the DLM and the TLM.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#weight-only-quantization-using-microscaling-mx-formats","title":"Weight-Only Quantization using Microscaling (Mx) Formats","text":"<p>In addition to Speculative Sampling, Weight-only Quantization using Microscaling (Mx) Formats can also achieve ~2x speedup on LLM decoding.</p> <p>In 2023, AMD, Arm, Intel, Meta, Microsoft, NVIDIA, and Qualcomm formed the Microscaling Formats (MX) Alliance with the goal of creating and standardizing next-generation 6- and 4-bit data types for AI training and inferencing. Qualcomm Cloud AI 100 has implemented MxFP6 (a 6-bit datatype) as a weight compression format. When the user selects this compilation option, the compiler will automatically compress the weights from FP32 or FP16 into MxFP6 format during the offline compilation phase. Compressing in this form saves 61% of the size of the weights and thus reduces the pressure on DRAM capacity.</p> <p>At inference run time, the Qualcomm Cloud AI 100 performs on-the-fly decompression in software using its vector engine with an optimized decompression kernel. Decompression can be performed in parallel with weight fetching and computations, so the overhead is mostly hidden. After decompression, the calculations are performed as before in FP16 precision. The use of FP16 is acceptable since the LLMs still remain DRAM constrained so that the compute is not a bottleneck. FP16 also allows the retention the higher precision activations which overcomes loss of accuracy from the quantization.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#experimental-results","title":"Experimental Results","text":"<p>This section presents the details of the performance speedup achieved on Qualcomm Cloud AI 100 using both greedy SpD and MX6 weight compression with the baseline of no speculative decoding and with parameters in FP16.</p> <p>The speedup is reported on 3 different Qualcomm Cloud AI 100 variants:</p> <ul> <li> <p>Standard (75W TDP),</p> </li> <li> <p>Pro (75W TDP), and</p> </li> <li> <p>Ultra (150W TDP)</p> </li> </ul> <p>For diversity, we report the speedup on two networks CodeGen 1-7B mono and Llama 2-7B-chat-FT, each with different context lengths. CodeGen 1 is a family of models for program synthesis. The mono subfamily is finetuned to produce python programs from specifications in natural language. The model Llama 2-7B-chat-FT is a model fine-tuned by Qualcomm from Llama 2-7B from Meta.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#acceleration-on-qualcomm-cloud-ai-100-ultra","title":"Acceleration on Qualcomm Cloud AI 100 Ultra","text":"<p>Figure 1, from the beginning of the blog, shows three runs of Llama 2 7B-chat-FT for a particular prompt starting at the same time. The left column shows the execution of the model under a baseline condition with FP16 parameters; The middle column shows the model accelerated under MX6 compression; The right column shows the model accelerated with both MX6 compression and SpD.  </p> <p>The significant speedup is visible by the pace in which tokens are produced and completed in the right most column. Worth noting, the text highlighted blue in the MX6 + SpD column is the speculated completion from DLM that was accepted. Figure 7 documents the speedup in the metric of generated tokens per second (t/s) observed in Figure 1.</p> <p> Figure 7: Llama 2-7B acceleration on Cloud AI 100 Ultra</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#speedup-on-qualcomm-cloud-ai-100-standard-and-pro","title":"Speedup on Qualcomm Cloud AI 100 Standard and Pro","text":"<p>A similar range of speedups can be achieved on the Standard and Pro variations of Qualcomm Cloud AI 100, shown in Figure 8. </p> <p> Figure 8: Range of speedups on Qualcomm Cloud AI 100 Standard and Pro</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#configurations","title":"Configurations","text":"<p>This section provides the configurations used to measure this speed up.  </p> CodeGen 1-7B Llama 2-7B Chat Fine Tuned TLM ~7B parameters in MXFP6 ~7B parameters in MXFP6 DLM ~350M parameters in FP16 115M parameters in FP16 Max Prompt Length 32 128 Max Generation Length 224 896 Speculation Length (K) 7 7 Batch Size 1 1 Diverse set of prompts Hello World, BFS, Topological Sort, Tarjan\u2019s SCC, Linear Regression Elicit information about weather, trip guidance, restaurant suggestions, car control, Music Search, Flight Search"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#conclusions-and-next-steps","title":"Conclusions and Next Steps","text":"<p>Large Language Models can be accelerated by Speculative Decoding, increasing performance efficiency without a loss in accuracy. Weight compression using MXFP6 is another technique capable of speeding up LLMs 2x. Synergistically, both features together offer a multiplicative speedup of LLM decoding by a factor of ~4x. Both techniques are available on Qualcomm Cloud AI 100 inference accelerators, which can be utilized in Amazon AWS and Cirrascale Cloud Services.  </p> <p>Be sure to check out the model recipes provided for Speculative Decoding in the Qualcomm Cloud AI GitHub repository to streamline your LLM workflow!</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#what-are-the-requirements-on-the-draft-model","title":"What are the requirements on the draft model?","text":"<p>Typically, the DLM should be a model with the same vocabulary as the TLM and the rule of thumb is for DLM to have 10x fewer parameters. Also preferable is that it is trained to do the same task as the DLM. The DLM could be in different precision than TLM.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#where-do-i-get-the-draft-model","title":"Where do I get the draft model?","text":"<p>Typically, when models are released, they are done in a series of models with increasing capacity. You may choose an appropriate lower-capacity model as a draft model and possibly at a lower precision as well.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#is-there-any-accuracy-impact-because-of-speculated-decoding","title":"Is there any accuracy impact because of speculated decoding?","text":"<p>No. The MRS applied in this solution is proven to accept a speculation from TLM with the same probability with which the speculated text would have been generated by the TLM. So, probabilistically there is no difference.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#what-performance-improvement-will-i-see-in-my-model-by-just-speculative-decoding-technique-alone","title":"What performance improvement will I see in my model by just speculative decoding technique alone?","text":"<p>It depends on many factors. The rule of thumb is that it is typically  possible to achieve 1.5x to 2x. Besides the relative capacity of the draft model and speculation length, the performance improvement can depend on the type of prompt, the nature of completion, and the domain of application. We have seen code generations models - due to their structured output - show relatively more speed up. Common \"easy\" completions that DLM can produce will get accepted at a higher rate by the TLM. The speed up is expected to be robust with respect to increase in context length.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#can-we-use-batch-size-1-to-further-improve-the-speedup-of-spd","title":"Can we use batch size &gt; 1 to further improve the speedup of SpD?","text":"<p>Yes. Synergy is possible.</p> <p>In general, the maximum batch size that could be used depends on the latency target and the amount of storage available for the KV$, which scales linearly with the batch size. When SpD is deployed and latency target is not met, the speculation length \\(K\\) may have to be adjusted as a function of batch size, as both SpD and batching try to exploit the same underutilized computational resources during the decode phase.</p> <p>Note that SpD can improve the Time per output token (TPOT) metric, and hence, the latency to complete a prompt that batching alone cannot do.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#can-there-be-a-live-lock-situation-when-all-the-speculations-from-dlm-are-rejected-by-tlm-and-no-token-from-tlm-is-generated","title":"Can there be a \"live lock\" situation when all the speculations from DLM are rejected by TLM and no token from TLM is generated?","text":"<p>No. In every speculate-verify cycle, the TLM always produces one valid token like it would happen on a normal autoregressive decode.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#will-this-technique-help-with-improving-the-time-to-generate-the-first-token-from-tlm","title":"Will this technique help with improving the time to generate the first token from TLM?","text":"<p>No. This technique only accelerates the decoding phase.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#references","title":"References","text":"<ol> <li> <p>Accelerating Large Language Model Decoding with Speculative     Sampling, Chen et al., 2023</p> </li> <li> <p>Qualcomm Cloud AI 100 Accelerates Large Language Model Inference by ~2x Using Microscaling (Mx) Formats Verrilli, 2023</p> </li> <li> <p>CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis\\     Erik Nijkamp* et al., ICLR, 2023</p> </li> <li> <p>Llama 2: Open Foundation and Fine-Tuned Chat Models,     Meta, 2023</p> </li> <li> <p>Qualcomm Cloud AI 100 Developer documentation: https://quic.github.io/cloud-ai-sdk-pages/latest/</p> </li> <li> <p>Qualcomm Cloud AI 100 GitHub : https://github.com/quic/cloud-ai-sdk</p> </li> <li> <p>Qualcomm Cloud AI 100 on AWS: https://aws.amazon.com/ec2/instance-types/dl2q/</p> </li> <li> <p>Qualcomm Cloud AI 100 on Cirrascale Cloud Services: https://cirrascale.com/solutions-qualcomm-cloud-ai100.php </p> </li> </ol>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#appendix","title":"Appendix","text":""},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#introduction","title":"Introduction","text":"<p>This section gives details of the processing when TLM and DLM do not do greedy (Top-1) sampling. Instead TLM and DLM do general multinomial sampling from the next-token pdfs on the vocabulary.</p> <ol> <li> <p>For the given prompt of \\(m\\) tokens, the DLM will autoregressively     generate \\(K\\) tokens \\(t_{1},\\ldots,t_{K}\\) one at a time with \\(K\\)     invocations calculating the probabilities     \\(P_{DLM}(u_{1},u_{2},\\ldots,u_{m},t_{1},\\ldots,t_{i})\\) for     \\(i = 1\\ldots K\\).</p> <ul> <li>Under greedy sampling these probabilities are 1.</li> </ul> </li> <li> <p>The TLM will score the sequence     \\(u_{1},u_{2},\\ldots,u_{m},t_{1},\\ldots,t_{K}\\) with one invocation of TLM     (Figure 4 )and calculate:</p> <ul> <li> <p>The probabilities     \\(P_{TLM}(u_{1},u_{2},\\ldots,u_{m},t_{1},\\ldots,t_{i})\\) for \\(i = 1\\ldots K\\)</p> </li> <li> <p>The conditional probability distribution:     \\(P_{TLM}(\\ .\\ |\\ u_{1}\\ldots,u_{m},v_{1},\\ldots,v_{K})\\)</p> </li> <li> <p>Under greedy sampling these probabilities are 1 and pdf is 1-hot</p> </li> </ul> </li> <li> <p>Modified Rejection Sampling Algorithm:</p> <ul> <li> <p>This will sequentially check one token at a time whether to accept</p> <ul> <li> <p>\\(t_{1}\\) as completion of \\(u_{1},u_{2},\\ldots,u_{m}\\) </p> </li> <li> <p>\\(t_{i}\\) as completion of \\(u_{1},u_{2},\\ldots,u_{m},t_{1},\\ldots,t_{i - 1}\\) for \\(i = 2\\ldots K\\)</p> </li> </ul> </li> <li> <p>If \\(t_{i}\\) is rejected, then all the subsequent speculated tokens \\(t_{i + 1}\\ldots t_{K}\\) will be rejected as well. In other words, only a prefix of speculation will be accepted. In particular, all of the speculated tokens could be rejected or accepted.</p> </li> <li> <p>In addition, one more completion tokens will be sampled from \\(P_{TLM}(\\ .\\ |\\ u_{1},u_{2},\\ldots,u_{m},t_{1},\\ldots,t_{i-1})\\) if  \\(t_{i}\\) was rejected as an alternative to the rejected token. If all speculated tokens are accepted, then the additional token will be sampled from \\(P_{TLM}(\\ .\\ |u_{1},u_{2},..u_{m},t_{1},\\ldots,t_{K})\\). This additional token is guaranteed to be a valid completion because it is sampled from \\(P_{TLM}\\) itself.</p> </li> <li> <p>At this point in time, we would have generated at least 1 and at most \\(K + 1\\) tokens for the TLM with just 1 invocation of TLM and \\(K\\) invocations of DLM.</p> </li> <li> <p>The above speculate-accept interaction is repeated until required number of \\(n\\) tokens are generated.</p> </li> </ul> </li> </ol>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#modified-rejection-sampling-at-a-single-token-level","title":"Modified Rejection Sampling at a single-token level","text":"<p>As mentioned above, with the speculated completion from DLM and its score from TLM, MRS scans the speculation sequentially one token at a time and decides to accept it or sample an alternative token from \\(P_{TLM}\\). Now we look in detail at the probabilistic decision process by which a single token \\(t\\) following the prompt \\(x\\) for the TLM is probabilistically sampled using a sample from DLM. This is an instance of a general problem of generating a sample from a pdf which is computationally more expensive when a sample from a different easier-to-sample distribution defined on the same support is available. When the two distributions are similar, we will be able to use the given sample itself as a sample from the desired pdf with high likelihood. The algorithm is presented in Figure 9 below. A proof may be found in Accelerating Large Language Model Decoding with Speculative Sampling, Chen, et al., 2023. </p> <p> Input:  </p> <ul> <li> <p>A speculated token sample from \\(P_{DLM}(\\ .\\ |\\ x\\ )\\) : $t $</p> </li> <li> <p>The pdf \\(P_{DLM}(\\ .\\ |\\ x )\\) - Sampling is less expensive from this distribution</p> </li> <li> <p>The pdf \\(P_{TLM}(\\ .\\ |\\ x\\ )\\) - Sampling is harder from this distribution</p> </li> </ul> <p>Output: A token \\(u\\) sampled from \\(P_{TLM}(\\ .\\ |\\ x\\ )\\)</p> <p>Process:</p> <ol> <li> <p>Take token \\(t\\) itself as \\(u\\) with the probability     \\(min\\{ 1,\\ P_{TLM}(\\ t\\ |\\ x\\ )/P_{DLM}(\\ t\\ |\\ x\\ )\\}\\).</p> </li> <li> <p>If \\(t\\) was rejected, sample a different token from the pdf     \\(( P_{TLM}(\\ .\\ |\\ x\\ ) - P_{DLM}(\\ .\\ |\\ x\\ ))_{+}\\), where \\(f(z)_{+}\\) is defined      as \\(\\frac{ \\max(0,\\ f(z)) } {\\Sigma_{y}\\max(0,\\ f(y)) }\\)      is referred to as the residual pdf.  </p> </li> </ol> <p>Figure 9: MRS at a single token level </p> <p>Now we try to give intuition behind the process shown in Figure 9 above. Restating the problem, \\(P_{T}\\) is expensive to sample from while it is easier to sample from \\(P_{D}.\\) We get a sample \\(t\\) from \\(P_{D}\\) whereas we actually want a sample from \\(P_{T}\\). And for economy, we would like to keep \\(t\\) itself as a sample from \\(P_{T}\\) if it is probabilistically acceptable.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#mrs-greedy-p_tlm-p_dlm-are-1-hot","title":"MRS Greedy: \\(P_{TLM},\\ P_{DLM}\\) are 1-hot","text":"<p>Let us begin with the simple case where \\(P_{TLM}\\) and \\(P_{DLM}\\) are the  result of Top-1 warping (greedy sampling), hence they will be 1-hot and sampling from them will always result in same tokens \\(T_{TLM}\\) and \\(T_{DLM}\\) respectively. In this case, intuition says that we should reject \\(T_{DLM}\\) if it is different from \\(T_{TLM}\\) as we are trying to sample from \\(P_{TLM}\\) which always produces \\(T_{TLM}\\).</p> <p>The process given in Figure 9 indeed does the same: \\(T_{DLM}\\) is taken as the sample with probability</p> <p>\\(min\\{ 1,\\ P_{TLM}(\\ T_{DLM}\\ |\\ x\\ )\\ /\\ P_{DLM}(\\ T_{DLM}\\ |\\ x\\ )\\}\\)</p> <p>\\(= \\ min\\{1,\\ P_{TLM}(\\ T_{DLM}\\ |\\ x\\ )\\}\\), where \\(P_{DLM}(\\ T_{DLM}\\ |\\ x\\ ) = 1\\)</p> <p>\\(=\\) 1 or 0 depending on \\(T_{DLM} = \\ T_{TLM}\\)</p> <p>If \\(T_{DLM} \\neq T_{TLM}\\), then the proposal from DLM is rejected. To see, in this case, indeed \\(T_{TLM}\\) gets picked, observe \\((P_{TLM}(\\ .\\ |\\ x\\ ) - P_{DLM}(\\ .\\ |\\ x\\ ) )_{+}\\) has probability 1 on \\(T_{TLM}\\) and 0 elsewhere.</p>"},{"location":"blogs/Speculative_Decode/spec_decode_ai100/#mrs-general-p_tlm-p_dlm-are-general-pdfs","title":"MRS General: \\(P_{TLM}\\ ,\\ P_{DLM}\\) are general pdfs.","text":"<p>Now let us consider the general case where pdfs we are trying to match are not necessarily 1-hot.</p> <p>The intuition behind the process in Figure 9 is best illustrated through a toy example. In this example the vocabulary has 4 tokens \\(\\{ t_{1},t_{2},t_{3},t_{4}\\}\\). Let \\(P_{D}(P_{DLM} )\\) and \\(P_{T}(P_{TLM})\\) be the probability distributions given in the Figure 10.  </p> <p></p> Token \\(P_{D}\\) \\(P_{T}\\) \\(\\max(0, P_{T} - P_{D})\\) \\(\\max(0, P_{T} - P_{D})\\) \\((P_{T} - P_{D})\\) \\(t_{1}\\) 0.4 0.30 0 0.1 0 \\(t_{2}\\) 0.3 0.45 0.15 0 0.15 / (0.15+0.05) = 0.75 \\(t_{3}\\) 0.2 0.10 0 0.1 0 \\(t_{4}\\) 0.1 0.15 0.05 0 0.05 / (0.15+0.05) = 0.25 <p>Figure 10 Toy Example for MRS</p> <p></p> <p>When a token \\(t\\) is sampled from \\(P_{D}\\), the required processing will depend on whether \\(P_{D}(t) \\leq P_{T}(t)\\) or not. The tokens \\(t_{1}\\) and \\(t_{3}\\) have the property \\(P_{D}(t) \\leq P_{T}(t)\\) while the others \\(t_{2}\\) and \\(t_{4}\\) do not. Now we examine the two cases:</p> <p>Case \\(P_{D}(t) &gt; P_{T}(t):\\) Suppose the sample token from \\(P_{D}\\) is \\(t_{1}\\). Processing is analogous if the sampled token was \\(t_{3}\\).</p> <p>Can \\(t_{1}\\) be taken as a sample from \\(P_{T}\\) ? No. Because according to \\(P_{T}\\), the token \\(t_{1}\\) can only be sampled with probability 0.3 while it has been sampled now with probability 0.4. Since we want to keep \\(t_{1}\\) as much as possible, what we do is probabilistically reject \\(t_{1}\\) to ensure it gets effectively accepted only with probability of 0.3:</p> <ul> <li> <p>So, keep \\(t_{1}\\) as the sample from \\(P_{T}\\) with the probability of     \\(P_{T}(t_{1})/P_{D}(t_{1})\\) = 0.3 / 0.4 = 0.75 as per     step (1) of Figure 9 and look for another value as a sample from     \\(P_{T}\\) with the remaining probability of 0.25. Note that now     \\(t_{1}\\) has been effectively selected with probability 0.75*0.4 =     0.3 and that matches the required \\(P_{T}(t_{1})\\).</p> </li> <li> <p>What to do when \\(t_{1}\\) was rejected, which happens with probability     0.25 ? We will pick \\(t_{2}\\) or \\(t_{4}\\) with probability 0.75 and     0.25 respectively, which is equal to sampling from     \\((P_{T} - P_{D})_{+}\\).   </p> <p>The reasoning behind this will become clearer after we examine the case sample from \\(P_{D}\\) is \\(t_{1}\\).</p> </li> </ul> <p>Case \\(P_{D}(t) \\leq P_{T}(t):\\) Now suppose the sample from \\(P_{D}\\) is \\(t_{2}\\). Processing is analogous if the sampled token was \\(t_{4}\\).</p> <p>Can \\(t_{2}\\) be taken as a sample from \\(P_{T}\\) ?  Yes! Because according to \\(P_{T}\\), the token \\(t_{2}\\) needs to be sampled with probability 0.45 while it has been picked with probability 0.3 by \\(P_{D}\\). But if \\(t_{2}\\) is picked only this case of \\(P_{D}\\) picking \\(t_{2}\\) then \\(t_{2}\\) will not get sampled adequately enough. It needs to get sampled additionally when \\(P_{D}\\) samples some other token and it was rejected during the probabilistic decision process.</p> <p>Now we see that,</p> <ul> <li> <p>if \\(t_{2}\\) or \\(t_{4}\\) ( tokens for which \\(P_{D}(t) \\leq P_{T}(t)\\) )     get sampled from \\(P_{D}\\) they will be taken as samples from \\(P_{T}\\)     immediately. But that is not enough. We need to provide additional     opportunities (probability mass) for \\(t_{2}\\) and \\(t_{4}\\) to be     selected.</p> <ul> <li>The additional opportunities are quantified by \\(max(0,\\ P_{T} - P_{D})\\)</li> </ul> </li> <li> <p>These opportunities occur when \\(t_{1}\\) or \\(t_{3}\\) ( tokens for     which \\(P_{D}(t) &gt; P_{T}(t)\\) ) get sampled from \\(P_{D}\\) and are     rejected to be taken as such as samples from \\(P_{T}\\).</p> </li> <li> <p>These are quantified by \\(max(0,\\ P_{D} - P_{T})\\)</p> </li> <li> <p>And clearly the needed and available opportunities match as one is     the negative of the other.</p> </li> <li> <p>So, whenever the proposed token either \\(t_{1}\\) or \\(t_{3}\\) is     rejected, an opportunity arises for one of the remaining tokens     \\(t_{2}\\) or \\(t_{4}\\) to be taken as sample from \\(P_{T}\\). But which     one should we choose? \\(t_{2}\\) or \\(t_{4}\\) ? This choice needs to be     probabilistic and proportional to the required probabilities to be     made up for them. In this example, \\(t_{2}\\) and \\(t_{4}\\) need to     make up probabilities 0.15 and 0.05 respectively. That is, the     tokens \\(t_{2}\\) should be chosen 3 \\(( = \\frac{0.15}{0.05})\\)      times more often than \\(t_{4}\\), which is equivalent to sampling      from the residual pdf \\((P_{T} - P_{D})_{+}\\).</p> </li> </ul> <p>This completes the description of MRS in the general case of multinomial sampling done by DLM and TLM.</p> <p>Qualcomm branded products are products of Qualcomm Technologies, Inc. and/or its subsidiaries.</p>"},{"location":"blogs/Train_Anywhere/train_anywhere/","title":"Train anywhere, Infer on Qualcomm\u00ae Cloud AI 100","text":"<p>Posted By Parmeet Kohli</p> <p>Co-written with Nitin Jain.</p> <p>In this blog post we will go through the journey of taking a model from any framework, trained on any GPU or AI  accelerator and deploying it on DL2q instance that hosts the  highly efficient Qualcomm Cloud AI 100 accelerator.  We will provide the background and how Qualcomm Cloud AI Stack can help deploy a trained model for inference in  three simple steps.</p> <p>Generative AI has caused a paradigm shift in the overall technology landscape and has reached the end user in a  way never seen before. In the AI world, traditionally model training and inference was limited to data and large  organizations who used existing AI HW and SW Tools for research and application development, respectively. With  the explosion in the number of use cases across myriad domains and the ability to get quick and correct results,  many more end users are using Generative AI services. As AI becomes increasingly ubiquitous and use cases expand,  there is a definite need to differentiate the training of large models and their deployment for inference. And  while inference requirements increase, cost effectiveness and ease of use will become necessary for a mass scale  out of AI services.</p>"},{"location":"blogs/Train_Anywhere/train_anywhere/#training-to-inference-workflow","title":"Training to Inference WorkflowPrecisionGraph FormatsPerformance Tuning","text":"<p>Inference is typically stated as an equivalent of forward pass of the training graph, however there are lots of  intricacies involved when taking a trained model for production level deployment. Key factors/differences between  training and inference that impact the accuracy performance tradeoff of the deployed are explained in Figure 1 below.</p> <p></p>        Neural networks are typically trained on 32-bit floating point (FP32) to get good training accuracy.        However, FP32 is slow and power inefficient as weights need more memory  compared to lower precision        formats and proportionately higher DDR bandwidth (BW). Natural Language Processing (NLP) and Large        Language Models (LLM) desired accuracy can be achieved with FP16, 16-bit Integer (INT16), INT8 or        MXFP6. Qualcomm Cloud AI 100 is the first accelerator that supports OCP MicroScaling formats (MXFP) [6]             Training is usually performed in JIT/Eager compilation mode and thus can easily support Control Flow        constructs, Dynamism in tensor shapes and sizes. AoT compiler can produce highly optimized code for        getting the last bit of performance which is essential for deployment. Thus deployment-friendly        inference solutions need modifications of the original training graphs.              Accuracy vs. Performance trade off. Users can pick and choose the performance knobs like        precision format, batch size to suit as per their requirements.      <p>Figure 1: Key Differentiators between Training and Inference</p>"},{"location":"blogs/Train_Anywhere/train_anywhere/#qualcomm-cloud-ai-100-sw-packages-documentation","title":"Qualcomm Cloud AI 100 SW Packages &amp; DocumentationApps SDKPlatform SDK","text":"<p>As shown in Figure 2, Qualcomm Cloud AI 100 accelerator provides a complete Deep Learning SW solution  through two SW packages which enable the user to compile deep learning models and deploy them on the  target Cloud platforms. For LLM, the software packages facilitate Model and Data parallel deployments  which helps the user to optimize the usage of multiple cards attached on single server. A C++ and  python runtime is included for developing end applications, providing a seamless user experience  from Training to Inference. Online resources include documentation as well as a public GitHub  repository with examples of compiling and executing state of art Deep Learning Models.</p>        For development (producer of AI Images)             For development (consumer of AI Images)      <p>Figure 2: Qualcomm Cloud AI 100 Software Packages</p>"},{"location":"blogs/Train_Anywhere/train_anywhere/#deploying-on-qualcomm-cloud-ai-100","title":"Deploying on Qualcomm Cloud AI 100","text":"<p>Qualcomm Cloud AI 100 accelerator is a tailor-made highly sophisticated inference product, and its  software solution has been built for quickly deploying Trained models.</p> <p></p> <p>Figure 3: Three Easy Steps to Deploy Model</p> <p>Figure 4 provides an extensive view of multiple mature tools provided as part of Qualcomm Cloud AI 100  SW packages. Power users can make use of them to enhance their workflows and extract the desired  performance for their workloads on Qualcomm Cloud AI 100.</p> <p></p> <p>Figure 4:  Mature SW Tools for Power Users</p>"},{"location":"blogs/Train_Anywhere/train_anywhere/#export-and-prepare","title":"Export and Prepare","text":"<p>Networks are usually trained in PyTorch &amp; TensorFlow framework and are represented in the form of a  graph, the trained graph may not be optimized for inference and can impact the overall performance  of the model. The first step is to take a trained graph and export into an inference-friendly format  like ONNX. In certain use case, where the user may be required to write custom operators which are  not present as part of the standard operator definitions in a certain framework. The Tool Chain  provides a user friendly interface to write custom operators and make the graph inference friendly.  It is neccesary to prepare the inference graph as a first step since it provides an opportunity to  the user for inspection of the graph and as well as use the source inference graph as a reference.</p>"},{"location":"blogs/Train_Anywhere/train_anywhere/#compile-and-optimize","title":"Compile and Optimize","text":"<p>Once the inference graph is prepared, the next step is to subject it to the Qualcomm Cloud AI 100  compiler. The Qualcomm Cloud AI 100 compiler is a parallelizing solution with the ability to take  a graph and optimally map to the Qualcomm Cloud AI 100 hardware. The compiler flexibly deploys  workloads to each AI core individually or spread across multiple cores and/or multiple accelerators,  whichever is required for optimal performance. This capability gives flexibility for the user to  modulate the deployment for throughput or latency. Memory management, synchronization, tiling, and  scheduling are all handled by the Qualcomm Cloud AI 100 compiler. For futher improvement in throughput,  users also get options to quantize the model or run a hardware in loop tool which provides the  best possible configuration for performance to be used for a specific input graph. Qualcomm Cloud AI 100  Tool Chain includes performance analysis and profiling tools which provide a direct insight into how  the graph is being executed on the hardware. Qualcomm Cloud AI 100 Tool Chain supports AI Model  Efficiency ToolKit (AIMET) supporting model pruning and mixed precision tuning which allows a  graph to be executed much faster with minimal loss in accuracy. There are several examples and  documentation which help the user to migrate their models from training frameworks to an optimized  Inference format.</p>"},{"location":"blogs/Train_Anywhere/train_anywhere/#deploy","title":"Deploy","text":"<p>Once the compiled network satisfies the KPI for the given use case, it can be deployed with the  help of inference servers like Triton and orchestrated through Kubernetes. Virtualization and  Docker support is available as part of the Platform SDK shown in Figure 4. A C++ and Python API  runtime shipped with the Platfrom SDK can be used to create end to end applications for inference  deployment. Several examples are also available in the  online user guide  explainng how models can be deployed for inference on Qualcomm Cloud AI 100.</p> <p>Let\\'s go over one example LLM, GPT-J, for inference deployment on Qualcomm Cloud AI 100.</p>"},{"location":"blogs/Train_Anywhere/train_anywhere/#llms-on-qualcomm-cloud-ai-100","title":"LLMs on Qualcomm Cloud AI 100","text":"<p>Efficient deployment of LLMs on inference accelerators need to address multiple challenges without  sacrificing performance. For example:</p> <ol> <li>Instead of recalculating KV values, maintain a growing cache of KV values (KV$) on the  accelerator between LLM decoding steps</li> <li>Though the above would naturally lead to variable-sized tensors, generate only fixed-shape  tensors to enable ahead-of-time (AOT) AI 100 compiler to generate performant code</li> </ol> <p>To address these challenges, example scripts are included for making the changes in a PyTorch  model, exporting it to ONNX format, compiling and running it on Qualcomm Cloud AI 100.</p> <p>For a more detailed deployment of the GPT-J LLM on Qualcomm Cloud AI 100, please refer to the  documentation available here.</p>"},{"location":"blogs/Train_Anywhere/train_anywhere/#conclusion","title":"Conclusion","text":"<p>As the deep learning space continues to expand at a breakneck pace, its necessary to continuously  evolve the HW, SW and user experience. For any software tool chain to be widely acceptable,  simplicty in use is paramount along with the ability to compile once and deploy on multiple  platforms. The simple training to Inference workflow will not only make the life of a developer  easy but also significantly reduce the time and cost of deploying Large Language Models across  different verticals and simplifying the process of meeting the required KPI. Stay tuned for  future blogs that will contain detailed information on the Qualcomm Cloud AI 100 SW solution.</p>"},{"location":"blogs/Train_Anywhere/train_anywhere/#references","title":"References","text":"<ol> <li> <p>DL2q Instance: https://aws.amazon.com/ec2/instance-types/dl2q/</p> </li> <li> <p>Cloud AI Home: https://www.qualcomm.com/products/technology/processors/cloud-artificial-intelligence</p> </li> <li> <p>User Guide: https://quic.github.io/cloud-ai-sdk-pages/</p> </li> <li> <p>Cloud AI SDK Download: https://www.qualcomm.com/products/technology/processors/cloud-artificial-intelligence/cloud-ai-100#Software</p> </li> <li> <p>Cloud AI 100 API reference: https://quic.github.io/cloud-ai-sdk-pages/latest/API/</p> </li> <li> <p>OCP Microscaling Formats (MX) Specification - https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf</p> </li> </ol> <p></p> <p>Snapdragon and Qualcomm branded products are products of Qualcomm Technologies, Inc. and/or its subsidiaries.  AIMET is a product of Qualcomm Innovation Center, Inc.</p>"},{"location":"images/","title":"Index","text":"<p>Space for images used in documentation</p>"}]}