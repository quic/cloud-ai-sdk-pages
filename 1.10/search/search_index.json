{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cloud AI SDK","text":"<p>Qualcomm Cloud AI SDKs (Platform and Apps) enable high performance deep learning inference on Qualcomm Cloud AI platforms delivering high throughput and low latency across Computer Vision, Object Detection, Natural Language Processing and Generative AI models. The SDKs enable end to end workflows - from onboarding a pre-trained model to deployment of the ML inference application in production. </p> AI 100 Instances on Cloud User Guide  Quick Start Guide  Models, Tutorials and Sample Code Download SDK"},{"location":"API/","title":"API","text":"Python API  C++ API  OnnxRT API"},{"location":"Cpp-API/example/","title":"InferenceSet IO Example","text":"<p>The following document describes <code>AIC100</code> Example named <code>InferenceSetIOBuffersExample.cpp</code>.</p> <p>This example contains a single C++ file and a <code>CMakeLists.txt</code> that can be used for compiling as part of <code>Qualcomm Cloud AI 100</code> distributed Platform SDK.</p> InferenceSetIOBuffersExample.cpp InferenceSetIOBuffersExample.cpp<pre><code>//-----------------------------------------------------------------------------\n// Copyright (c) 2023 Qualcomm Innovation Center, Inc. All rights reserved.\n// SPDX-License-Identifier: BSD-3-Clause-Clear \n//-----------------------------------------------------------------------------\n#include &lt;string&gt;\n#include &lt;vector&gt;\n#include &lt;iostream&gt;\n#include &lt;random&gt;\n#include \"QAicApi.hpp\"\nnamespace {\n/**\n* used to generate radnom data into input buffers\n* Input buffers are uint8_t arrays\n*/\nstruct RandomGen final {\nstatic constexpr const int from = std::numeric_limits&lt;uint8_t&gt;::min();\nstatic constexpr int to = std::numeric_limits&lt;uint8_t&gt;::max();\nstd::random_device randdev;\nstd::mt19937 gen;\nstd::uniform_int_distribution&lt;uint8_t&gt; distr;\nexplicit RandomGen() : gen(randdev()), distr(from, to) {}\n[[nodiscard]] auto next() { return distr(gen); }\n};\n/**\n* Simple helper to return true if the buffer mapping instance is an input one\n* @param bufmap buffer mapping instance\n* @return true if the instance is an input buffer one.\n*/\n[[nodiscard]] bool isInputBuffer(const qaic::rt::BufferMapping &amp;bufmap) {\nreturn bufmap.ioType == BUFFER_IO_TYPE_INPUT;\n}\n/**\n* Helper function to print input/output buffer counts so far, zero based\n* @param bufmap buffer map instance\n* @param inputCount input count to use if the instance is input\n* @param outputCount output count to use if the instance is output\n* @return string formatted using the above info.\n*/\n[[nodiscard]] std::string getPrintName(const qaic::rt::BufferMapping &amp;bufmap,\nconst std::size_t inputCount,\nconst std::size_t outputCount) {\nusing namespace std::string_literals;\nreturn isInputBuffer(bufmap) ? (\"Input \"s + std::to_string(inputCount))\n: (\"Output \"s + std::to_string(outputCount));\n}\n/**\n* Populate input, output vectors with QBuffer information\n* @param bufmap Buffer mapping instance\n* @param buf Actual QBufffer that was generated at callsite/caller.\n* @param inputBuffers Vector to use in case this is input instance\n* @param outputBuffers Vector to use in case this is an output instance\n*/\nvoid populateVector(const qaic::rt::BufferMapping &amp;bufmap, const QBuffer &amp;buf,\nstd::vector&lt;QBuffer&gt; &amp;inputBuffers,\nstd::vector&lt;QBuffer&gt; &amp;outputBuffers) {\nif (isInputBuffer(bufmap)) {\ninputBuffers.push_back(buf);\n} else {\noutputBuffers.push_back(buf);\n}\n}\n/**\n* Given a buffer and size, populate it with random [0..128] random data\n* @param buf buffer to populate\n* @param sz size of this buffer\n*/\nvoid populateBufWithRandom(uint8_t *buf, const std::size_t sz) {\nRandomGen gen;\nfor (auto iter = buf; iter &lt; buf + sz; ++iter) {\n*iter = gen.next();\n}\n}\n/**\n* Prepare buffers, vectors given a single buffer mapping. Depending on the\n* input/output instance of the buffer mapping, handle logic accordingly.\n* Only inputbuffers needs to be populated with random data.\n* @param bufmap buffer mapping passed as const\n* @param inputCount input buffers counter\n* @param outputCount output buffers counter\n* @param inputBuffers input vector of QBuffer to append to new QBuffer\n* @param outputBuffers output vector of QBuffer to append to new QBuffer\n*/\nvoid prepareBuffers(const qaic::rt::BufferMapping &amp;bufmap,\nstd::size_t &amp;inputCount, std::size_t &amp;outputCount,\nstd::vector&lt;QBuffer&gt; &amp;inputBuffers,\nstd::vector&lt;QBuffer&gt; &amp;outputBuffers) {\nstd::cout &lt;&lt; getPrintName(bufmap, inputCount, outputCount) &lt;&lt; '\\n';\nstd::cout &lt;&lt; \"\\tname = \" &lt;&lt; bufmap.bufferName &lt;&lt; '\\n';\nstd::cout &lt;&lt; \"\\tsize = \" &lt;&lt; bufmap.size &lt;&lt; '\\n';\nQBuffer buf{bufmap.size, new uint8_t[bufmap.size]}; // Need to dealloc\npopulateVector(bufmap, buf, inputBuffers, outputBuffers);\n//\n// Provide the input to the inference in \"inputBuffers\". Here random data\n// is used. For providing input as file, use a different api. This example\n// is for input in memory.\n//\nif (isInputBuffer(bufmap)) {\npopulateBufWithRandom(buf.buf, buf.size);\n++inputCount;\n} else {\n++outputCount;\n}\n}\n/**\n* Given input and output buffers, release all heap allocated\n* @param inputBuffers vector of QBuffers - inputs\n* @param outputBuffers vector of Qbuffers - outputs\n*/\nvoid releaseBuffers(std::vector&lt;QBuffer&gt; &amp;inputBuffers,\nstd::vector&lt;QBuffer&gt; &amp;outputBuffers) {\nconst auto release([](const QBuffer &amp;qbuf) { delete[] qbuf.buf; });\nstd::for_each(inputBuffers.begin(), inputBuffers.end(), release);\nstd::for_each(outputBuffers.begin(), outputBuffers.end(), release);\n}\n/**\n* Given buffer mapping instance, return true if this instance does not\n* contain input or output buffers (e.g. it contains uninitialized or invalid)\n* @param bufmap buffer mapping instance\n* @return true if the buffer mapping instance does not container a valid buffer\n*/\n[[nodiscard]] bool notInputOrOutput(const qaic::rt::BufferMapping &amp;bufmap) {\nconst std::initializer_list&lt;QAicBufferIoTypeEnum&gt; bufTypes{\nBUFFER_IO_TYPE_INPUT, BUFFER_IO_TYPE_OUTPUT};\nconst auto func([type = bufmap.ioType](const auto v) { return v == type; });\nreturn std::none_of(bufTypes.begin(), bufTypes.end(), func);\n}\n} // namespace\nint main([[maybe_unused]] int argc, [[maybe_unused]] char *argv[]) {\nQID qid = 0;\nstd::vector&lt;QID&gt; qidList{qid};\n// *** QPC ***\nconstexpr const char *qpcPath =\n\"/opt/qti-aic/test-data/aic100/v2/2nsp/2nsp-conv-hmx\"; // (1)\nauto qpc = qaic::rt::Qpc::Factory(qpcPath);\n// *** CONTEXT ***\nconstexpr QAicContextProperties_t *NullProp = nullptr;\nauto context = qaic::rt::Context::Factory(NullProp, qidList);\n// *** INFERENCE SET ***\nconstexpr uint32_t setSize = 10;\nconstexpr uint32_t numActivations = 1;\nauto inferenceSet = qaic::rt::InferenceSet::Factory(\ncontext, qpc, qidList.at(0), setSize, numActivations);\n// *** SETUP IO BUFFERS ***\nqaic::rt::shInferenceHandle submitHandle;\nauto status = inferenceSet-&gt;getAvailable(submitHandle);\nif (status != QS_SUCCESS) {\nstd::cerr &lt;&lt; \"Error obtaining Inference Handle\\n\";\nreturn -1;\n}\nstd::size_t numInputBuffers = 0;\nstd::size_t numOutputBuffers = 0;\nstd::vector&lt;QBuffer&gt; inputBuffers, outputBuffers;\nconst auto &amp;bufferMappings = qpc-&gt;getBufferMappings();\nfor (const auto &amp;bufmap : bufferMappings) {\nif (notInputOrOutput(bufmap)) {\ncontinue;\n}\nprepareBuffers(bufmap, numInputBuffers, numOutputBuffers, inputBuffers,\noutputBuffers);\n}\nsubmitHandle-&gt;setInputBuffers(inputBuffers);\nsubmitHandle-&gt;setOutputBuffers(outputBuffers);\n// *** SUBMIT ***\nconstexpr uint32_t inferenceId = 0; // also named as request ID\nstatus = inferenceSet-&gt;submit(submitHandle, inferenceId);\nstd::cout &lt;&lt; status &lt;&lt; '\\n';\n// *** COMPLETION ***\nqaic::rt::shInferenceHandle completedHandle;\nstatus = inferenceSet-&gt;getCompletedId(completedHandle, inferenceId);\nstd::cout &lt;&lt; status &lt;&lt; '\\n';\nstatus = inferenceSet-&gt;putCompleted(std::move(completedHandle));\nstd::cout &lt;&lt; status &lt;&lt; '\\n';\n// *** GET OUTPUT ***\n//\n// At this point, the output is available in \"outputBuffers\" and can be\n// consumed.\n//\n// *** Release user allocated buffers ***\nreleaseBuffers(inputBuffers, outputBuffers);\n}\n</code></pre> <ol> <li> Replace the path with model QPC file of interest.</li> </ol> CMakeLists.txt CMakeLists.txt<pre><code># ==============================================================================\n# Copyright (c) 2023 Qualcomm Innovation Center, Inc. All rights reserved.\n# SPDX-License-Identifier: BSD-3-Clause-Clear \n# ==============================================================================\nproject(inference-set-io-buffers-example)\ncmake_minimum_required (VERSION 3.15)\nset(CMAKE_CXX_STANDARD 17)\ninclude_directories(\"/opt/qti-aic/dev/inc\")\nadd_executable(inference-set-io-buffers-example InferenceSetIOBuffersExample.cpp)\nset_target_properties(\ninference-set-io-buffers-example\nPROPERTIES\nLINK_FLAGS \"-Wl,--no-as-needed\"\n)\ntarget_compile_options(inference-set-io-buffers-example PRIVATE\n-fstack-protector-all\n-Werror\n-Wall\n-Wextra\n-Wunused-variable\n-Wunused-parameter\n-Wnon-virtual-dtor\n-Wno-missing-field-initializers)\ntarget_link_libraries(inference-set-io-buffers-example PRIVATE\npthread\ndl)\n</code></pre>"},{"location":"Cpp-API/example/#main-flow","title":"Main Flow","text":"<p>Main function has 8 parts. The example using few helper functions defined in the top anonymous namespace.</p>"},{"location":"Cpp-API/example/#qid","title":"QID","text":"<p>The first part of the <code>main()</code> example will pick <code>QID 0</code>. This is usually the first Enumerated device ID.</p> <p>Though the API is capable of accepting a list of QID's, in this example we only pass a single one in the <code>vector&lt;&gt; int</code> container.</p>"},{"location":"Cpp-API/example/#qpc","title":"QPC","text":"<p>QPC is a container file that includes various parts of the compiled network.</p> <p>The path is hardcoded in this example and could be changed or passed to the program via other means like environment variable or command line arguments.</p> <p><code>qaic::rt::Qpc::Factory</code> API will accept a path to the QPC and returns pack a QPC object to use in the next steps.</p>"},{"location":"Cpp-API/example/#context","title":"Context","text":"<p><code>QAIC</code> Runtime requires a Context object to be created and passed around to various APIs.</p> <p>In this phase we use <code>qaic::rt::Context::Factory</code> API to obtain a new instance of Context.</p> <p>We pass <code>NullProp</code> for no special <code>QAicContextProperties_t</code> attributes and the QID vector that was instantiated before.</p>"},{"location":"Cpp-API/example/#inference-set","title":"Inference Set","text":"<p>Creating an instance of InferenceSet is the next step.</p> <p>InferenceSet is considered as a top-level entity when it comes to running Inferences on Hardware.</p> <p>In this example, we set the Size of the Software/Hardware backlog as 10 possible pending buffers.</p> <p>We have a single activation requested. This means that the provided program (encapsulated in QPC), will be activated as a single instance on the Hardware. We use the single QID provided when creating the InferenceSet instance.</p>"},{"location":"Cpp-API/example/#io-buffers","title":"IO Buffers","text":"<p>Next step is required for setting up Input as well as output buffers.</p> <p>In both Input and Output buffers, the user application will be allocating buffers and also will need to deallocate the buffers before application tear-down.</p> <p>This part have few subsections:</p> <ol> <li>Obtain InferenceHandle to submit the I/O buffers once these created.</li> <li>Allocate the buffers using BufferMappings container. We iterate over each BufferMapping instance to obtain information that helps us to allocate new buffers.</li> <li>Once we have a vectors of allocated Input and Output buffers, we will use the InferenceHandle to submit it. It will be used during inference.</li> </ol> <p>In this example, the helper functions will populate Input buffers with random data just to demonstrate the capabilities of the system.</p>"},{"location":"Cpp-API/example/#submission","title":"Submission","text":"<p>This is the part where the actual submission request is happening.</p> <p>The inferenceSet is used to submit the request, passing submitHandle and user defined inferenceId (which was picked as ID 0)</p>"},{"location":"Cpp-API/example/#completion","title":"Completion","text":"<p>This is a blocking call to wait on inference completion and device output's buffers received in the Application.</p> <p>We use inferenceSet to obtain the completedHandle passing our inferenceId as mentioned above.</p> <p>User is responsible to return the completedHandle back to the Runtime pool and doing so by calling putCompleted using inferenceSet.</p>"},{"location":"Cpp-API/example/#obtaining-output","title":"Obtaining output","text":"<p>In this example we do not do anything with the obtained Output buffers and real-life Application will consume such output data.</p>"},{"location":"Cpp-API/example/#cleanup","title":"Cleanup","text":"<p>Since the buffers are user-allocated buffers using the System's Heap, the users is also in charge of properly releasing these buffers as demonstrated in the last phase of this example.</p>"},{"location":"Cpp-API/example/#helper-functions","title":"Helper Functions","text":"<p>Throughout this example, the following helper functions and constructs are used:</p> <ul> <li><code>RandomGen</code> : Random data generator - to generate random input buffer data.</li> <li><code>isInputBuffer</code> : Query if a specific BufferMapping instance is an input one.</li> <li><code>getPrintName</code> : Return Input or Output strings for standard output printing.</li> <li><code>populateVector</code> : Populate vector - to populate inputs or output vector&lt;&gt; containers.</li> <li><code>populateBufWithRandom</code> : Populate buffer with random data - using the above mentioned Random data generator, given a buffer, populate it with random values.</li> <li><code>prepareBuffers</code> : Prepare buffers - iterates over the BufferMappings container and for each BufferMapping instance, populate input/output vector&lt;&gt; as well as invoke helper function to populate inputs buffers with random data.</li> <li><code>releaseBuffers</code> : Release buffers - iterates over allocated inputs/outputs and release/delete buffers (return memory back to the system's Heap).</li> <li><code>notInputOrOutput</code> : Not input our Output boolean function - Given a BufferMapping instance  return true if this instance is not Input, nor Output buffer instance. (For example could be invalid, or uninitialized). We skip these kind of instances.</li> </ul>"},{"location":"Cpp-API/example/#compile-and-run-commands","title":"Compile and Run Commands","text":"<p>Copy the <code>InferenceSetIOBuffersExample.cpp</code> and <code>CMakeLists.txt</code> in a folder</p> <p>Then compile the example with following commands <pre><code>mkdir build\ncd build\ncmake ..\nmake -j 8\n</code></pre> Finally, run the executable <code>./inference-set-io-buffers-example</code>, accordingly change the <code>qpcPath</code>.</p>"},{"location":"Cpp-API/features/","title":"Features","text":""},{"location":"Cpp-API/features/#profiling-support-in-runtime","title":"Profiling Support in Runtime","text":"<p>When running inferences on AIC100, we might want to get deeper insights into the performance of the AIC100 stack to either triage low performance or for general monitoring.</p> <p>Current AIC100 stack provides mechanism to get performance metrics of key milestones through the inference cycle.</p> <p>Profiling data can be broadly classified into two components:</p> <ol> <li>Host metrics    An inference passes through various software layers on host before it make it to the network on device. Examining the performance on host we can identify if tweaking in network pre/post processing stages or host side multi-threading knobs is required.</li> <li>Network/device metrics    Network metrics provide plethora of information, starting from whole network execution time to detailed operator level performance. This information can be put to use to make the most out of existing network or to optimize network itself.    Note: Network perf collection is not baked into the network by default. It needs to be enabled during network compilation itself. The amount of network perf details present depends on the parameters passed to AIC compiler.</li> </ol>"},{"location":"Cpp-API/features/#profiling-report-types","title":"Profiling Report Types","text":"<p>Using AIC100 software stacks, profiling information can be requested in following types (layouts):</p>"},{"location":"Cpp-API/features/#latency-type","title":"Latency type","text":"<p>Latency type is a CSV style table of key of AIC100 stack. Contains both host and device side information.</p>"},{"location":"Cpp-API/features/#trace-type","title":"Trace type","text":"<p>Trace type is a json formatted Chrome trace data. It can be viewed on any interface that consumes Chrome traces. Contains both host and device side information.</p>"},{"location":"Cpp-API/features/#profiling-report-collection-methods","title":"Profiling Report Collection methods","text":"<p>The above information can be requested from AIC100 stack using two mechanisms. The core content is the same in both mechanisms, just the delivery mechanism changes. The two ways are as follows:</p> <ol> <li>Num-iter based profiling</li> <li>Duration based profiling</li> </ol>"},{"location":"Cpp-API/features/#num-iter-based-profiling","title":"Num-iter based profiling","text":"<p>Alias: Legacy profiling</p> <p>When user creates a profiling handle for num-iter based profiling, they need to specify:</p> <ol> <li>The program to profile,</li> <li>Number of samples to collect,</li> <li>Profiling callback and,</li> <li>Type of profiling output type i.e. latency or trace.</li> </ol> <p>The fundamental idea is that during the creation of profiling handle, the user specifies the number of inferences that needs to be sampled. After profiling is started by the user, the profiling stops and calls user provided callback when:</p> <p>a. Number of samples requested by user has been collected.</p> <p>b. User explicitly stops profiling    In this case, the number of samples collected might be less than that requested during profiling handle creation.</p> <p>After the profiling is stopped, the user can again call start profiling using the same handle. The behavior of the infra will be as if the handle is being triggered for the first time.</p> <p>Refer to section <code>ProfilingHandle</code>_ for HPP API interface.</p> <p>Note in the APIs how at the creation of profiling handle, the user needs to be aware of the program needs to be profiled. Only 1 program can be profiled by a given profiling handle. If user wants to profile multiple programs, multiple profiling handles needs to be created, one for each program.</p>"},{"location":"Cpp-API/features/#duration-based-profiling","title":"Duration based profiling","text":"<p>Alias: Stream profiling</p> <p>When user creates a profiling handle of type duration based profiling or stream profiling, the user needs to specify:</p> <ol> <li>Reporting rate,</li> <li>Sampling rate,</li> <li>Callback,</li> <li>RegEx, and</li> <li>Type of profiling output type i.e. latency or trace.</li> </ol> <p>Notice how there is no condition to specify when profiling should automatically end, hence once the user calls start profiling, the samples are collected till user explicitly calls stop profiling. Also, we do not specify which program we want to profile. We add and remove programs at runtime (even after profiling has started) using appropriate API. Hence, allowing more than one program to be profiled by same profiling handle.</p>"},{"location":"Cpp-API/features/#reporting-rate","title":"Reporting Rate","text":"<p>The profiling callbacks are called at every reporting rate boundary, i.e. suppose the reporting rate set by user is 500ms, and profiling is started at 4seconds and 400ms time-point, the first callback will be called at 4 seconds and 500ms time-point (not the callback did not get called after 500ms but at 500ms boundary). Next callback at 5 seconds time-point and then at 5 seconds 500ms time-point and so on. The callback contains profiling data for inferences that took place between the last report and the current report.</p>"},{"location":"Cpp-API/features/#sampling-rate","title":"Sampling rate","text":"<p>User may not be interested in performance of each and every inference, they may want to just get an over view of the performance and hence can choose to record data for every - second, fourth, eighth or sixteenth inference using the sampling rate knob.</p>"},{"location":"Cpp-API/features/#regex","title":"RegEx","text":"<p>User may want to profiling all the programs running under the process matching a regular expression say \"Resnet50*\". If the user creates a profiling handle with a specific regular expression, any new program created, whose name passes the regEx filter, will automatically start getting profiled. Once the program is released by the user, it automatically is also removed from the profiling handle's list of programs.</p> <p>Note: Addition/removal of program can lead to a report generation getting delayed or preponed. Note: RegEx engine used is ECMAScript.</p> <p>Refer to section <code>StreamProfilingHandle</code>_ for HPP API interface.</p>"},{"location":"Cpp-API/features/#device-partitioning-tool","title":"Device Partitioning Tool","text":"<p>Device partitioning is a virtualization technique for Qualcomm AIC 100 card that creates a Shadow device with discrete set of resources on the native device. The Shadow Device is tied to a \"Resource Group Reservation\" made by the application and is presented with Device id(QID) greater than 100 [Eg: QID 100]. The Shadow Device is allowed to use only the pre-reserved set of resources tied to its reservation and is non-persistent in nature.</p> <p>Either of the below two scenarios will destroy the device and release the resources associated with it.   a. The device would be destroyed when the Resource Group Reservation is      released by application either explicitly or when the application dies.   b. A hard reset is performed on the device.</p> <p>Below listed are the set of resources that are available for reservation under \"Resource Group\"   - Neural Signal Processors(NSP)   - Multicast IDs (MCID)   - Virtual Channels for DMA data   - Device Semaphores   - Device Memory (DDR, L2)</p> <p>qaic-dev-partition, is an application tool to create the Shadow Device(s) with the specified set of device resources.</p> <p>Since the device allocation is non-persistent in nature, below use cases are UNSUPPORTED as they may invalidate the QID allocated by an already existing instance of application. 1. Multiple instances of qaic-dev-partition tool. 2. Hotplug a QAic device when there is a active qaic-dev-partition.</p>"},{"location":"Cpp-API/features/#usage","title":"Usage","text":"<pre><code>/opt/qti-aic/tools/qaic-dev-partition -h\n</code></pre>"},{"location":"Cpp-API/features/#create-and-test-the-shadow-devices","title":"Create and Test the Shadow Device(s)","text":""},{"location":"Cpp-API/features/#1-create-the-shadow-device","title":"1. Create the Shadow device","text":"<p>Launch the qaic-dev-partition utility as a daemon, either in foreground or background to keep    the Shadow device alive.</p> <p>To request resource reservation using the QPC file on Device-id 2</p> <pre><code>  sudo qaic-dev-partition -q &lt;path to qpc&gt;/programqpc.bin -d 2\n</code></pre> <p>To request resource reservation using json config file:</p> <pre><code>  sudo qaic-dev-partition -p &lt;path_to_json_config&gt;\n</code></pre> <p>Multiple reservations can be created through json configuration. The first resource set is allotted QID 100,    and QID is incremented by 1 for every resource set listed thereafter.</p>"},{"location":"Cpp-API/features/#2-verify-the-derived-device-creation","title":"2. Verify the Derived device creation:","text":"<p>Run qaic-util tool to list the newly generated device. Typically the    Shadow devices have QID &gt;= 100. Note that PCI fields are invalid with dummy values as it is an    emulated device, and PCI Hardware attributes are not applicable.    Example: <code>sudo qaic-util -q -d 101</code></p>"},{"location":"Cpp-API/features/#3-destroy-the-device","title":"3. Destroy the device.","text":"<p>On killing the utility the Shadow device should disappear and one may verify that the resources are added    back to the native device's resource pool.</p> <p>Example configuration files are present in <code>/opt/qti-aic/test-data/json/qaic-dev-partition</code></p>"},{"location":"Cpp-API/features/#partition-device-configuration","title":"Partition Device Configuration","text":"<p>Alias names: Partition Device/ Shadow Device/Derived Device.</p> <p>Documentation uses the term \"Derived Device\". The starting Device ID for Derived Device(s) is QID 100. Multiple Shadow/Derived devices can be created on the same QAIC device. Each shadow device would have unique QID and Dev link assigned as shown below.</p> <p>Example:</p> <pre><code>Parent device:  QID 17\n                Dev Link: /dev/qaic_aic100_15\n\nShadow devices for QID 17\n QID 112 :\n           Dev Link : /dev/qaic_aic100_31\n     Parent Dev Link: /dev/qaic_aic100_15\n\n QID 113 :\n           Dev Link : /dev/qaic_aic100_33\n     Parent Dev Link: /dev/qaic_aic100_15\n</code></pre>"},{"location":"Cpp-API/runtime/","title":"Runtime","text":"<p>The following document describes Qualcomm AIC 100 User space Linux Runtime classes design and implementation.</p>"},{"location":"Cpp-API/runtime/#qpc-elements","title":"QPC Elements","text":""},{"location":"Cpp-API/runtime/#qpc","title":"QPC","text":"<p>Class <code>Qpc</code> is a main QPC API class type that provides functionality to allocate a QPC from a filename or buffer, and also provides API to query information related to the loaded <code>QPC</code>.</p> <p>Class <code>Qpc</code> has 2 Factory functions to create a <code>std::shared_ptr&lt;&gt;</code> of <code>Qpc</code>. There are couple of ways to create <code>QPC</code> object.</p> <p>The class is non-copyable, nor movable.</p> <ol> <li>Creating from buffer and size.</li> <li>Creating from a given filename (base-path plus filename).</li> </ol> <p>If <code>Factory</code> instance creation is successful, the functions will return an instance of <code>std::shared_ptr&lt;&gt;</code>, otherwise a proper exception will be thrown.</p> <p>Important API in class type <code>Qpc</code> includes the following:</p> <ol> <li><code>getInfo()</code> - returns <code>QpcInfo</code>. See below for more info.</li> <li><code>getBufferMappings()</code> - returns a container of <code>BufferMappings</code>. Each <code>BufferMapping</code> instance in the container includes information on the buffers as obtained from <code>Qpc</code> file. Name, size, direction and index of buffers.</li> <li><code>getBufferMappingsDma</code> - Same return type as above (<code>getBufferMapping()</code>) but for DMA allocated buffers instead of user's heap allocated ones.</li> <li><code>getIoDescriptor()</code> - returns pointer to QData which is a buffer and length of the QPC buffer.</li> <li><code>get()</code> - returns a const pointer to QAicQpcObj - which is an opaque Data structure to Internal Program Container. Users have no visibility to the API of QAicQpcObj since it is Opaque in the C layer.</li> <li><code>buffer()</code> - returns the QPC buffer as a const pointer to uint8_t.</li> <li><code>size()</code> - returns the size of the QPC buffer.</li> </ol>"},{"location":"Cpp-API/runtime/#qpcfile","title":"QpcFile","text":"<p>Class <code>QpcFile</code> is encapsulating the QPC file basepath and filename as well as <code>DataBuffer&lt;QData&gt;</code> data member which holds the buffer of the QPC file. <code>QpcFile</code> takes the base-path and the filename of the QPC, where <code>programqpc.bin</code> is a default filename provided in the constructor. <code>QpcFile</code> is a non-movable, non-copyable type.</p> <p>It has a <code>load()</code> function that will load the content of the QPC into  <code>DataBuffer&lt;&gt;</code> which holds <code>QData</code> buffer representation internally.</p> <p>There are few APIs provided by the <code>QpcFile</code> class type.</p> <ol> <li><code>getBuffer()</code> - returns a <code>QData</code> buffer const reference.</li> <li><code>data()</code> - returns a <code>const uint8_t</code> pointer to buffer.</li> <li><code>size()</code> - returns the size of the loaded QPC file buffer.</li> </ol>"},{"location":"Cpp-API/runtime/#qpcinfo","title":"QpcInfo","text":"<p>Struct <code>QpcInfo</code> is a simple struct type that aggregates a collection of <code>QpcProgramInfo</code> (also referred to as \"program\") and corresponding collection of <code>QpcConstantsInfo</code> (also referred to as \"constants\").</p>"},{"location":"Cpp-API/runtime/#qpcprograminfo","title":"QpcProgramInfo","text":"<p>Struct <code>QpcProgramInfo</code> is a simple struct aggregating information related to the content of the loaded QPC.</p> <p>For example:</p> <ol> <li>BufferMappings, user allocated or DMA.</li> <li>Name identifying a segment in <code>QPC</code> file.</li> <li>Number of cores requires to run the program.</li> <li>Program Index in the QPC.</li> <li>Size of the program</li> <li>Batch size.</li> <li>Number of Semaphores, number of MC(MultiCast) IDs.</li> <li>Total required memory to run the program.</li> </ol>"},{"location":"Cpp-API/runtime/#qpcconstantsinfo","title":"QpcConstantsInfo","text":"<p>Struct <code>QpcConstantsInfo</code> defines the Constants info that are obtained from the QPC file. It has the following attributes:</p> <ol> <li><code>name</code></li> <li><code>index</code></li> <li><code>size</code></li> </ol>"},{"location":"Cpp-API/runtime/#buffermappings","title":"BufferMappings","text":"<p>Vector <code>BufferMappings</code> is a vector of <code>BufferMapping</code>.</p> <p><code>BufferMappings</code> is created from QPC.</p> <p><code>BufferMappings</code> is used by API to store the Input/Output buffer information that is also used to create <code>inferenceVector</code>.</p>"},{"location":"Cpp-API/runtime/#buffermapping","title":"BufferMapping","text":"<p>Struct <code>BufferMapping</code> is a simple struct type that describes the information of a buffer.</p> <p>Struct <code>BufferMapping</code> has two constructors:</p> <ol> <li>Creating by providing all of the data members.</li> <li>Default constructor that creates an uninitialized <code>BufferMapping</code> instance.</li> </ol> <p>Struct <code>BufferMapping</code> has following structure data members:</p> <ol> <li><code>bufferName</code> - string name identifying the buffer.</li> <li><code>index</code> - an unsigned int that represent the index in an array of buffers.</li> <li><code>ioType</code> - define the direction of a buffer from user's perspective. An input buffer is from user to device. An output buffer is from device to user.</li> <li><code>size</code> - buffer size in bytes.</li> <li><code>isPartialBufferAllowed</code> - <code>Partial buffer</code> is a feature that allows buffer to have actual size that is smaller than what is specified in IO descriptor. <code>isPartialBufferAllowed</code> is set by IO descriptor. By setting <code>isPartialBufferAllowed</code> true, this buffer takes user buffer that is smaller than what is specified by <code>size</code>.</li> <li><code>dataType</code> - define the format of buffer. The types of format is defined in struct <code>QAicBufferDataTypeEnum</code></li> </ol>"},{"location":"Cpp-API/runtime/#qaicbufferdatatypeenum","title":"QAicBufferDataTypeEnum","text":"<p>Struct <code>QAicBufferDataTypeEnum</code> is a simple struct type that defines the data type of the <code>BufferMapping</code>.</p> <p>Struct <code>QAicBufferDataTypeEnum</code> defines following data types:</p> <ol> <li><code>BUFFER_DATA_TYPE_FLOAT</code> - 32-bit float type (float)</li> <li><code>BUFFER_DATA_TYPE_FLOAT16</code> - 16-bit float type (half, fp16)</li> <li><code>BUFFER_DATA_TYPE_INT8Q</code> - 8-bit quantized type (int8_t)</li> <li><code>BUFFER_DATA_TYPE_UINT8Q</code> - unsigned 8-bit quantized type (uint8_t)</li> <li><code>BUFFER_DATA_TYPE_INT16Q</code> - 16-bit quantized type (int16_t)</li> <li><code>BUFFER_DATA_TYPE_INT32Q</code> - 32-bit quantized type (int32_t)</li> <li><code>BUFFER_DATA_TYPE_INT32I</code> - 32-bit index type (int32_t)</li> <li><code>BUFFER_DATA_TYPE_INT64I</code> - 64-bit index type (int64_t)</li> <li><code>BUFFER_DATA_TYPE_INT8</code> - 8-bit type (int8_t)</li> <li><code>BUFFER_DATA_TYPE_INVAL</code> - invalid type</li> </ol>"},{"location":"Cpp-API/runtime/#context-elements","title":"Context Elements","text":""},{"location":"Cpp-API/runtime/#context","title":"Context","text":"<p>There are various Linux Runtime core components like <code>qpc</code>, <code>program</code>, <code>execObj</code>, and <code>queue</code> etc. which are needed to run inference and enhance performance/usability. Class <code>Context</code> is a primary class which helps to link all LRT core components. <code>Context</code> object should be created first. Application creates a context to obtain access to other API functions, the context is passed in other API calls. The caller can also register for logging and error callbacks. A context ID is passed to the error handler to uniquely identify the <code>Context</code> object.</p> <p>Class <code>Context</code> has a Factory functions to create a <code>std::shared_ptr&lt;&gt;</code> of <code>Context</code>.</p> <p>Context object is created from context properties, list of devices used by this context, logging callback function, specific user data to be included in log callback, an error handler to call in case of critical errors and specific user data to be included in error handler callback. If logging callback and error handler are not provided then default <code>defaultLogger</code> and <code>defaultErrorHandler</code> will be used.</p> <p>If <code>Factory</code> instance creation is successful, the functions will return an instance of <code>std::shared_ptr&lt;&gt;</code>, otherwise a proper exception will be thrown.</p> <p>Important API in class type <code>Context</code> includes the following:</p> <ol> <li><code>findDevice()</code> - returns a suitable device for the network and check selected device has enough resources.</li> <li><code>setLogLevel()</code> - set new logging level to get logging information while running the program. See below for more details about <code>QLogLevel</code>.</li> <li><code>getLogLevel()</code> - returns current logging level for given <code>Context</code>.</li> <li><code>get()</code> -  returns a const pointer to <code>QAicContext</code>. Users have no visibility to the API of <code>QAicContext</code> since it is Opaque in the C layer.</li> <li><code>getId()</code> - returns an unsigned int that represent id of the <code>Context</code>. This id will be returned in error reports to refer a specific created context.</li> <li><code>objName()</code> - returns <code>const std::string</code> which is name of the object. For context object name is <code>Context</code>.</li> <li><code>objNameCstr()</code>- returns a pointer to an array that contains a null-terminated sequence of char representing the name of an object.</li> </ol>"},{"location":"Cpp-API/runtime/#qloglevel","title":"QLogLevel","text":"<p>There are diffrent type of logging level to see different kind of logs.</p> <ol> <li><code>QL_DEBUG</code> : set to this level to see debug logs</li> <li><code>QL_INFO</code> : set to this level to see informative logs</li> <li><code>QL_WARN</code> : set to this level to see warning logs</li> <li><code>QL_ERROR</code> : set to this level to see error logs</li> </ol> <p><code>LogCallback</code> - It is a logging callback lambda function.</p> <p><code>ErrorHandler</code> - It is an error handler lambda function to call in case of critical errors.</p>"},{"location":"Cpp-API/runtime/#profiling-elements","title":"Profiling Elements","text":"<p>For overview of profiling feature refer to Profiling Support in Runtime.</p>"},{"location":"Cpp-API/runtime/#profilinghandle","title":"ProfilingHandle","text":"<p><code>ProfilingHandle</code> provides interface to use num-iter based profiling. Refer to Num-iter based profiling for more details on num-iter based profiling feature.</p> <p>A <code>ProfilingHandle</code> object should be created using the <code>Factory</code> method. User needs to specify the <code>Program</code> that should be profiled, number of samples to collect, callback to call to deliver report, and type of profiling output expected.</p> Note <p>Profiling type parameter has a default value set to Latency type.</p> <p>Important API in class type <code>ProfilingHandle</code> includes the following:</p> <ol> <li><code>start()</code> - Start profiling. After the API call, profiling data from all the inferences for specified <code>Program</code> will be collected till either user calls <code>stop()</code> or number of requested samples have been collected.</li> <li><code>stop()</code> Stop profiling. Stops profiling even if the num-samples requirement has not been met. This API calls triggers a callback to the user specified callback with profiling report of all collected samples.</li> </ol> Note <p>If <code>stop()</code> is called without any inferences being complete for the specified <code>Program</code>, callback will not get triggered.</p>"},{"location":"Cpp-API/runtime/#streamprofilinghandle","title":"StreamProfilingHandle","text":"<p>ProfilingHandle provides interface to use duration based profiling. Refer to Duration based profiling for more details on duration based profiling feature.</p> <p>A <code>StreamProfilingHandle</code> object should be created using the <code>Factory</code> method. User needs to specify the sampling rate, reporting rate, callback to call to deliver report and profiling output format expected. User may optionally specify a name for the handle, and regEx to auto add/remove programs.</p> Note <p>Profiling type is specified using ProfilingProperties field. ProfilingProperties has a default param nullptr which results in profiling type to be Latency type.</p> <p>Important API in class type <code>StreamProfilingHandle</code> includes the following:</p> <ol> <li> <p><code>start()</code> - Start profiling. After the API call, user should expect a callback at every reporting rate boundary containing information of profiling inferences during that duration.</p> Note <p>User will get callback even if there are no samples collected.</p> </li> <li> <p><code>stop()</code> Stop profiling. A final report will be delivered to user when the profiling is stopped with the profiling data of samples collected from last report till the point <code>stop()</code> is called.</p> </li> <li> <p><code>addProgram()</code> - Add a program to list of program being profiled.</p> Note <p>Adding program when profiling is active can cause spurious report callback or a delayed report callback.</p> </li> <li> <p><code>removeProgram()</code> - Remove a program from list of program being profiled.</p> Note <p>Removing program when profiling is active can cause spurious report callback or a delayed report callback.</p> </li> <li> <p><code>flushReports()</code> - After profiling is stopped using <code>stop()</code> API, user should make sure that all the reports on the queue of the profiling infrastructure are delivered to user as callback before application exit. <code>flushReports()</code> API only returns after there are no more reports left to be delivered to the user, thus ensuring a clean application exit.</p> </li> </ol>"},{"location":"Cpp-API/runtime/#inferencing-elements","title":"Inferencing Elements","text":""},{"location":"Cpp-API/runtime/#qbuffer","title":"QBuffer","text":"<p><code>QBuffer</code> is a struct that contains pointer to the buffer and its size. It can have Input or output buffer address from heap or DMA memory. <code>handle</code>, <code>offset</code> and <code>type</code> are considered only when type is QBUFFER_TYPE_DMABUF or QBUFFER_TYPE_PMEM. It has following Members:</p> <ol> <li><code>size</code> - Total size of memory pointed by buf pointer or handle.</li> <li><code>buf</code> - Buffer Pointer, must be valid in case of heap buffer.</li> <li><code>handle</code> - Buffer Handle, must be valid in case of DMA (or PMEM) buffer.</li> <li><code>offset</code> - Offset within handle.</li> <li><code>type</code> - Type of the buffer heap, DMA or PMEM.</li> </ol>"},{"location":"Cpp-API/runtime/#inferencevector","title":"InferenceVector","text":"<p><code>InferenceVector</code> contains a vector of <code>QBuffer</code>. Vector of <code>QBuffer</code> is a vector containing both input and output buffers. User can create <code>InferenceVector</code> from multiple sources like files from disk or create <code>QBuffer</code> with data available with the user and set them in <code>InferenceVector</code> with <code>setBuffers()</code> API of this class. The input buffer will be used for inference and result of inference will be stored in output buffer. User needs to keep reference of InferenceVector until inference is complete and user can read output buffers from inference vector after completion of inference.</p> <p><code>InferenceVector</code> APIs</p> <ul> <li><code>getVector()</code>: Returns a vector of <code>QBuffer</code>. Vector contains both input and output buffers. <code>QBuffer</code> is a struct that contains pointer to the buffer and its size.                 User can call this after the inference is completed to read output buffers.</li> <li><code>setBuffers()</code>: Sets input and output buffers of this <code>InferenceVector</code></li> <li><code>Factory()</code>: Instantiates InferenceVector</li> </ul>"},{"location":"Cpp-API/runtime/#inferencehandle","title":"InferenceHandle","text":"<p><code>InferenceHandle</code> contains <code>InferenceVector</code> and id given at the time of submission of inference. <code>InferenceHandle</code> cannot be created directly by the user, user can get an available <code>InferenceHandle</code> by calling <code>getAvailable()</code> API of <code>InferenceSet</code>. <code>InferenceHandle</code> is a container that has data needed for inference stored in <code>InferenceVector</code>. Number of <code>InferenceHandle</code> objects created depends on the set_size and num_activations parameters passed during instantiation of <code>InferenceSet</code>. Number of <code>InferenceHandle</code> and number of <code>ExecObj</code> created will be the same.</p>"},{"location":"Cpp-API/runtime/#lifecycle-of-inferencehandle","title":"LifeCycle of <code>InferenceHandle</code>","text":"<ul> <li><code>InferenceHandle</code> objects are created when <code>InferenceSet</code> is instantiated and all objects are moved to availableList vector from which user can retrieve it by calling <code>getAvailable()</code> API of <code>InferenceSet</code></li> <li>When user calls <code>getAvailable()</code> if availableList vector has an <code>InferenceHandle</code>, it is popped out from the availableList and returned to user, otherwise this call is blocked until the user puts the used <code>InferenceHandle</code> using <code>putCompleted()</code> API</li> <li>User sets buffers in the InferenceHandle it got using <code>setBuffers()</code> API</li> <li>User submits InferenceHandle using <code>submit()</code> API of <code>InferenceSet</code></li> <li>To get the completed <code>InferenceHandle</code> user can call <code>getCompleted()</code> or <code>getCompletedId()</code> and extract/read the output of inference from <code>InferenceHandle</code></li> <li>After processing the output of inference, user needs to call <code>putCompleted()</code> API of <code>InferenceSet</code> to put completed <code>InferenceHandle</code> back to availableList vector otherwise <code>getAvailable()</code> call will be blocked</li> </ul>"},{"location":"Cpp-API/runtime/#inferenceset","title":"InferenceSet","text":"<p><code>InferenceSet</code> is a C++ class that is used to submit inference. It abstracts out lower level classes like Queue, Program and ExecObj and provides an easier way of handling multiple activations in a single group to submit inference.</p> <p>List of APIs of <code>InferenceSet</code></p> <ul> <li><code>submit()</code>: Submit an inference through <code>InferenceVector</code>. The submission will be blocked until an <code>InferenceHandle</code> is available</li> <li><code>submit()</code>: Submit an inference through <code>InferenceHandle</code></li> <li><code>getCompleted()</code>: Returns a completed <code>InferenceHandle</code> object. User can access output of the inference using this <code>InferenceHandle</code> object by calling <code>getBuffers()</code> method</li> <li><code>getCompletedId()</code>: Returns a completed <code>InferenceHandle</code> object with specified ID</li> <li><code>putCompleted()</code>: Move a completed InferenceHandle back into the availableList vector</li> <li><code>getAvailable()</code>: Returns an available <code>InferenceHandle</code> object from availableList</li> <li><code>waitForCompletion()</code>: Wait for all inferences submitted to be completed on all activations</li> <li><code>Factory()</code>: Instantiates InferenceSet</li> </ul>"},{"location":"Cpp-API/runtime/#numactivations-and-setsize","title":"NumActivations and SetSize","text":"<p><code>NumActivations</code> and <code>SetSize</code> are arguments of <code>InferenceSet::Factory</code> API.</p> <ul> <li><code>NumActivations</code>: <code>InferenceSet</code> creates this many numbers of network instances inside device. User can decide <code>NumActivations</code> based on number of cores required to run his network and number of available cores.</li> <li><code>SetSize</code>: For each network instance, user application can simultaneously enqueue this many numbers of input/output buffers to run inferences. Recommended value is between 2 to 10. User should find an optimal value to achieve desired throughput (inferences/sec) and latency.</li> </ul> <p> </p> Activations and SetSize"},{"location":"Cpp-API/runtime/#inference-flow","title":"Inference Flow","text":"<p>Inference flow using <code>InferenceSet</code> would be as follows:</p> <ol> <li>Instantiate <code>InferenceSet</code> using the Factory method</li> <li>Acquire one of the available <code>InferenceHandle</code>.</li> <li>Set Input and Output buffers in that <code>InferenceHandle</code>.</li> <li>Submit <code>InferenceHandle</code> to <code>InferenceSet</code> to run inference in device.</li> <li>Call <code>getCompletedId</code> API to wait for Inference to complete. Inference results are available in Output buffers, once this API returns.</li> <li>Once application reads output data, <code>putCompleted</code> must be called to return the <code>InferenceHandle</code> back to available List of handles.</li> </ol> <p> </p> Inference Flow"},{"location":"Cpp-API/runtime/#inferencesetproperties","title":"InferenceSetProperties","text":"<p><code>InferenceSetProperties</code> defines properties to be consumed by <code>InferenceSet</code></p> <p>List of members of <code>InferenceSetProperties</code></p> <ul> <li><code>programProperties</code>: User can set different program properties which will be consumed internally by <code>Program</code> object. Notable programProperties are:<ul> <li><code>SubmitRetryTimeoutMs</code>: After submission of inference, runtime waits for this milliseconds timeout period, if inference is not complete in this timeout period, error is returned</li> <li><code>SubmitNumRetries</code>: Number of times submission should be retried when the above timeout occurs</li> <li><code>devMapping</code>: devMapping specifies the physical devices to be used by the program and is valid only for the network's that need multiple devices to run</li> </ul> </li> <li><code>queueProperties</code>: User can set queue properties which will be consumed internally by <code>Queue</code> object. Notable queueProperties are:<ul> <li><code>numThreadsPerQueue</code>: Number of threads spawned to process elements in the queue. Default 4</li> </ul> </li> <li><code>name</code>: Defines name of the InferenceSet Object</li> <li><code>id</code>: Defines id of the InferenceSet Object</li> </ul>"},{"location":"FAQ/","title":"Frequently Asked Questions","text":""},{"location":"FAQ/#general","title":"General","text":"What is Cloud AI 100 Accelerator? <p>Cloud AI 100 accelerators enable high performance inference on deep learning models. The accelerators are available in multiple form factors and associated SKUs. Cloud AI SDKs enable end to end workflows - from onboarding a pre-trained model to deployment of the ML inference application in production. </p>"},{"location":"FAQ/#cloud-ai-sdk-installation-and-platformos-support","title":"Cloud AI SDK Installation and Platform/OS Support","text":"What operating systems and platforms are supported? <p>See Operating System and Platform Support</p> Where do I download the SDKs? <p>Cloud AI SDK consists of a Platform and Apps SDK. Refer to Cloud AI SDKs for more information. </p> <p>For Platform SDK download, see Platform SDK Download</p> <p>For Apps SDK download, see Apps SDK Download</p> What environment variables need to be set to resolve toolchain errors such as libQAic.so? <p>Set the environment variables as mentioned here</p>"},{"location":"FAQ/#deep-learning-frameworks-and-networks","title":"Deep Learning frameworks and networks","text":"Which deep learning frameworks for supported by Cloud AI SDKs? <p>Onnx, tensorflow, pytorch, caffe or caffe2 are supported by the compiler.  <code>qaic-exec</code> can dump the operators supported across different frameworks. Onnx has the best operator support. </p> Which deep learning neural networks are supported? <p>Cloud AI platforms supports many network categories - Computer vision, object detection, Semantic segmentation, Natural language processing, ADAS and Generative AI networks.  Performance information can be found in the Qualcomm Cloud AI 100 page  Model recipes can be found in the <code>cloud-ai-sdk</code> github. </p> I have a neural network that I would like to run on Cloud AI platforms. How do I go about it? <p>There are 3 steps run an inference on Cloud AI platforms. </p> <ol> <li>Export the model in ONNX format (preferred due to operator support) and prepare the model </li> <li>Compile the model to generate a QPC (Qaic Program Container)</li> <li>Execute, integrate and deploy into production pipeline</li> </ol> <p>The quick start guide provides a quick overview of the steps involved in running inference using a vision transformer model as an example. </p> <p>Refer to Inference Workflow for detailed information how to onboard and run inference on Cloud AI platforms. </p> <p>Users can also refer to the model recipes that provide the best performance for several networks across several categories. </p> <p>Tutorials are another resource that walks through the workflows to onboard models, tune for best performance, profile inferences etc. </p>"},{"location":"FAQ/#system-management","title":"System Management","text":"Which utility/tool is used to query health, telemetry etc of all Cloud AI cards in the server? <p>Use the qaic-util CLI tool to query health, telemetry etc of Cloud AI cards in the server. </p> The Cloud AI device shows <code>Status:Error</code>. How do i fix it? <p><code>Status: Error</code> could be due to one of the following:</p> <ul> <li>indicates the respective card(s) has not booted up completely </li> <li>user has not used <code>sudo</code> prefix if user has not been added to <code>qaic</code> group. <code>sudo /opt/qti-aic/tools/qaic-util</code></li> <li>unsupported OS/platforms, secure boot etc</li> </ul> <p>Users can try to issue an soc_reset to see if the device recovers. </p>"},{"location":"Getting-Started/","title":"User Guide","text":"<p>Cloud AI SDKs enable developers to optimize trained deep learning models for high-performance inference. The SDKs provide workflows to optimize the models for best performance,  provides runtime for execution and supports integration with ONNX Runtime and Triton Inference Server for deployment.</p> <p>Cloud AI SDKs support</p> <ul> <li>High performance Generative AI, Natural Language Processing, and Computer Vision models</li> <li>Optimizing performance of the models per application requirements (throughput, accuracy and latency) through various quantization techniques</li> <li>Development of inference applications through support for multiple OS and Docker containers.  </li> <li>Deployment of inference applications at scale with support for Triton inference server</li> </ul>"},{"location":"Getting-Started/#cloud-ai-sdks","title":"Cloud AI SDKs","text":"<p>The Cloud AI SDK consists of the Application (Apps) SDK and Platform SDK.</p> <p>The Application (Apps) SDK is used to convert models and prepare runtime binaries for Cloud AI platforms.  It contains model development tools, a sophisticated parallelizing graph compiler, performance and integration tools, and code samples. Apps SDK is supported on x86-64 Linux-based systems.</p> <p>The Platform SDK provides driver support for Cloud AI accelerators, APIs and tools for executing and debugging model binaries, and tools for card health, monitoring and telemetry. Platform SDK consists of a kernel driver, userspace runtime with APIs and language bindings, and card firmware. Platform SDK is supported on x86-64 and ARM64 hosts.  </p> <p> </p>"},{"location":"Getting-Started/#installation","title":"Installation","text":"<p>The installation guide covers</p> <ul> <li>Platforms, operating systems and hypervisors supported and corresponding pre-requisites</li> <li>Cloud AI SDK (Platform and Apps SDK) installation</li> <li>Docker support</li> </ul>"},{"location":"Getting-Started/#inference-workflow","title":"Inference Workflow","text":"<p>Inference workflow details the Cloud AI SDK workflow and tool support - from onboarding a pre-trained model to deployment on Cloud AI platforms. </p>"},{"location":"Getting-Started/#release-notes","title":"Release Notes","text":"<p>Cloud AI release notes provide developers with new features, limitations and modifications in the Platform and Apps SDKs.   </p>"},{"location":"Getting-Started/#sdk-tools","title":"SDK Tools","text":"<p>SDK Tools provides details on usage of the tools in the SDKs used in the Inference workflow as well as card management. </p>"},{"location":"Getting-Started/#tutorials","title":"Tutorials","text":"<p>Tutorials, in the form of Jupyter Notebooks walk the developer through the Cloud AI inference workflow as well as the tools used in the process. Tutorials are divided into CV and NLP to provide a better developer experience even though the inference workflows are quite similar. </p>"},{"location":"Getting-Started/#model-recipes","title":"Model Recipes","text":"<p>Model recipes provide the developer the most performant and efficient way to run some of the popular models across categories. The recipe starts with the public model. The model is then exported to ONNX, some patches are applied if required, compiled and executed for best performance. Developers can use the recipe to integrate the compiled binary into their inference application.   </p>"},{"location":"Getting-Started/#sample-code","title":"Sample Code","text":"<p>Sample code helps developers get familiar with the usage of Python and C++ APIs for inferencing on Cloud AI platforms. </p>"},{"location":"Getting-Started/#system-management","title":"System Management","text":"<p>System Management details management for Cloud AI Platforms. </p>"},{"location":"Getting-Started/#architecture","title":"Architecture","text":"<p>Architecture provides insights into the architecture of Cloud AI SoC and AI compute cores. </p>"},{"location":"Getting-Started/Architecture/","title":"Architecture","text":""},{"location":"Getting-Started/Architecture/#cloud-ai-platforms","title":"Cloud AI Platforms","text":"<p>Cloud AI Platforms/cards contain one or more Cloud AI 100 SoCs per card and are designed to operate at a specific TDP. The cards interface to the host via PCIe and to the BMC via SMBus/I2C.  </p> <p>Cloud AI 100 Product brief provides details on the form factors and SKUs. </p>"},{"location":"Getting-Started/Architecture/#cloud-ai-100-soc","title":"Cloud AI 100 SoC","text":"<p>Cloud AI100 SOC\u2019s multi-core architecture, shown in figure below, is purpose-built for deep learning inference in the Cloud.  </p> <p>The SOC is composed of 16 seventh-generation AI cores delivering 400+ Int8 TOPs and 200+ FP16 TOPs of compute performance, with 144 MB of on-chip memory for data storage. </p> <p>The on-chip memory sub-system is connected to an external LPDDR4X memory subsystem, comprising of 4 channels of 64b width(4 x 64b), providing 136 GB/s of memory bandwidth and up to 32 GB of memory capacity. </p> <p>The SoC also provides 8 lanes of PCIe Gen4 IO interfaces (PCIe-gen4 x 8) to connect to the host CPU complex and other peripherals. </p> <p>The AI cores and all internal subsystems are connected by three advanced NoCs that deliver 186 GB/s of data bandwidth and support multicast and AI core synchronization. The Compute NoC connects the AI cores and PCIe, the Memory NoC connects the AI cores to the DDR memory, and the Configuration NoC is used for boot and hardware configuration. </p> <p>The SoC is also equipped with a sophisticated power management system, optimizing both transient and peak power consumption. The SOC also implements thermal detection and control. </p> <p>The SOC implements several cloud-readiness security features, including ECC, secure boot and DDR memory zero-out on reset.</p>"},{"location":"Getting-Started/Architecture/#ai-core","title":"AI Core","text":"<p>The seventh-generation AI Core, leveraging over a decade of Qualcomm AI research, is shown in Figure below. The AI Core implements the architecture principle of separation-of-concerns, with three types of compute units for tensor, vector and scalar operations. </p> <p> </p> <p>The tensor unit implements two 2D MAC arrays\u20148K for 8b integer, and 4K for 16b floating point, with 125+ instructions conducive for linear algebra, providing throughput of 8192/4096 MAC operations per clock for 8/16b integer/floating point operations respectively.  </p> <p>The vector unit implements over 700+ rich instructions for AI, content verification, and image processing supporting 8/16b integer and 16/32b precision floating point, providing throughput of 512/256 MAC operations per clock for 8/16b integer/floating point operations respectively. </p> <p>The scalar processor is a 4-way VLIW (very large instruction word) machine supporting six hardware threads, each with a local scalar register file, instruction and data caches, and support for 8/16b integer and 16/32b floating point operations\u2014a rich instruction set of over 1800 instructions that provide flexibility in compute operations. </p> <p>An 8 MB Vector Tightly Coupled Memory (VTCM) provides scratch-pad data storage for both the vector unit as well as the tensor unit. The 1 MB L2 cache in the core is shared by all three compute units (scalar, vector, and tensor), and it implements hardware for the intelligent prefetching of data and for advanced DMA operations. </p>"},{"location":"Getting-Started/Glossary/","title":"Cloud AI Glossary","text":"Term Description AI Cores / AI compute cores / NSP These terms are interchangably used to refer to the Neural Signal Processor cores in the Cloud AI Platforms. The NSP cores run the inference. Instance / activation A single Instance or activation refers to a QPC executing on a set of AI cores. QAIC / AIC / Cloud AI/ AIC100 These terms are interchangably used to refer to Cloud AI. These terms are typically followed by the subject (like Compiler / SDK / library etc) being discussed. QPC (Qaic Program Container) / model binary / network binary Cloud AI platforms can execute a QPC. A pre-trained model needs to be compiled to generate a QPC. QPC / model binary / network binary terms are used interchangably. TCM / VTCM (Vector) Tightly-Coupled Memory.  On-chip or in-NSP memory as featured in the ARM specification. ECC Error Correcting Code.  Error correction in the TCM as featuerd in the ARM specification. VM/KVM (Kernel-based) Virtual Machine. ONNX Open Neural Network Exchange file format."},{"location":"Getting-Started/Inference-Workflow/","title":"Inference workflow on Cloud AI","text":"<p>Developers can deploy a pre-trained model for inference on any Cloud AI platform using three easy steps:</p> <ol> <li>Export the model in ONNX format and prepare the model </li> <li>Compile the model to generate a QPC (Qaic Program Container)</li> <li>Execute, integrate and deploy into production pipeline</li> </ol> <p></p> <p>Cloud AI SDK provides tools for each of the steps as shown below. </p> <p></p>"},{"location":"Getting-Started/Inference-Workflow/#export-and-prepare-the-model","title":"Export and prepare the Model","text":"<p>Exporting and preparing the pre-trained model as explained in this section is a requirement to extract the best performance and accuracy. Exporting the model in ONNX format is strongly recommended due to operator support. The ONNX file is then passed through a few checks (for model accuracy and performance) before it can be compiled in the next step.   </p>"},{"location":"Getting-Started/Inference-Workflow/#compile-the-model","title":"Compile the Model","text":"<p>Compilation of the prepared ONNX file generates a QPC which can be loaded and executed on Cloud AI devices. To get the best performance from the device based on user requirements of throughput and latency, compilation needs to be done with the right parameters. Tune the performance goes over how to derive the best parameters for compilation. </p>"},{"location":"Getting-Started/Inference-Workflow/#execute-integrate-and-deploy-in-production-pipeline","title":"Execute, integrate and deploy in production pipeline","text":"<p>This section will go over the different ways (CLI, C++, Python etc) for developers to execute inferences on Cloud AI platforms and all the tools/utilities available to integrate into inference application and deploy in the production pipeline.  </p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Export-the-Model/","title":"Exporting ONNX Model from Different Frameworks","text":"<p>We will start with the process of exporting an ONNX model from various deep learning frameworks such as PyTorch, TensorFlow, and Caffe2.</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Export-the-Model/#exporting-from-pytorch","title":"Exporting from PyTorch","text":"<p>PyTorch is a popular deep learning framework that provides dynamic computation graphs. To export a PyTorch model to ONNX format, you need to follow these steps:</p> <pre><code>import torch\nimport torchvision.models as models\n# Load a pre-trained PyTorch model\nmodel = models.resnet18(pretrained=True)\n# Set the model to evaluation mode\nmodel.cpu().eval()\n# Define example input tensor\ninput_tensor = torch.randn(1, 3, 224, 224).cpu()  # Replace with your own input shape\n# Export the model to ONNX format\ntorch.onnx.export(model, input_tensor, \"model.onnx\", verbose=True, opset=13)\n</code></pre> <p>In the above code, we import the necessary libraries and load a pre-trained ResNet-18 model. We then set the model to evaluation mode and define an example input tensor. Finally, we use the <code>torch.onnx.export</code> function to export the model to the ONNX format. You can replace <code>\"model.onnx\"</code> with the desired output file path.</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Export-the-Model/#exporting-from-tensorflow","title":"Exporting from TensorFlow","text":"<p>TensorFlow is another widely used deep learning framework that provides both static and dynamic computation graphs. Here's how you can export a TensorFlow model to ONNX format:</p> <pre><code>import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nimport tf2onnx\n# Load a pre-trained TensorFlow model\nmodel = ResNet50(weights='imagenet')\n# onnx_model = tf2onnx.convert.from_keras(model)\ninput_tensor_spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name=\"input\"),)\n# Convert the TensorFlow model to ONNX format\n# Save the ONNX model to a file\nmodel_proto, _ = tf2onnx.convert.from_keras(model, input_signature=input_tensor_spec, opset=13, output_path=\"model.onnx\")\n</code></pre> <p>In the above code, we import the necessary libraries and load a pre-trained ResNet-50 model using Keras. We then convert the TensorFlow model to the ONNX format using the <code>tf2onnx.convert.from_keras</code> function. Finally, we save the ONNX model to a file using the <code>tf2onnx.save_model</code> function.</p> <p>In case you have save tensorflow model or saved checkpoint path of tensorflow model, you can load the model in tensorflow/keras or you can directly use the following commands from <code>tf2onnx</code> libraries.</p> <pre><code># You have path to your saved TensorFlow model\npython -m tf2onnx.convert --saved-model tensorflow-model-path --opset 11 --output model.onnx\n\n# For checkpoint format:\npython -m tf2onnx.convert --checkpoint  tensorflow-model-meta-file-path --output model.onnx --inputs input0:0,input1:0 --outputs output0:0\n\n# For graphdef format:\npython -m tf2onnx.convert --graphdef  tensorflow-model-graphdef-file --output model.onnx --inputs input:0,input:0 --outputs output0:0\n</code></pre> <p>You can refer this notebook from tf2onnx</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Export-the-Model/#exporting-from-caffe2","title":"Exporting from Caffe2","text":"<p>Caffe2 is a deep learning framework primarily developed by Facebook. It focuses on performance and is widely used for production deployments. Here's an example of exporting a Caffe2 model to ONNX format:</p> <pre><code>from caffe2.python.onnx import backend\nfrom caffe2.python.predictor import predictor_exporter\n# Load the Caffe2 model from .pb and .init files\ninit_net = \"model.init\"  # Replace with the path to the .init file\npredict_net = \"model.pb\"  # Replace with the path to the .pb file\n# Export the Caffe2 model to ONNX format\nonnx_model = backend.prepare(model, device='CPU')\nonnx_model.export_onnx(\"model.onnx\")\n</code></pre> <p>In the above code, we pre-trained caffe2 model from .init and .pb file. Finally, we use the <code>caffe2.python.onnx.backend.prepare</code> function to export the model to the ONNX format. You can replace <code>\"model.onnx\"</code> with the desired output file path. Please refer caffe2-installation to setup the environment</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Export-the-Model/#loading-onnx-model","title":"Loading ONNX Model","text":"<p>After exporting the model from your preferred framework to ONNX, you can load it for inference using the <code>onnx</code> package. Here's an example:</p> <pre><code>import onnx\nimport onnx.checker\n# Load the ONNX model\nmodel = onnx.load(\"path/to/exported_model.onnx\")\ntry:\nonnx.checker.check_model(model)\nexcept:\nonnx.checker.check_model_path(\"path/to/exported_model.onnx\")\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Operator-and-Datatype-support/","title":"Operator and Datatype support","text":""},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Operator-and-Datatype-support/#operator-support","title":"Operator support","text":"<p>In this section we will discuss the operator support available for the device across various frameworks. </p> <p>To determine the list of operators supported by the device for various frameworks, you can execute the following command</p> <p>Example command to generate operators supported for onnx<pre><code>/opt/qti-aic/exec/qaic-exec -operators-supported=onnx\n</code></pre> This command generators a file <code>OnnxSupportedOperators.txt</code> which comprehensive list of ops supported. </p> <p>It is important to note that the operator support keep expanding with the release of new SDK versions.</p> Note <pre><code>-operators-supported supports only onnx, tensorflow, pytorch, caffe, and caffe2.\n</code></pre> Note <p><code>onnx</code> is the preferred format to compile the model for the device. </p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Operator-and-Datatype-support/#handling-unsupported-operators","title":"Handling Unsupported Operators","text":"<p>In some cases, you might encounter errors related to unsupported operations while compiling the model for the device. </p> <p>For instance, certain operations like <code>einsum</code> present in the model file might not be directly supported by the device. </p> <p>In such scenarios, the Model Preparator tool can be employed to modify the model and substitute these unsupported operations with their corresponding mathematical equivalent subgraphs.</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Operator-and-Datatype-support/#datatype-support","title":"Datatype Support","text":"<p>The device performance is optimum for trained model weights in either <code>fp16</code> (half-precision) or <code>int8</code> formats.</p> <p>If the model is originally trained in <code>fp32</code> format, it gets downconverted to <code>fp16</code> format with <code>-convert-to-fp16</code> during the compilation process. In certain scenarios, models may contain constants which are beyond the <code>fp16</code> range. In those scenarios, it is recommended to clip to <code>fp16</code> range as shown in this notebook, see the <code>fix_onnx_fp16</code> function. </p> <p>The device also offers support for mixed precision, specifically <code>fp16</code> and <code>int8</code>. While the device is technically capable of running <code>fp32</code> precision models using the scalar processor, its important to note that the performance achieved will be suboptimal compared to models utilizing <code>fp16</code> or <code>int8</code> precision. </p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Operator-and-Datatype-support/#conversion-from-bf16-to-fp16","title":"Conversion from bf16 to fp16","text":"<p>If the model is trained in <code>bf16</code> (bfloat16),  Qualcomm can provide a script to identify the scaling factors to scale down the weights of the models such that intermediate activations will not overflow 'fp16'.</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/","title":"Introduction to the Model Preparator Tool","text":"<p>The QAic Model Preparator is an optional tool that automates generating Optimal AIC Friendly models for usage. It applies various optimizations and cleaning techniques for generation of the model. Developers can use this tool if the exported model fails compilation. </p> <p>The current version supports ONNX and TF models. The tool checks the model, applies shape inference, cleans the model, applies various optimization, handles the pre-processing/post-processing nodes in the graph, and generates models as per the given user configuration (YAML).</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#model-preparator-overview","title":"Model Preparator Overview","text":"<p>Checkout the Model Export page on details on exporting the model from one framework to another framework, the most preferred and tested framework on MP is Onnx.</p> <p>For detailed usage of the tool, refer to this link</p> <p></p> <p>usage: Sample Config Details are  For Single model:</p> <pre><code>MODEL: \n    INFO: \n        MODEL_TYPE: EFFICIENTDET\n        INPUT_INFO: \n        - - input:0\n          - - 1\n            - 512\n            - 512\n            - 3\n        DYNAMIC_INFO: \n        - - input:0\n          - - batch_size\n            - 512\n            - 512\n            - 3\n        EXPORT_TYPE: ONNX\n        OPSET: 13\n        MODEL_PATH: efficientdet-d0.pb\n        NAME: Sample Model\n        DESCRIPTION: SSD MobileNet V1 Opset 10 Object Detection Model\n        VALIDATE: False\n        AIC_DEVICE_ID: 0\n        WORKSPACE: WORKSPACE\n        VERBOSE: INFO\n        INPUT_LIST_FILE: None\n        CUSTOM_OP_INFO_FILEPATH: None\n\nPRE_POST_HANDLE: \n    NMS_PARAMS: \n        MAX_OUTPUT_SIZE_PER_CLASS: 100\n        MAX_TOTAL_SIZE: 100\n        IOU_THRESHOLD: 0.65\n        SCORE_THRESHOLD: 0.25\n        CLIP_BOXES: False\n        PAD_PER_CLASS: False\n\n    PRE_PLUGIN: True\n    TRANSFORMER_PACKED_MODEL: False\n    COMPRESSED_MASK: False\n    POST_PLUGIN: SMARTNMS\n    ANCHOR_BIN_FILE: None\n\nATTRIBUTES: None\n</code></pre> <p>Usage on Multi Model Config:</p> <pre><code>MODELS: \n  MODEL: \n      INFO: \n          MODEL_TYPE: CLASSIFICATION\n          INPUT_INFO: \n          - - model_1_input\n            - - 1\n              - 100\n          DYNAMIC_INFO: \n          - - model_1_input\n            - - batch\n              - 100\n          EXPORT_TYPE: ONNX\n          OPSET: 13\n          MODEL_PATH: model_1.onnx\n          NAME: Model_1\n          DESCRIPTION: Sample model-1\n          VALIDATE: True\n          AIC_DEVICE_ID: 0\n          WORKSPACE: ./workspace\n          VERBOSE: INFO\n          INPUT_LIST_FILE: None\n          CUSTOM_OP_INFO_FILEPATH: None\n\n  PRE_POST_HANDLE: \n      NMS_PARAMS: \n          MAX_OUTPUT_SIZE_PER_CLASS: 100\n          MAX_TOTAL_SIZE: 500\n          IOU_THRESHOLD: 0.5\n          SCORE_THRESHOLD: 0.01\n          CLIP_BOXES: False\n          PAD_PER_CLASS: False\n\n      PRE_PLUGIN: True\n      TRANSFORMER_PACKED_MODEL: False\n      COMPRESSED_MASK: False\n      POST_PLUGIN: NONE\n      ANCHOR_BIN_FILE: None\n\n  ATTRIBUTES: None\n\n\n  MODEL: \n    INFO: \n        MODEL_TYPE: CLASSIFICATION\n        INPUT_INFO: \n        - - model_2_input\n          - - 1\n            - 10\n        DYNAMIC_INFO: \n        - - model_2_input\n          - - batch\n            - 10\n        EXPORT_TYPE: ONNX\n        OPSET: 13\n        MODEL_PATH: model_2.onnx\n        NAME: Model_2\n        DESCRIPTION: Sample model-2\n        VALIDATE: True\n        AIC_DEVICE_ID: 0\n        WORKSPACE: ./workspace\n        VERBOSE: INFO\n        INPUT_LIST_FILE: None\n        CUSTOM_OP_INFO_FILEPATH: None\n\n  PRE_POST_HANDLE: \n      NMS_PARAMS: \n          MAX_OUTPUT_SIZE_PER_CLASS: 100\n          MAX_TOTAL_SIZE: 500\n          IOU_THRESHOLD: 0.5\n          SCORE_THRESHOLD: 0.01\n          CLIP_BOXES: False\n          PAD_PER_CLASS: False\n\n      PRE_PLUGIN: True\n      TRANSFORMER_PACKED_MODEL: False\n      COMPRESSED_MASK: False\n      POST_PLUGIN: NONE\n      ANCHOR_BIN_FILE: None\n\n  ATTRIBUTES: None\n</code></pre> <p>CHAINING_INFO:      EDGES:      - - Model_1::model_1_output       - Model_2::model_2_input</p> <pre><code>WORKSPACE: ./workspace_chaining/\nEXPORT_TYPE: ONNX\nOPSET: 13\nAIC_DEVICE_ID: 0\n</code></pre> <p>qaic-preparator options</p> <p>Config Options:   --config CONFIG  Path to config file   --silent         Run in silent mode</p> <p>For more details checkout the help menu in the tool: \"python3 qaic-model-preparator.py --help\"  Tool location in the SDK: \"/opt/qti-aic/tools/qaic-pytools\"</p>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#sample-details-on-model-configuration-settings-file","title":"Sample details on Model Configuration Settings file","text":""},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#model-preparator-configurations","title":"Model Preparator Configurations","text":"<ul> <li>MODEL <ul> <li> DESCRIPTION : Model Description</li> <li> MODEL_TYPE  : Provide the Model Category it belongs to</li> <li> MODEL_PATH  : Provide path to the downloaded original model file</li> <li> INPUT_INFO  : Provide Dictionary of Input names and its corresponding shapes.</li> <li> DYNAMIC  : Provide Dynamic Info to generate dynamic models</li> <li> EXPORT_TYPE  : ONNX or TENSORFLOW</li> <li> VALIDATE  : Validate the prepared model outputs with native framwork. (Ex: QAic FP16 vs OnnxRuntime FP32)</li> <li> WORKSPACE   : Provide the workspace directory to save the log files and final prepared model.</li> </ul> </li> <li>PRE_POST_HANDLE<ul> <li> ANCHOR_BIN_FILE   : Path to Anchor bin files or None.</li> <li> POST_PLUGIN  : None by Default, If Object Detection Models, Provide \"Smart-nms\" or \"QDetect\" to either generate split model or full model.</li> <li> PRE_PLUGIN   : Handle pre plugin, This will be either keep/remove the pre processing from the graph.</li> <li> NMS_PARAMS   : This fields are specific to Object Detection models, and are helpful while creating full model based on QDetect nodes.<ul> <li> MAX_OUTPUT_SIZE_PER_CLASS   : A scalar integer representing the maximum number of boxes to be selected by non-max suppression per class</li> <li> MAX_TOTAL_SIZE   : A integer representing maximum number of boxes retained over all classes. Note that setting this value to a large number may result in OOM error depending on the system workload.</li> <li> IOU_THRESHOLD   : A float representing the threshold for deciding whether boxes overlap too much with respect to IOU.</li> <li> SCORE_THRESHOLD   : A float representing the threshold for deciding when to remove boxes based on score.</li> <li> CLIP_BOXES   : If true, the coordinates of output nmsed boxes will be clipped to [0, 1]. If false, output the box coordinates as it is. Defaults to true.</li> <li> PAD_PER_CLASS   : If false, the output nmsed boxes, scores and classes are padded/clipped to max_total_size.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#optimizations-around-pre-and-post-processing-handling-in-preparator","title":"Optimizations Around Pre and Post Processing Handling in Preparator","text":"<ul> <li>PRE_PLUGIN<ul> <li>The  PRE_PLUGIN  field in the config is a boolean flag indicating the application of pre-processing optimizations if any. For example: In most of the computer vision models from the TensorFlow network we have seen that the preprocessing and post processing is part of the model graph making it dynamic in nature.</li> <li>Since we are working mostly on static models (due to AOT), this will cause issues in the model compilation. Making the PRE_PLUGIN True will help in handling such pre-processing in the model graph and making it AIC friendly.</li> <li>The preprocessing which is part of the model is handled and details regarding the same is tapped onto the log files.</li> <li> <p>The TF Optimizer looks for control-flow operators inside the model and replaces them with the appropriate/relevant operators.Major Optimizations include:</p> <ul> <li> <p>Switch-Merge Optimizer:</p> <ul> <li>This arises due to the \u201ctf.while_loop()\u201d and \u201ctf.cond()\u201d APIs. The \u201cprediction\u201d flag of the switch operator is evaluated, and the corresponding path is selected for the model. A switch operator whose prediction flag is not constant cannot be optimized.</li> </ul> </li> <li> <p>Loop Unrolling:</p> <ul> <li>This arises due to the \u201ctf.while_loop()\u201d APIs. The loop is unrolled to make a Directed Acyclic Graph (DAG) based on the number of loop iterations. It cannot be applied in cases where the number of loop iterations is not fixed or there is a nested loop.</li> </ul> </li> <li> <p>TensorArray Optimizations:</p> <ul> <li>This includes all tensor-array operators like (TensorArray, TensorArrayWrite, TensorArrayRead, TensorArrayGather, TensorArrayScatter, and so on). This optimization replaces tensor-array operators with relevant known and supported operators.</li> <li>For most of the NLP models, we have tokenization encodings of the inputs which we do in the host and pass the tokenized inputs to the model.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p></p> <ul> <li>POST_PLUGIN<ul> <li>This field is mostly applicable to the Object Detection CV models, at QAic we have two options to run the ABP + NMS. Smart-NMS and QDetect.</li> <li>SMART-NMS is a software application to help run the post processing part of the Object Detection (OD) models on host. Enabling this in the config will help add SMART-NMS to the model. Here we have intelligent way of identifying the post processing of the model if any and generating SMART-NMS specific model.</li> <li>QDetect is a known custom operator from QAic to enabling running the post processing of the OD models on the device. Enabling this in the config, will help generate add QDetect to the model.</li> <li>For NLP networks, the post-processing is decoding the tokens generated from model. Here we can use HuggingFace generation utils to do the post processing on the host.</li> </ul> </li> </ul>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#tutorial-converting-and-executing-a-yolov5-model-with-qaic","title":"Tutorial: Converting and executing a YoloV5 model with QAic","text":"<p>The following tutorial will demonstrate the end to end usage of Model Preparator Tool and the QAic Execution. This process begins with a trained source framework model, which is converted and built into a optimization model using Model Preparator, which are then executed on a QAic backend.</p> <p>The tutorial will use YoloV5 as the source framework model.</p> <p>The sections of the tutorial are as follows:</p> <ul> <li>Tutorial Setup: Download the Original Model</li> <li>Model Conversion: Generate the Optimized Model using Model Preparator APIs</li> <li>Executing Example Model: Run the model on QAic Backend.</li> <li>Plot Model Outputs on Sample Image</li> </ul>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#tutorial-setup","title":"Tutorial Setup","text":"<p>The tutorial assumes general setup instructions have been followed at Setup.</p> <pre><code>import os\ncwd = os.getcwd()\n# Steps to generate the models from ultralytics repo.\n!rm -rf yolov5\n# Clone the Ultralytics repo.\n!git clone --branch v6.0 --depth 1 https://github.com/ultralytics/yolov5.git\n# Install the requirements\n%cd yolov5\n!git checkout v6.0\n!pip3 install seaborn\n# Export the Original yolo models.\n!python3 -W ignore export.py --weights ./yolov5s.pt --include onnx --opset 13\n# Copy the original model to the relevant paths in sdk\n!cp yolov5s.onnx /opt/qti-aic/tools/qaic-pytools\n%cd ..\n!rm -rf yolov5/\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#model-conversion","title":"Model Conversion","text":"<p>After the model assets have been acquired the model can be converted to optimized QAic model, and subsequently built for use by an application. In the below example we will be generating the optimized QDetect model as explained in the above pre/post processing section.</p> <pre><code># Path to the installed SDK apps tools\n%cd /opt/qti-aic/tools/qaic-pytools\n# Run the preparator plugin with the relevant config file\n!python -W ignore qaic-model-preparator.py --config /opt/qti-aic/tools/qaic-pytools/model_configs/samples/preparator/public/yolov5_ultralytics_model_info_qdetect.yaml --silent\n# More details on various configs can be found here: /opt/qti-aic/tools/qaic-pytools/model_configs/samples/preparator/public\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#qaic-backend-execution","title":"QAic Backend Execution","text":"<p>Compile the model binary using the qaic-exec and execute it using the qaic-runner with the following commands:</p> <pre><code>!rm -rf yolo-binaries-qdetect\n!/opt/qti-aic/exec/qaic-exec -m=/opt/qti-aic/tools/qaic-pytools/WORKSPACE/yolov5s_preparator_aic100.onnx \\\n-aic-hw -onnx-define-symbol=batch_size,1 -aic-binary-dir=yolo-binaries-qdetect -convert-to-fp16 -compile-only\n</code></pre> <pre><code>!/opt/qti-aic/exec/qaic-runner -t yolo-binaries-qdetect -a 1 -d 0 --time 15\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/Export-the-model/Prepare-the-model/#plot-model-outputs-on-sample-image","title":"Plot Model Outputs on Sample Image","text":"<p>Once we have the performance numbers, we will setup some model artifacts based on sample real image, Pre-Process, convert it to raw generate model outputs based on qaic-runner. Then using the generated outputs, we will plot it on the input images and get the final bounding boxes.</p> <pre><code># Imports\nimport os\nimport time\nimport cv2\nimport numpy as np\nimport torch\nfrom PIL import Image\ndef letterbox_image(image, size):\niw, ih = image.size\nw, h = size\nscale = min(w / iw, h / ih)\nnw = int(iw * scale)\nnh = int(ih * scale)\nimage = image.resize((nw, nh), Image.BICUBIC)\nnew_image = Image.new(\"RGB\", size, (128, 128, 128))\npad_left = (w - nw) // 2\npad_top = (h - nh) // 2\nnew_image.paste(image, (pad_left, pad_top))\nreturn new_image, scale, pad_left, pad_top\ndef preprocessImg(image_path, input_h, input_w):\nimage_src = Image.open(image_path)\nresized, scale, pad_left, pad_top = letterbox_image(image_src, (input_w, input_h))\nimg_in = np.transpose(resized, (2, 0, 1)).astype(np.float32)  # HWC -&gt; CHW\nimg_in = np.expand_dims(img_in, axis=0)\nimg_in /= 255.0\nreturn img_in, np.array(image_src, dtype=np.uint8), scale, pad_left, pad_top\n</code></pre> <pre><code># Download the sample image and pre-process\n!rm -rf *.jpg\n!wget -c https://ultralytics.com/images/zidane.jpg\nimg_preproc, img_orig, scale, pad_left, pad_top = preprocessImg(\"zidane.jpg\", 640, 640)\nimg_preproc.tofile(\"input.raw\")\n</code></pre> <pre><code># Generate the outputs\n!/opt/qti-aic/exec/qaic-runner -t yolo-binaries-qdetect -i input.raw --write-output-dir yolo-output-qdetect -d 0\n</code></pre> <pre><code># Model artifacts to post process and plot the outputs on the image\ndef scale_coords(\ncoords, img_orig_h, img_orig_w, scale, pad_left, pad_top, xy_swap=True\n):\n# Rescale coords (xyxy) from preprocessed img to original img resolution\nif xy_swap:\ncoords[:, :, [0, 2]] -= pad_top  # y padding\ncoords[:, :, [1, 3]] -= pad_left  # x padding\nelse:\ncoords[:, :, [0, 2]] -= pad_left  # x padding\ncoords[:, :, [1, 3]] -= pad_top  # y padding\ncoords[:, :, :4] /= scale\n# Clip bounding xyxy bounding boxes to image shape (height, width)\nif xy_swap:\ncoords[:, :, 0] = coords[:, :, 0].clip(0, img_orig_h - 1)  # y1\ncoords[:, :, 1] = coords[:, :, 1].clip(0, img_orig_w - 1)  # x1\ncoords[:, :, 2] = coords[:, :, 2].clip(0, img_orig_h - 1)  # y2\ncoords[:, :, 3] = coords[:, :, 3].clip(0, img_orig_w - 1)  # x2\nelse:\ncoords[:, :, 0] = coords[:, :, 0].clip(0, img_orig_w - 1)  # x1\ncoords[:, :, 1] = coords[:, :, 1].clip(0, img_orig_h - 1)  # y1\ncoords[:, :, 2] = coords[:, :, 2].clip(0, img_orig_w - 1)  # x2\ncoords[:, :, 3] = coords[:, :, 3].clip(0, img_orig_h - 1)  # y2\nreturn coords\nclass_names = [\n\"person\",\n\"bicycle\",\n\"car\",\n\"motorcycle\",\n\"airplane\",\n\"bus\",\n\"train\",\n\"truck\",\n\"boat\",\n\"traffic light\",\n\"fire hydrant\",\n\"stop sign\",\n\"parking meter\",\n\"bench\",\n\"bird\",\n\"cat\",\n\"dog\",\n\"horse\",\n\"sheep\",\n\"cow\",\n\"elephant\",\n\"bear\",\n\"zebra\",\n\"giraffe\",\n\"backpack\",\n\"umbrella\",\n\"handbag\",\n\"tie\",\n\"suitcase\",\n\"frisbee\",\n\"skis\",\n\"snowboard\",\n\"sports ball\",\n\"kite\",\n\"baseball bat\",\n\"baseball glove\",\n\"skateboard\",\n\"surfboard\",\n\"tennis racket\",\n\"bottle\",\n\"wine glass\",\n\"cup\",\n\"fork\",\n\"knife\",\n\"spoon\",\n\"bowl\",\n\"banana\",\n\"apple\",\n\"sandwich\",\n\"orange\",\n\"broccoli\",\n\"carrot\",\n\"hot dog\",\n\"pizza\",\n\"donut\",\n\"cake\",\n\"chair\",\n\"couch\",\n\"potted plant\",\n\"bed\",\n\"dining table\",\n\"toilet\",\n\"tv\",\n\"laptop\",\n\"mouse\",\n\"remote\",\n\"keyboard\",\n\"cell phone\",\n\"microwave\",\n\"oven\",\n\"toaster\",\n\"sink\",\n\"refrigerator\",\n\"book\",\n\"clock\",\n\"vase\",\n\"scissors\",\n\"teddy bear\",\n\"hair drier\",\n\"toothbrush\",\n]\ndef drawBoxes(\ndetections,\nimg_orig,\nscale=1.0,\npad_left=0,\npad_top=0,\nxy_swap=True,\nfilter_score=0.2,\nline_thickness=None,\ntext_bg_alpha=0.0,\n):\ndetection_boxes = detections[0]  # [1, N, 4]\ndetection_scores = detections[1]  # [1, N]\ndetection_classes = detections[2]  # [1, N]\nnum_detections = detections[3]  # [1]\nimg_orig_h, img_orig_w = img_orig.shape[:2]\ndetection_boxes = scale_coords(\ndetection_boxes, img_orig_h, img_orig_w, scale, pad_left, pad_top, xy_swap\n)\ntl = line_thickness or round(0.002 * (img_orig_w + img_orig_h) / 2) + 1\nassert (\ndetection_boxes.shape[0] == 1\n), \"Currently plotting for single batch size only.\"\nfor i in range(num_detections[0]):\nbox = detection_boxes[0][i]\nscore = detection_scores[0][i]\ncls = detection_classes[0][i]\nif score &lt; filter_score:\ncontinue\nif xy_swap:\ny1, x1, y2, x2 = map(int, box)\nelse:\nx1, y1, x2, y2 = map(int, box)\nnp.random.seed(int(cls) + 2020)\ncolor = [np.random.randint(0, 255), 0, np.random.randint(0, 255)]\ncv2.rectangle(\nimg_orig,\n(x1, y1),\n(x2, y2),\ncolor,\nthickness=max(int((img_orig_h + img_orig_w) / 600), 1),\nlineType=cv2.LINE_AA,\n)\nlabel = \"%s %.2f\" % (class_names[int(cls)], score)\nt_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=1)[0]\nc2 = x1 + t_size[0] + 3, y1 - t_size[1] - 5\nif text_bg_alpha == 0.0:\ncv2.rectangle(img_orig, (x1 - 1, y1), c2, color, cv2.FILLED, cv2.LINE_AA)\nelse:\nalphaReserve = text_bg_alpha\nBChannel, GChannel, RChannel = color\nxMin, yMin = int(x1 - 1), int(y1 - t_size[1] - 3)\nxMax, yMax = int(x1 + t_size[0]), int(y1)\nimg_orig[yMin:yMax, xMin:xMax, 0] = img_orig[\nyMin:yMax, xMin:xMax, 0\n] * alphaReserve + BChannel * (1 - alphaReserve)\nimg_orig[yMin:yMax, xMin:xMax, 1] = img_orig[\nyMin:yMax, xMin:xMax, 1\n] * alphaReserve + GChannel * (1 - alphaReserve)\nimg_orig[yMin:yMax, xMin:xMax, 2] = img_orig[\nyMin:yMax, xMin:xMax, 2\n] * alphaReserve + RChannel * (1 - alphaReserve)\ncv2.putText(\nimg_orig,\nlabel,\n(x1 + 3, y1 - 4),\n0,\ntl / 3,\n[255, 255, 255],\nthickness=1,\nlineType=cv2.LINE_AA,\n)\nprint(\nf\"X1:{x1}, Y1:{y1}, X2:{x2}, Y2:{y2}, Score:{score:.4f}, Cls_id:{class_names[int(cls)]}\"\n)\nreturn img_orig\n</code></pre> <pre><code>boxes = np.fromfile(\"yolo-output-qdetect/detection_boxes-activation-0-inf-0.bin\", dtype=\"float32\").reshape(1, 100, 4)\nscores = np.fromfile(\"yolo-output-qdetect/detection_scores-activation-0-inf-0.bin\", dtype=\"float32\").reshape(1, 100)\nclasses = np.fromfile(\"yolo-output-qdetect/detection_classes-activation-0-inf-0.bin\", dtype=\"int32\").reshape(1, 100)\ndetections = np.fromfile(\"yolo-output-qdetect/num_detections-activation-0-inf-0.bin\", dtype=\"int32\").reshape(1)\ndecoded_output = [boxes, scores, classes, detections]\nimage_src = Image.open(\"zidane.jpg\")\nimg_orig = np.array(image_src, dtype=np.uint8)\nimg_orig_plotted = drawBoxes(decoded_output, img_orig, scale, pad_left, pad_top, True)\nimg_orig_plotted = cv2.cvtColor(img_orig_plotted, cv2.COLOR_RGB2BGR)\ncv2.imwrite(\"output_qdetect.jpg\", img_orig_plotted)\n</code></pre> <pre><code># Plot the outputs on the image\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimg = np.array(Image.open(\"output_qdetect.jpg\"))\nfig, ax = plt.subplots(figsize=(15, 15))\nax.imshow(img)\n</code></pre> <p>For more examples on various models, Checkout our notebooks in SDK. Location to notebooks in SDK: <code>/opt/qti-aic/tools/qaic-pytools/docs/preparator/examples/notebooks</code></p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Compile%20the%20Model/","title":"Compile the Model","text":"<p>The AI/ML models compilation step compiles a pre-trained model defined in other formats (ONNX is preferred) into QPC (Qaic Program Container) format. This is required, since  Cloud AI devices works on this format, to run inference.</p> <p>A pretrained model can be compiled in three ways:</p> <ol> <li>Using qaic-exec (Binary executable shipped with the Apps SDK). This CLI tool has support for all the latest compiler flags.  </li> <li>Using High Level Python APIs </li> <li>Using C++ APIs </li> </ol>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Compile%20the%20Model/#before-compiling-model-into-qpc-format","title":"Before compiling model into QPC format:","text":"<ol> <li>Use ONNX format as input.</li> <li>Try to squash the original model into a single file ONNX format.</li> </ol>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Compile%20the%20Model/#compilation-using-qaic-exec","title":"Compilation using qaic-exec","text":"<p>The qaic-exec (QAic executor), a CLI tool, is used to compile the model. <code>qaic-exec</code> is a sophisticated tool with a number of arguments. </p> <p>qaic-exec CLI is located at <code>/opt/qti-aic/exec/qaic-exec</code></p> <p>The tutorials and models folder (#FIXME) provides several examples of <code>qaic-exec</code> usage. </p> <p>The help/usage command provides the extensive list of arguments and their descriptions for usage. <pre><code>/opt/qti-aic/exec/qaic-exec -h\n/opt/qti-aic/exec/qaic-exec --usage\n</code></pre></p> <p>Few of the frequently used arguments are additionally explained: <pre><code>-m=&lt;path&gt;;, -model=&lt;path&gt; # specifies the path of input ONNX model \n\n-onnx-define-symbol=&lt;sym, value&gt; # defines the names and values of the ONNX symbols that needed to be passed into the QPC.\n  For example\n -onnx-define-symbol=sequence,10 # For single symbol\n -onnx-define-symbol=sequence,10 -onnx-define-symbol=batch_size,8 # For more than one symbol\n\n-aic-num-cores=&lt;numCores&gt;\n  The Cloud AI cards can have a number of Neural Signal Processing cores (a.k.a AI cores) based on the SKU. The Inferencing workload can be distributed among different cores, so that they can execute concurrently and can produce more efficient inferencing. Refer to Tune Performance section on how to set this value.\n\n-ols= &lt;1,2,4,8.. numCores&gt;\n  Factor to increase splitting of network operations for more fine-grained parallelism. Refer to Tune Performance section on how to set this value.\n\n-mos= &lt;1,2,4,8.. numCores&gt;\n  Effort level to reduce on-chip memory usage. Refer to Tune Performance section on how to set this value.\n\n-multicast-weights\n  Reduce DDR bandwidth by loading weights used on multiple-cores only once and multicasting to other cores. \n\n-convert-to-fp16\n  mandatory flag, ensures that compiled QPC executes all floating point computations on the AIC 100 device is 16-bit precision. \n\n-batchsize=&lt;numBatch&gt;\n  batchsize refers to number of number of input samples that can be passed to the model during inferencing. Ideally a careful selection of batch size can facilitate better parallel processing and hence a better throughput from the device. Tune performance section expands on batchsize selection. \n\n-stats-batchsize=&lt;numBatch&gt;\n  Set this value to numBatch. Used in performance statistics reporting\n\n-aic-binary-dir=&lt;path&gt;\n  specifies the output QPC path.\n\n-aic-hw\n  Mandatory flag. This flag enables the QPC to be run on hardware.\n\n-compile-only \n  Mandatory flag, allows to only compile and produce the QPC format file for the model and does not run the model with random data. Use qaic-runner CLI to execute the QPC. \n\n-aic-hw-version=2.0\n  The version string must be passed as \"2.0\" which is the .\n\n-aic-profiling-type=&lt;stats|trace|latency&gt;\n  Used in device level profiling. Refer to Profiler notebook under tutorials. \n</code></pre></p> <p><code>qaic-exec</code> can also be used to dump the operators supported across onnx, tensorflow, pytorch, caffe or caffe2 ML frameworks.</p> <pre><code>/opt/qti-aic/exec/qaic-exec -operators-supported=&lt;onnx | tensorflow | pytorch | caffe | caffe2&gt;\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune%20performance/","title":"Tune Performance","text":"<p>Throughput (inferences per second) and latency (time per inference) tuning techniques for Cloud AI platforms are discussed in this section. </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune%20performance/#key-performance-parameters","title":"Key Performance Parameters","text":"<p>Cores, Batch Size, Instances (a.k.a activations) and Set-size are the key performance parameters that need to be tuned to extract the best performance. </p> <ul> <li>Cores and Batch Size are compile parameters.</li> <li>Instances and Set-size are runtime parameters.</li> </ul>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune%20performance/#core","title":"Core","text":"<p>Cloud AI Platforms contain multiple AI cores depending on the SKU. Each AI core contains one or more scalar, vector and tensor engines which provide a rich instruction set to accelerate ML operations. A model can be compiled for one or more AI cores.   </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune%20performance/#instance-aka-activation","title":"Instance (a.k.a activation)","text":"<p>Instance refers to the compiled binary of a model executing inferences on a set of AI cores. Let's say bert-large was compiled for 2 AI cores and the Cloud AI device has 14 AI cores. Each instance will run on 2 AI cores. Up to 7 instances (on 14 AI cores) can be executed in parallel.  </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune%20performance/#batch-size","title":"Batch Size","text":"<p>Batch size refers to the number of input elements inferred by an instance. </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune%20performance/#set-size","title":"Set-size","text":"<p>Set-size denotes the number of inferences that can be queued up on the host per activation. Set-size helps hide host side overhead by pipelining inferences. Models that require large input/output data to be transferred (from/to host and device) or some pre-processing/post-processing on the host can see throughput increases with increasing set-size up to a certain value beyond which the device utilization cannot be improved. </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune%20performance/#instance-and-batch-size","title":"Instance and Batch size","text":"<p>The product of number of instances and batch size provides the total input samples that can be inferred in parallel on a single Cloud AI device. </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune%20performance/#cores-and-instances","title":"Cores and Instances","text":"<p>The product of 'number of instances' and 'number of AI cores used per instance' cannot exceed the total number of AI cores available on the Cloud AI platform/card.</p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune%20performance/#optimizing-for-best-throughput-least-latency","title":"Optimizing for Best Throughput / Least Latency","text":"<p>Model Configurator is a hardware-in-loop test tool that runs through various configurations and identifies the best configuration for a model. Model configurator tool offers two workflows - highest throughput and least latency. </p> <p>Refer to the Performance-Tuning Tutorial #1 - CV and Tutorial #2 - NLP for step by step walkthrough for performance tuning. </p> <p>Refer to Performance tuning tutorial for the workflow for optimizing for best throughput and least latency. </p> <p>For least latency configuration, batch-size should be set to 1. Set-size of 1 provides the least latency. </p> <p>Higher set-sizes may improve throughput significantly for a slight increase in latency for models that require some host side pre-processing (eg. CV models).  </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune%20performance/#general-performance-tuning-observations","title":"General Performance Tuning Observations","text":"<p>Now that we have covered the key performance tuning parameters , there are some general observations that would help developers compile models for best performance. </p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune%20performance/#throughput-and-latency-vs-batch-size","title":"Throughput and Latency vs Batch-size","text":"<p>For an instance on a fixed number of cores, increasing the batch size (BS) from 1 will typically improve throughput. Increasing beyond the optimal BS will cause the performance to drop. </p> <p></p>"},{"location":"Getting-Started/Inference-Workflow/model-compilation/Tune%20performance/#throughput-vs-cores-and-instances","title":"Throughput vs Cores and Instances","text":"<p>Based on the total number of AI cores available on the device, a few combinations of cores and instances exist and some of these combinations provide better performance that others. </p> <p>Lets use the Standard SKU of Cloud AI 100 with 14 AI cores to illustrate this. Throughput for different combinations of cores and instances are shown in the figure below. The Batch-size is fixed. For example: C1 x I12 represents the model compiled on 1 core and 12 instances deployed on 12 cores. </p> <p></p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Inference-Profiling/","title":"Inference Profiling","text":"<p>Cloud AI supports both system and device level profiling to aid developers to identify performance bottlenecks. System profiling can be performed without any changes to the QPC while device profiling requires the model to be recompiled. </p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Inference-Profiling/#system-level-profiling","title":"System Level Profiling","text":"<p>System level profiling includes the breakdown of the inference time between Application, linux runtime, kernel mode driver and device processing. This can provide insights into the inference time spent on the host vs device per inference. Developers can optimize their application or the model with this information. </p> <p>Refer to Profiler notebooks in the tutorials section for the complete system level profiling workflow for a model using the qaic-runner CLI. </p> <p>Profiling using Python APIs is also supported. Refer to this page.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Inference-Profiling/#device-level-profiling","title":"Device Level Profiling","text":"<p>Device level profiling is aimed at advanced developers looking to identify bottlenecks in inference execution on the device. This requires a good understanding of the AI core and SoC architecture. There are 3 key features that would be interesting to developers -</p> <ul> <li>Memory metrics - This provides a compiler estimate of the usage of on-board DDR vs VTCM (Vector tightly coupled memory) for a model.</li> <li>Summary view - This provides a histogram of the operations, total time taken by every operation, where the operands are stored (DDR vs VTCM), effective usage of the individual IP blocks in the AI cores etc. This feature is for debug only as it  may impact performance based on the size of the model. </li> <li>Timeline view - This provides a timeline view of all the operations executing across all IP blocks from start of an inference till the end. This feature is primarily used to zoom into the operations to understand bottlenecks. This feature is for debug only as it will impact performance. </li> </ul> <p>Refer to Profiler Jupyter notebook for the complete device level profiling workflow for a model using the qaic-exec and qaic-runner CLI. </p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/","title":"Triton Inference Server","text":"<p>Triton Inference Server is an open source inference serving software that streamlines AI inferencing. </p> <p>AIC100 SDK enables two backends for inference execution workflow.  These backends, once used on a host with AIC100 cards, will detect the AIC100 cards during initialization and route the inferencing call (requested to Triton server) to the hardware.</p> <ul> <li>onnxruntime_onnx as platform with QAic EP(execution provider)</li> <li>QAic as a customized C++ backend</li> </ul> <p> </p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#creating-a-aic100-backend-enabled-triton-docker-image-using-aic100-development-kits","title":"Creating a AIC100 backend enabled Triton docker image using AIC100 development kits","text":"<p>In order to add a customized backends (to process inferencing on AIC100 hardware) into a Vanilla Triton server image, we need to run few scripts by passing sdk_path as parameters. \"docker-build.sh\" script will generate a docker image as output.This script is a part of apps-sdk contents and can be run after unzipping it.</p> <p></p> <p><pre><code>sample&gt; cd &lt;/path/to/app-sdk&gt;/tools/docker-build\n\nsample&gt; ./build_image.sh --apps-sdk &lt;/path/to/apps-sdk.zip&gt; --platform-sdk &lt;/path/to/platform-sdk.zip&gt; --tag &lt;tag-name&gt; --os triton --create-model-repo\n</code></pre> The above command will take 15-20 minutes to complete and generate a Triton docker image in local docker repository.</p> <p><pre><code>sample&gt; docker image ls\nREPOSITORY                                         TAG                 IMAGE ID       CREATED         SIZE\nqaic-triton-x86_64-latest                          1.11.0.46-triton    cc217d09baea   6 days ago      34.4GB\n</code></pre> Docker can be launched using docker \"run\" command passing the desired image name. Please note that a shared memory argument(--shm-size) to pass for supporting ensembles and python backends.</p> <pre><code>sample&gt; docker run -it --rm --privileged --shm-size=4g --ipc=host --net=host &lt;triton-docker-image-name&gt; /bin/bash\n\nsample&gt; docker ps\nCONTAINER ID   IMAGE                                                                COMMAND                  CREATED      STATUS      PORTS     NAMES\nb88d5eb98187   qaic-triton-x86_64-latest                                            \"/opt/tritonserver/n\u2026\"   2 days ago   Up 2 days             thirsty_beaver\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#creating-a-model-repository-and-configuration-file","title":"Creating a model repository and configuration file","text":"<p>The model configuration file specifies the execution properties of a model. It indicates input/output structure, backend, batchsize, parameters, etc.  User needs to follow Triton's model repository and model configuration rules while defining a config file.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#model-configuration-onnxruntime","title":"Model configuration - onnxruntime","text":"<p>For onnxruntime configuration, platform should be set to \"onnxruntime_onnx\". The \"use_qaic\" parameter should be passed and set to true.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#aic100-specific-parameters","title":"AIC100 specific parameters","text":"<p>Parameters are user-provided key-value pairs which Triton will pass to backend runtime environment as variables and can be used in the backend processing logic.</p> <ul> <li>config : path for configuration file containing compiler options.</li> <li>device_id : id of AIC100 device on which inference is targeted. </li> <li>use_qaic : flag to indicate to use qaic execution provider.</li> <li>hare_session : flag to enable the use of single session of runtime object across model instances.</li> </ul> <p>sample example of a config.pbtxt</p> <pre><code>name: \"resnet_onnx\"\nplatform: \"onnxruntime_onnx\"\nmax_batch_size : 16\ndefault_model_filename : \"aic100/model.onnx\"\ninput [\n  {\n    name: \"data\"\n    data_type: TYPE_FP32\n    dims: [3, 224, 224 ]\n  }\n]\noutput [\n  {\n    name: \"resnetv18_dense0_fwd\"\n    data_type: TYPE_FP32\n    dims: [1000]\n  }\n]\nparameters [\n  {\n    key: \"config\"\n    value: { string_value: \"1/aic100/resnet.yaml\" }\n  },\n  {\n    key: \"device_id\"\n    value: { string_value: \"0\" }\n  },\n  {\n    key: \"use_qaic\"\n    value: { string_value: \"true\" }\n  },\n  {\n    key: \"share_session\"\n    value: { string_value: \"true\" }\n  }\n]\ninstance_group [\n  {\n    count: 2\n    kind: KIND_MODEL\n  }\n]\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#model-configuration-qaic-backend","title":"Model configuration - qaic backend","text":"<p>For qaic backend configuration, the \"backend\" parameter should be set to \"qaic\".</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#aic100-specific-parameters_1","title":"AIC100 specific parameters","text":"<p>Parameters are user-provided key-value pairs which Triton will pass to backend runtime environment as variables and can be used in processing logic of backend.</p> <p>Parameters are user-provided key-value pairs which Triton will pass to backend runtime environment as variables and can be used in processing logic of backend.</p> <ul> <li>qpc_path : path for compiled binary of model.(programqpc.bin)</li> <li>device_id : id of AIC100 device on which inference is targeted. device is set 0</li> <li>set_size : size of inference queue for runtime,default is set to 20</li> <li>no_of_activations : flag to enable multiple activations of a model\u2019s network,default is set to 1</li> </ul> <p>sample example of a config.pbtxt</p> <pre><code>name: \"yolov5m_qaic\"\nbackend: \"qaic\"\nmax_batch_size : 4\ndefault_model_filename : \"aic100/model.onnx\"\ninput [\n  {\n    name: \"images\"\n    data_type: TYPE_FP32\n    dims: [3, 640, 640 ]\n  }\n]\noutput [\n  {\n    name: \"feature_map_1\"\n    data_type: TYPE_FP32\n    dims: [3, 80, 80, 85]\n  },\n  {\n    name: \"feature_map_2\"\n    data_type: TYPE_FP32\n    dims: [3, 40, 40, 85]\n  },\n  {\n    name: \"feature_map_3\"\n    data_type: TYPE_FP32\n    dims: [3, 20, 20, 85]\n  }\n]\nparameters [\n  {\n    key: \"qpc_path\"\n    value: { string_value: \"/path/to/qpc\" }\n  },\n  {\n    key: \"device_id\"\n    value: { string_value: \"0\" }\n  }\n]\ninstance_group [\n  {\n    count: 2\n    kind: KIND_MODEL\n  }\n]\n</code></pre>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#launching-triton-server-inside-container","title":"Launching Triton server inside container","text":"<p>To launch Triton server, execute the tritonserver binary within Triton docker with the model repository path.</p> <pre><code>/opt/tritonserver/bin/tritonserver --model-repository=&lt;/path/to/repository&gt;\n</code></pre> <p></p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/Triton-Inference-Server-Support/#supported-features","title":"Supported Features","text":"<ul> <li>Model Ensemble</li> <li>Dynamic Batching</li> <li>Auto device-picker</li> <li>Support for ARM64</li> </ul>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/","title":"Model Execution","text":"<p>There are 4 ways to call inference on Qualcomm Cloud AI 100 device.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#1-cli-api","title":"1. CLI API","text":"<p><code>qaic-runner</code> is a CLI (command line inferface) tool designed to run inference on the device using precompiled binary (also called QPC (Qaic Program Container) binary). It provides various options and functionalities to facilitate inference and performance/benchmarking analysis. In this document, we will discuss the different options available in <code>qaic-runner</code> tool, their default values and provide examples for their usage.</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#options-and-default-values","title":"Options and Default values","text":"Parameter Description Default <code>-d, --aic-device-id &lt;id&gt;</code> Specify AIC device ID. 0 <code>-D, --dev-list &lt;qid&gt;[:&lt;qid&gt;]</code> Map of device IDs for a multi-device network. 0[:1] <code>-d, --aic-device-id &lt;id&gt;</code> AIC device ID, default Auto-pick 0 <code>-D, --aic-device-map &lt;qid&gt;[:&lt;qid&gt;]</code> Map of Device IDs for multi-device network, 0[:1] <code>-t, --test-data &lt;path&gt;</code> Location of program binaries <code>-i, --input-file &lt;path&gt;</code> Input filename from which to load input data. Specify multiple times for each input file.     If no -i is given, look for available inputs in bindings.json at the -t directory. If bindings.json is not available, random input will be generated. <code>-n, --num-iter &lt;num&gt;</code> Number of iterations, 40 <code>--time &lt;t&gt;</code> Duration (in seconds) for which to submit inferences <code>-l, --live-reporting</code> Enable Live reporting periodic at 1 sec interval off <code>-r, --live-reporting-period</code> Set Live Reporting Period in Ms 1000 <code>-s  --stats</code> Enable Live Profiling Stats reporting periodically at 1 sec interval <code>-a, --aic-num-of-activations &lt;num&gt;</code> Number of activations 1 <code>--aic-profiling-start-iter &lt;num&gt;</code> Profiling Start Iteration 0 <code>--aic-profiling-start-delay &lt;num&gt;</code> Profiling Start delay (in milliseconds). Profiling will start after given delay period has elapsed <code>--aic-profiling-num-samples &lt;num&gt;</code> Profiling Num Samples to save to file 1 <code>--aic-profiling-format &lt;level&gt;</code> Deprecated DEF <code>--aic-profiling-type &lt;type&gt;</code> Profiling Type, <code>'stats'\\|'trace'\\|'latency'</code> for legacy profiling and <code>'trace_stream' \\| 'latency_stream'</code> for stream profiling. Set multiple times for multiple formats none <code>--aic-profiling-duration &lt;num&gt;</code> Profiling duration to run profiling for (in ms). After starting profiling, it will stop at the expiry of profiling duration <code>--aic-profiling-sampling-rate &lt;num&gt;</code> Profiling sampling rate [<code>full/half/fourth/eighth/sixteenth</code>]. Programs will generate profiling samples at the requested rate.  To profile all samples select full, for every second sample select half and so on full <code>--aic-profiling-reporting-rate &lt;num&gt;</code> Profiling report generation rate (in ms) [<code>500/1000/2000/4000</code>]. Profiling report will be generated at every requested interval for profiling duration 500 <code>--aic-profiling-out-dir &lt;path&gt;</code> Location to save files, dir should exist and be writable '.' <code>--write-output-start-iter &lt;num&gt;</code> Write outputs start iteration 0 <code>--write-output-num-samples &lt;num&gt;</code> Number of outputs to write 1 <code>--write-output-dir &lt;path&gt;</code> Location to save output files, dir should exist and be writable '.' <code>--aic-lib-path DEPRECATED</code> Deprecated, please set env variable QAIC_LIB to the full path of the custom library, by default loads libQAic.so from install location <code>--aic-batch-input-directory</code> Batch mode: process all files from input directory. Only the networks with single input file are currently supported DEF <code>--aic-batch-input-file-list</code> Batch mode: Specify an input file containing comma-separated absolute path for buffers. Each line is 1 inference and must have number and size of buffers required by program DEF <code>--aic-batch-max-memory &lt;mb&gt;</code> Batch mode: Limit memory usage when loading files, provide parameter in Mb 1024 <code>--submit-timeout &lt;num&gt;</code> Time to wait for an inference request completion on kernel. default 0 ms. When 0, kernel defaults to 5000ms <code>--submit-retry-count &lt;num&gt;</code> Number of wait-call retries when an inference request times out. 5 <code>--unbound-random</code> When populating random values in buffer, do not consider input buffer format and fill each byte with random input between 0 to 255. This can result in unexpected behavior from certain network. <code>--dump-input-buffers</code> Dump input buffers used in benchmarking mode <code>-S, --set-size &lt;num&gt;</code> Set Size for inference loop execution, min:1 10 <code>-T, --aic-threads-per-queue</code> Number of threads per queue 4 <code>--auto-batch-input</code> Automatically batch inputs to meet batchsize requirements of network. Inputs should be for Batch size 1 1 <code>-p, --pre-post-processing</code> Pre-post processing [<code>on\\|off</code>] on <code>-v, --verbose</code> Verbose log from program <code>-h, --help</code> help"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#usage-examples","title":"Usage Examples","text":""},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#1-running-inference-with-random-inputs","title":"1. Running inference with Random inputs","text":"<p>Example: <pre><code>sudo /opt/qti-aic/exec/qaic-runner -t /path/to/qpc  -a 3 -n 5000 -d 0 -v\n</code></pre></p> <ul> <li>In this example we are using a precompiled binary that is already generated. <ul> <li>3 activations - activations here refers to number of instances of network you want to run on the device. In this case 3 copies of the network can run parallely on the device. </li> <li>Lets assume each network was compiled with 4 cores (<code>sudo /opt/qti-aic/tools/qaic-qpc validate -i /path/to/qpc/programqpc.bin</code> look for Number of NSP required value in output of this command.)<ul> <li>Make sure the device you are using has at least 12 i.e. 3x4 cores Free.</li> </ul> </li> <li>Since input is not provided we are feeding a randomly generated input (with appropriate dimensions and type inferred from qpc) to the device. </li> <li>This single randomly generated input is used for 5000 inferences.</li> <li>on device ID <code>0</code>. Check your device ID using <code>/opt/qti-aic/tools/qaic-util -q</code> CLI tool, look for QID value in it.</li> <li>verbose log is enabled with <code>-v</code> option.</li> </ul> </li> <li>Tool in this configuration can be used to measure performance. </li> </ul>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#2-running-inference-on-a-set-of-inputs","title":"2. Running inference on a set of inputs","text":"<p>Before running inference, it is necessary to convert the inputs to the appropriate format based on input size and type.  Look at this Jupyter notebook example</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#3-generating-dumps-for-latency-capture","title":"3. Generating dumps for latency capture","text":"<ul> <li><code>aic-profiling-format latency</code></li> <li><code>aic-profiling-out-dir</code> : output directory for latency capture (needs to exist before this command is run)</li> <li><code>aic-profiling-start-iter</code> : Set this value high enough to start capturing samples after device warmup </li> <li><code>aic-profiling-num-samples</code> : # of samples to be captured. Can be set greater than # of inferences</li> </ul> <p>Example command to generate latency stats<pre><code>!/opt/qti-aic/exec/qaic-runner -t ./BERT_LARGE -a 8 -S 1 -d 0 \\ #-i inputFiles/input.raw \\\n--aic-profiling-format latency --aic-profiling-out-dir ./BERT_LARGE_STATS \\\n--aic-profiling-start-iter 100 --aic-profiling-num-samples 99999 --time 20 </code></pre> Look at this Jupyter notebook example</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#conclusion","title":"Conclusion","text":"<p><code>qaic-runner</code> CLI tool is primarily intended for performance testing purposes. For actual inference tasks, it is recommended to utilize the Python or C++ APIs, depending on your preferred technology stack. </p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#2-use-python-apis","title":"2. Use Python APIs","text":"<p>Refer to Python API</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#3-use-c-apis","title":"3. Use C++ APIs","text":"<p>Refer to C++ API</p>"},{"location":"Getting-Started/Inference-Workflow/model-execution/model-execution-options/#4-use-onnxrt","title":"4. Use Onnxrt","text":"<p>Using Qualcomm Cloud AI 100 as execution provider in onnxrt.</p>"},{"location":"Getting-Started/Installation/","title":"Introduction","text":"<p>Developers can access Qualcomm Cloud AI accelerators on Cloud or by purchasing servers equipped with Qualcomm Cloud AI accelerators. </p>"},{"location":"Getting-Started/Installation/#cloud-instances","title":"Cloud Instances","text":"<p>Cloud AI 100 cards are now available at 2 Cloud service providers - Amazon Web Services (AWS) and Cirrascale Cloud Services. The AI 100 accelerator SKUs and instance configurations offered at these providers can vary. </p> <ul> <li>Refer to Getting Started on AWS for more information on the instances available at AWS.  </li> <li>Cirrascale Cloud Services have multiple configurations (from 1 to 8 Pro AI 100 accelerators per instance) for the user to choose from. </li> </ul> Note <p>Developers using cloud instances can skip the rest of the installation section. Click here to go to the next section, Inference Workflow</p>"},{"location":"Getting-Started/Installation/#on-premise-servers","title":"On-Premise Servers","text":"<p>Developers with on-premise servers need to work with system administators to ensure Cloud AI SDKs are installed and verified properly. It is recommended for developers and system admins to go through the installation section in its entirety. </p>"},{"location":"Getting-Started/Installation/#installation","title":"Installation","text":"<p>The Platform SDK (x86-64 and ARM64) and Apps SDK (x86-64 only) are targeted for Linux-based platforms. The SDKs can be installed natively on Linux operating systems. Container and orchestration are also supported through Docker and Kubernetes. Virtual machines, including KVM, ESXi, and Hyper-V, are also supported. This section covers:</p> <ul> <li>Installation of the SDKs across multiple Linux distributions</li> <li>Building a docker image with the SDKs and third-party packages for a seamless execution of QAic inference tools/workflow</li> <li>Setting up KVM, ESXi, and Hyper-V, and installation of SDKs</li> </ul>"},{"location":"Getting-Started/Installation/#compilation-and-execution-modes","title":"Compilation and Execution modes","text":"<p>Apps and Platform SDKs enable just-in-time(JIT) or ahead-of-time(AOT) compilation and execution on x86-64 platforms while only AOT compilation/execution is supported on ARM64. </p> <p>In JIT mode, compilation and execution are tightly coupled and require Apps and Platform SDKs to be installed on the same system/VM.</p> <p>In AOT mode, compilation and execution are decoupled. Networks can be compiled ahead-of-time on x86-64 (with Apps SDK only) and the compiled networks can be deployed  on x86-64 or ARM64 with Platform SDK.</p> <p>Both JIT and AOT are supported on x86-64 when Apps and Platform SDK are installed on the same server/VM. </p>"},{"location":"Getting-Started/Installation/#supported-operating-systems-hypervisors-and-platforms","title":"Supported Operating Systems, Hypervisors, and Platforms","text":"<p>The Cloud AI Platform SDK is compatible with the following operating systems (OS) and platforms.</p>"},{"location":"Getting-Started/Installation/#operating-systems","title":"Operating Systems","text":"Operating systems Kernel x86-64 ARM64 CentOS Linux 7 Linux Kernel 5.4.1 \u2714 \u2717 CentOS Linux 8 Linux Kernel 4.19 \u2717 \u2714 Ubuntu 18.04 Linux Kernel 5.4.1 / Default Kernel \u2714 \u2714 (Note1) Ubuntu 20.04 Default Kernel \u2714 \u2717 Ubuntu 22.04 Default Kernel \u2714 \u2717 Red Hat Enterprise Linux 7.9 Default Kernel \u2714 \u2717 Red Hat Enterprise Linux 8.3 Default Kernel \u2714 \u2717 Red Hat Enterprise Linux 8.4 Default Kernel \u2714 \u2717 Red Hat Enterprise Linux 8.6 Default Kernel \u2714 \u2717 Red Hat Enterprise Linux 8.7 Default Kernel \u2714 \u2717 Red Hat Enterprise Linux 9.0 Default Kernel \u2714 \u2717 Red Hat Enterprise Linux 9.1 Default Kernel \u2714 \u2717 <code>Note1</code>: Supported on certain ARM64-based Qualcomm platforms. <code>Note2</code>: Arm is a trademark of Arm Limited (or its subsidiaries) in the US and/or elsewhere. <code>Note3</code>: Apps SDK is available only for x86-64 platforms."},{"location":"Getting-Started/Installation/#hypervisors","title":"Hypervisors","text":"<p>Cloud AI only supports PCIe passthrough to a virtual machine. This means that the virtual machine completely owns the Cloud AI device. A single Cloud AI device cannot be shared between virtual machines or between a virtual machine and the native host. </p> Hypervisor x86-64 ARM64 KVM \u2714 \u2717 Hyper-V \u2714 \u2717 ESXi \u2714 \u2717 <code>Note</code> Cloud AI SDKs are not required to be installed on the hypervisor. All the Cloud AI software/SDKs are installed on the guest VMs."},{"location":"Getting-Started/Installation/AWS/aws/","title":"Getting started on AWS instances","text":"<p>Qualcomm Cloud AI 100 inference accelerator instances are now available on the AWS Cloud. </p>"},{"location":"Getting-Started/Installation/AWS/aws/#dl2q","title":"DL2q","text":"<p>DL2q is an accelerated computing instance powered by Cloud AI 100 Standard accelerator cards that can be used to cost-efficiently deploy deep learning (DL) inference workloads. </p> <p>Details regarding the DL2q instance can be found here. </p> <p>To get started quickly, customers can load one of many Deep Learning Amazon Machine Images (DLAMI) which come pre-packaged with Cloud AI Platform and Apps SDK. Search <code>Qualcomm AMI</code> for a list of AMIs that can be loaded on the DL2q instance. </p> <p>We recommend users to attach 200GB of EBS GP3 storage to the instance to start with.  </p> Note <p>Users of DL2q instance may need to request AWS to increase the vCPU limit to be able to spin up the first DL2q instance from an AWS account. </p> <p>SSH to the instance and add user to the <code>qaic</code> group to be able to run Cloud AI SDK tools without superuser (sudo) privileges.      <pre><code>sudo usermod -aG qaic $USER\n</code></pre> Close and re-open SSH session for the change to take effect. </p> <p>Users can clone the Cloud AI 100 GitHub repository containing model recipes, code samples and toolchain tutorials on the instance to get started. </p>"},{"location":"Getting-Started/Installation/Checklist/checklist/","title":"Installation Checklist","text":"<p>First, check for supported operating environments.</p>"},{"location":"Getting-Started/Installation/Checklist/checklist/#local-server-installation","title":"Local Server Installation","text":"<ol> <li> <p>Set up Linux server/workstation environment (Pre-requisites)</p> <ul> <li>Enable 32 message signaled interrupts (MSI) in UEFI/BIOS setup (MSI instructions)</li> </ul> </li> <li> <p>Install Platform and Apps SDKs (Cloud AI SDK)</p> </li> <li> <p>Tip: To run Platform SDK tools without superuser (sudo) privilege, add yourself to the qaic group. <pre><code>sudo usermod -aG qaic $USER\n</code></pre></p> <ul> <li>Log in again for the changes to take effect, or run \u2018newgrp qaic\u2019.</li> </ul> </li> </ol>"},{"location":"Getting-Started/Installation/Checklist/checklist/#verify-card-healthfunction","title":"Verify Card Health/Function","text":"<ol> <li> <p>Check PCIe enumeration:   <pre><code>$ lspci | grep Qualcomm\n01:00.0 Unassigned class [ff00]: Qualcomm Device a100\n</code></pre></p> <ul> <li>A PCIe enumeration failure may indicate an unsecure connection. Try re-inserting the card, or troubleshoot with a different PCIe card.</li> </ul> </li> <li> <p>Check device nodes:   <pre><code>ls /dev/mhi*\n/dev/mhi0_QAIC_DIAG  /dev/mhi0_QAIC_TIMESYNC /dev/mhi0_QAIC_QDSS /dev/mhi1_QAIC_DIAG\n</code></pre></p> <ul> <li>For every card, QAIC_DIAG, QAIC_TIMESYNC, QAIC_QDSS and QAIC_DIAG nodes are created. </li> <li>If mhi* folders do not exist, double-check the MSI settings in UEFI/BIOS setup</li> </ul> </li> <li> <p>Check card health and status with qaic-util <pre><code>sudo /opt/qti-aic/tools/qaic-util -q | grep -e Status -e QID\n\n    QID 0\n      Status:Ready\n    QID 1\n      Status:Ready\n    QID 2\n      Status:Ready\n    QID 3\n      Status:Ready\n</code></pre></p> <ul> <li>\u2018Status: Ready\u2019 indicates card(s) is in good health. Shown here are a system with 4 Cloud AI SoCs (Each card may have one or more SoCs based on the SKU).</li> <li>\u2018Status: Error\u2019 indicates the respective card(s) has not booted up completely. <ul> <li>Card could still be booting. Wait for a few mins and retry above command.</li> <li>Card error could be due to several reasons - unsupported OS/Platform, secure boot etc. </li> </ul> </li> </ul> </li> <li> <p>Run a test inference workload with qaic-runner CLI to sanity check HW/SW function.  Example:   <pre><code>sudo /opt/qti-aic/exec/qaic-runner -t /opt/qti-aic/test-data/aic100/v2/1nsp/1nsp-conv-hmx/ -a 14 -n 5000 -d 0\n\nInput file: /opt/qti-aic/test-data/aic100/v2/1nsp/1nsp-conv-hmx//user_idx0_in.bin\n---- Stats ----\nInferenceCnt 5000 TotalDuration 17598us BatchSize 1 Inf/Sec 284123.196\n</code></pre></p> <ul> <li><code>-d</code> specify QID to run the workload on.</li> <li><code>-a</code> edit value based on no of NSPs in specified QID.</li> </ul> <p>A positive Inf/Sec indicates HW/SW is functioning correctly. The Inf/Sec reported depends on <code>-a</code> specified in the command. </p> </li> </ol>"},{"location":"Getting-Started/Installation/Checklist/checklist/#virtual-machine-setup","title":"Virtual Machine Setup","text":"<ol> <li>Configure PCIe pass-through for the supported Hypervisor</li> <li>Configure message signaled interrupts in UEFI/BIOS setup and Hypervisor</li> <li>Install and configure Hypervisor</li> <li>Configure and launch Virtual Machine instance with one of the supported Operating Systems and Pre-requisites</li> <li>Install Cloud AI SDKs</li> <li>Check Card Health/Function</li> </ol>"},{"location":"Getting-Started/Installation/Checklist/checklist/#docker-setup","title":"Docker Setup","text":"<ol> <li>Install Platform SDK on host system or virtual machine (Cloud AI SDK)</li> <li>Build Docker image and start the container (Docker)</li> </ol>"},{"location":"Getting-Started/Installation/Cloud-AI-SDK/Cloud-AI-SDK/","title":"Installation - Cloud AI SDK","text":""},{"location":"Getting-Started/Installation/Cloud-AI-SDK/Cloud-AI-SDK/#platform-sdk","title":"Platform SDK","text":"<ul> <li>The Platform SDK is available at https://www.qualcomm.com/products/technology/processors/cloud-artificial-intelligence/cloud-ai-100#Software</li> <li>Log in with your Qualcomm ID to access the SDK. First time users need to register. </li> <li>The latest Platform SDKs are available under \"Software Packages\". <ul> <li>The Platform SDK is available in four different packages for the different combinations of x86-64/ARM64 and deb/rpm: <code>x86-deb, x86-rpm, aarch64-deb, and aarch64-rpm</code></li> </ul> </li> <li>On the host machine, log in as root or use <code>sudo</code> to have the right permissions to complete installation </li> <li>Copy the Platform SDK downloaded from the Qualcomm Portal to the host machine:<ul> <li>For networked x86-64 or ARM64 host:<ul> <li>Use scp, rsync, or samba to copy the Platform SDK zip file to the host machine</li> <li>Log in to the host machine (ssh or local terminal)</li> <li>Unzip the downloaded zip file to a working directory</li> <li><code>cd</code> to the working directory</li> </ul> </li> <li>For ARM64 hosts that support Google Android Debug Bridge (ADB):     <pre><code>- adb push &lt;Platform SDK zip file&gt; /data\n- adb shell\n- cd /data\n- Unzip the downloaded zip file to a working directory\n- cd to the working directory\n</code></pre></li> </ul> </li> </ul> Note <p>Extract the downloaded zip file until <code>qaic-platform-sdk-&lt;x86_64/aarch64&gt;-&lt;deb/rpm&gt;-&lt;SDK version&gt;.zip</code> is available. Confirm the architecture and installation linux package format works for your setup.  </p> <p>The Platform SDK is composed of the following tree structure. Users will see rpm or deb based on the SDK package:       <pre><code>\u251c\u2500\u2500 common\n\u2502   \u251c\u2500\u2500 sectools \n\u251c\u2500\u2500 &lt;architecture - x86_64 or aarch64&gt;  \n\u2502   \u251c\u2500\u2500 rpm \n\u2502   \u2502   \u251c\u2500\u2500 rpm \n\u2502   \u2502   \u2502   \u251c\u2500\u2500 qaic-fw-&lt;version&gt;.el7.x86_64.rpm \n\u2502   \u2502   \u2502   \u251c\u2500\u2500 qaic-kmd-&lt;version&gt;.el7.x86_64.rpm \n\u2502   \u2502   \u2502   \u2514\u2500\u2500 qaic-rt-&lt;version&gt;.el7.x86_64.rpm   \n\u2502   \u2502   \u251c\u2500\u2500 rpm-docker \n\u2502   \u2502   \u2502   \u2514\u2500\u2500 qaic-rt-docker-&lt;version&gt;.el7.x86_64.rpm  \n\u2502   \u2502   \u251c\u2500\u2500 install.sh \n\u2502   \u2502   \u251c\u2500\u2500 Notice.txt \n\u2502   \u2502   \u2514\u2500\u2500 uninstall.sh \n\u2502   \u251c\u2500\u2500 deb\n\u2502   \u2502   \u251c\u2500\u2500 deb \n\u2502   \u2502   \u2502   \u251c\u2500\u2500 qaic-fw_&lt;version&gt;.deb \n\u2502   \u2502   \u2502   \u251c\u2500\u2500 qaic-kmd_&lt;version&gt;-devel_amd64.deb \n\u2502   \u2502   \u2502   \u2514\u2500\u2500 qaic-rt_&lt;version&gt;_amd64.debm  \n\u2502   \u2502   \u251c\u2500\u2500 install.sh \n\u2502   \u2502   \u251c\u2500\u2500 Notice.txt \n\u2502   \u2502   \u2514\u2500\u2500 uninstall.sh \n\u2502   \u251c\u2500\u2500 test_suite   \n\u2502   \u2502   \u251c\u2500\u2500 pcietool   \n\u2514\u2500\u2500 \u2514\u2500\u2500 \u2514\u2500\u2500 powerstress    \n</code></pre></p> <p>Uninstall existing Platform SDK     <pre><code>sudo ./uninstall.sh\nsync\n</code></pre></p> <p>Run the install.sh script as root or with sudo to install with root permissions. Installation may take up to 30 mins depending on the number of Cloud AI cards in the server/VM. Cloud AI cards undergo resets several times during the installation.      <pre><code>cd &lt;architecture&gt;/&lt;deb/rpm&gt;\n# For Hybrid boot cards (PCIe CEM form factor cards), run \nsudo ./install.sh --auto_upgrade_sbl --ecc enable\n# For VM on ESXi hypervisor, run \nsudo ./install.sh --auto_upgrade_sbl --datapath_polling \u2013-ecc enable\n# For Flashless boot cards, run \nsudo ./install.sh \u2013-ecc enable\n# For VM on ESXi hypervisor, run \nsudo ./install.sh --datapath_polling \u2013-ecc enable\n</code></pre></p> <p>On successful installation of the platform SDK, the contents shown below are stored in /opt/qti-aic:     <pre><code>dev  drivers  examples  exec  firmware  services  test-data  tools\n</code></pre></p> <p>Check Platform SDK version using      <pre><code>cat /opt/qti-aic/versions/platform.xml\n</code></pre>   Add user to the qaic group to allow command-line tools to run without sudo:     <pre><code>sudo usermod -a -G qaic $USER\n</code></pre></p> Warning <p>Adding a user to the qaic group grants root-level privileges.  </p>"},{"location":"Getting-Started/Installation/Cloud-AI-SDK/Cloud-AI-SDK/#verify-card-operation","title":"Verify card operation","text":"<p>Refer to Verify Card Operation</p>"},{"location":"Getting-Started/Installation/Cloud-AI-SDK/Cloud-AI-SDK/#apps-sdk","title":"Apps SDK","text":"<p>The Apps SDK is only available for x86-64 Linux-based hosts. For ARM64-based Qualcomm platforms, models are first compiled on x86 with the Apps SDK. The compiled binary (QPC) is transferred to the ARM64 host for loading and execution by the Platform SDK on Cloud AI hardware.</p> <ul> <li>The Apps SDK is available at https://www.qualcomm.com/products/technology/processors/cloud-artificial-intelligence/cloud-ai-100#Software</li> <li>Log in with your Qualcomm ID to access the SDK. First time users need to register. </li> <li>The latest Apps SDK is available under \"Software Packages\". </li> <li>Download the Apps SDK. Unzip until you see the below folder structure. </li> </ul> <pre><code>  &lt;Apps-SDK&gt; \n  \u251c\u2500\u2500 dev  \n  \u2502   \u251c\u2500\u2500 hexagon_tools  \n  \u2502   \u251c\u2500\u2500 inc \n  \u2502   \u2514\u2500\u2500 lib \n  \u2502       \u2514\u2500\u2500 x86_64 \n  \u2502           \u2514\u2500\u2500 apps \n  \u251c\u2500\u2500 examples \n  \u2502   \u251c\u2500\u2500 apps \n  \u2502   \u2514\u2500\u2500 scripts \n  \u251c\u2500\u2500 exec \n  \u2502   \u2514\u2500\u2500 qaic-exec \n  \u251c\u2500\u2500 tools\n  \u2502   \u251c\u2500\u2500 docker-build\n  \u2502   \u251c\u2500\u2500 package-generator\n  \u2502   \u251c\u2500\u2500 qaic-pytools\n  \u2502   \u251c\u2500\u2500 qaic-version-util\n  \u2502   \u251c\u2500\u2500 rcnn-exporter\n  \u2502   \u2514\u2500\u2500 smart-nms\n  \u251c\u2500\u2500 install.sh  \n  \u251c\u2500\u2500 integrations  \n  \u2502   \u2514\u2500\u2500 qaic_onnxrt \n  \u2502   \u2514\u2500\u2500 triton \n  \u251c\u2500\u2500 Notice.txt  \n  \u251c\u2500\u2500 scripts \n  \u2502   \u2514\u2500\u2500 qaic-model-configurator\n  \u251c\u2500\u2500 versions \n  \u2502   \u2514\u2500\u2500 apps.xml  \n  \u2514\u2500\u2500 uninstall.sh\n</code></pre>"},{"location":"Getting-Started/Installation/Cloud-AI-SDK/Cloud-AI-SDK/#install-apps-sdk","title":"Install Apps SDK","text":"<ul> <li>Uninstall existing Apps SDK <code>sudo ./uninstall.sh</code></li> <li>Run the install.sh script as root or with sudo to install with root permissions. <code>sudo ./install.sh --enable-qaic-pytools</code></li> <li>On successful installation of the Apps SDK, the contents are stored to the /opt/qti-aic path under the dev and exec directories: <code>dev exec integrations scripts</code></li> <li>Check the Apps SDK version with the following command  <code>cat /opt/qti-aic/versions/apps.xml</code></li> <li> <p>Apply chmod commands </p> <pre><code>sudo chmod a+x /opt/qti-aic/dev/hexagon_tools/bin/*\nsudo chmod a+x /opt/qti-aic/exec/*\n</code></pre> </li> </ul>"},{"location":"Getting-Started/Installation/Cloud-AI-SDK/Cloud-AI-SDK/#activate-qaic-env","title":"Activate <code>qaic-env</code>","text":"<p><code>qaic-env</code> virtual environment is created during the installation of the APPS SDK. Developers are expected to activate <code>qaic-env</code> for seamless interaction with Cloud AI devices/SDKs.</p> <ul> <li> <p>For bash shell,    <pre><code>source /opt/qti-aic/dev/python/qaic-env/bin/activate\n</code></pre></p> </li> <li> <p>For csh shell,    <pre><code>source /opt/qti-aic/dev/python/qaic-env/bin/activate.csh\n</code></pre></p> </li> </ul> <p>To deactivate the virtual environment, type <code>deactivate</code> in any directory.    </p>"},{"location":"Getting-Started/Installation/Docker/Docker/","title":"Introduction","text":"<p>Docker is a product that allows users to build, test, and deploy applications through software containers. Docker for Cloud AI 100 packages the Platform SDK, Apps SDK (x86-64 only), libraries, system tools, etc., which enables the user to navigate the inference workflow seamlessly. </p> Note <p>Docker containers require the Cloud AI device drivers to communicate with the devices. Install the Platform SDK on the host bare metal OS or VM.</p> <p>The Docker scripts are in the Apps SDK in the <code>tools/docker-build</code> folder. The scripts to build a QAic Docker image are composed of the following structure. <pre><code>  \u251c\u2500\u2500 build_image.sh\n  \u251c\u2500\u2500 caffe\n  \u2502   \u251c\u2500\u2500 detection-output.patch\n  \u2502   \u251c\u2500\u2500 Dockerfile.ubuntu18\n  \u2502   \u251c\u2500\u2500 makefile.config.patch\n  \u2502   \u2514\u2500\u2500 mkldnn.patch\n  \u251c\u2500\u2500 config\n  \u2502   \u251c\u2500\u2500 aarch64\n  \u2502   \u2502   \u251c\u2500\u2500 Dockerfile.centos8\n  |   |   \u251c\u2500\u2500 Dockerfile.triton\n  \u2502   \u2502   \u2514\u2500\u2500 Dockerfile.ubuntu18\n  \u2502   \u251c\u2500\u2500 qaic_pytools\n  \u2502   \u2502   \u2514\u2500\u2500 pytools.dockerfile\n  \u2502   \u251c\u2500\u2500 qms_agent\n  \u2502   \u2502   \u2514\u2500\u2500 qms_agent.dockerfile\n  \u2502   \u2514\u2500\u2500 x86_64\n  \u2502   |   \u251c\u2500\u2500 Dockerfile.centos8\n  \u2502   |   \u251c\u2500\u2500 Dockerfile.triton\n  \u2502   |   \u251c\u2500\u2500 Dockerfile.ubuntu18\n  |   |   \u251c\u2500\u2500 Dockerfile.centos7\n  |   |   \u251c\u2500\u2500 Dockerfile.ubuntu20\n  |   |   \u2514\u2500\u2500 Dockerfile.ubuntu22\n  |   \u2514\u2500\u2500 qinf\n  |   |   \u2514\u2500\u2500 qinf.dockerfile\n  |   \u2514\u2500\u2500 aimet\n  |       \u2514\u2500\u2500 aimet.dockerfile\n  \u251c\u2500\u2500 README.md\n</code></pre></p>"},{"location":"Getting-Started/Installation/Docker/Docker/#build-the-docker-image","title":"Build the Docker image","text":"<p>Download Apps SDK and Platform SDK from Qualcomm site. Refer to Platform SDK Download and Apps SDK Download. Unzip Apps SDK and the build scripts are located under <code>/tools/docker-build/</code></p> <pre><code>unzip qaic-apps-1.10.0.193.zip\ncd qaic-apps-1.10.0.193/tools/docker-build/\n</code></pre> <p>QAic docker build script is used to create new docker image with Cloud AI 100 Apps SDK and Cloud AI 100 Platform SDK with the supported OS.</p> <p>The command to build the docker image is:</p> <pre><code>build_image.sh [ --mirror (Optional) &lt;docker-registry-mirror-location&gt; ] [ --apps-sdk (Optional) &lt;apps-sdk-zip-file-path&gt; ] [ --platform-sdk &lt;platform-sdk-zip-file-path&gt; ] [ --install-pkgs &lt;install-pkgs-zip-file-path&gt; ] [ --tag &lt;tag  for  the  created  image&gt; ][ --os &lt;centos8/centos7/ubuntu18&gt; ] [--arch (Optional) &lt;aarch64/x86_64&gt;] [--install-qaic-pytools] [--install-caffe] [--install-aimet] [--install-qms-agent] [--install-python-sdk]\n</code></pre> <p>Usage:  <pre><code>apps-sdk:   specifies path to the zip file of apps sdk (Optional argument)\nplatform-sdk:   specifies path to the zip file of platform sdk\ninstall-pkgs:   specifies path to the zip file of install-pkgs (Required for aarch64 based systems)\ntag:        specifies the tag name to build the image with. (Default: \u201clatest\u201d).\nos:     centos8/centos7/ubuntu18. (Default: centos8)\narch:       aarch64/x86_64. (Default: host arch)\ninstall-qaic-pytools: Option to install qaic-pytools\ninstall-caffe:  whether to install caffe along with pytools\n</code></pre></p> <p>For Example: <pre><code>./build_image.sh --apps-sdk ~/qaic-apps-1.10.0.193.zip --platform-sdk ~/qaic-platform-sdk-1.10.0.193.zip --tag \"1.10.0.193\" --os ubuntu20 --install-qaic-pytools\n</code></pre></p> <p>To check the docker image created with above script:  <pre><code>$ docker images\n   REPOSITORY      TAG      IMAGE ID        CREATED        SIZE \n   qaic-ubuntu20-x86_64&amp;nbsp 1.10.0.193  64a78fa17164  About a minute ago  13GB\n</code></pre></p>"},{"location":"Getting-Started/Installation/Docker/Docker/#create-the-container","title":"Create the container","text":"<p>Create the container using the docker image created above:</p> <pre><code>$ docker run -dit --name &lt;container name&gt; --device=&lt;AIC device that want to pass to the container&gt; &lt;docker image name&gt;:&lt;tag&gt;\n</code></pre> <p>For example:  <pre><code>$ docker run -dit --name qaic-ubuntu20-x86_64 --device=/dev/qaic_aic100_0 qaic-ubuntu20-x86_64:1.10.0.193\n</code></pre> To pass more than one Cloud AI device, use <code>--device</code> as many times as the number of devices to be passed to the container. </p> <p>For example:  <pre><code>$ docker run -dit --name qaic-ubuntu20-x86_64 --device=/dev/qaic_aic100_0 --device=/dev/qaic_aic100_1 qaic-ubuntu20-x86_64:1.10.0.193\n</code></pre></p> <p>Check container that got created. <pre><code>$ docker ps\n\nCONTAINER ID    IMAGE                            COMMAND      CREATED           STATUS            PORTS     NAMES    \nae6a9e34c519    qaic-ubuntu20-x86_64:1.10.0.193  \"/bin/bash\"  8 seconds ago     Up 6 seconds                qaic-ubuntu20-x86_64\n</code></pre></p>"},{"location":"Getting-Started/Installation/Docker/Docker/#execute-the-container","title":"Execute the container","text":"<p>Execute the container created in the above step using its container ID: </p> <p><pre><code>$ docker exec -it &lt;container ID&gt; /bin/bash\n</code></pre> For example:</p> <p><pre><code>$ docker exec -it ae6a9e34c519 /bin/bash\n</code></pre> This will bring the bash shell within the container.</p> <p>Query the Cloud AI devices from the container:  <pre><code>$ /opt/qti-aic/tools/qaic-util -q\n</code></pre></p> <p>This should show only one device \u201cQID 0\u201d in a ready state as we only passed one device to the container.</p>"},{"location":"Getting-Started/Installation/Docker/Docker/#build-docker-for-aarch64-from-x86_64","title":"Build Docker for aarch64 from x86_64","text":"<p>When passing an --arch option where the host os is not the same as the requested architecture,  Setup the host for docker multiarch using qemu.</p> <ul> <li>Ubuntu Host</li> </ul> <pre><code>  [user@localhost qaic-docker]$ sudo apt install qemu binfmt-support qemu-user-static\n  [user@localhost qaic-docker]$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n  // Check the setup\n  [user@localhost qaic-docker]$ docker run --rm -t --platform=linux/arm64 &lt;image name&gt; uname -m\n  The above should give the architecture as aarch64\n</code></pre> <ul> <li>CentOS Host</li> </ul> <pre><code>  [user@localhost qaic-docker]$ sudo apt install qemu qemu-kvm \n  [user@localhost qaic-docker]$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yes \n  // Check the setup \n  [user@localhost qaic-docker]$ docker run --rm -t --platform=linux/arm64 &lt;image name&gt; uname -m \n  The above should give the architecture as aarch64\n</code></pre>"},{"location":"Getting-Started/Installation/Hypervisors/hypervisor/","title":"Hypervisors","text":""},{"location":"Getting-Started/Installation/Hypervisors/hypervisor/#kvm","title":"KVM","text":"<p>Kernel-based Virtual Machine (KVM) is a module in the Linux kernel that allows the Linux OS to operate as a hypervisor. This enables a native install of Linux to act as the host for a virtual machine. In combination with QEMU and libVirt, a KVM virtual machine emulates a completely independent system that can be limited to a subset of the host hardware, and that can also run completely different operating systems from the host.</p> <p>An overview of KVM is at https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine.</p> <p>The benefit of a virtual machine is strong isolation. A virtual machine is isolated from other virtual machines and from the host. This isolation provides a high level of security because one application in a virtual machine may not be aware that it is in a virtual machine, much less that there may be other virtual machines with other applications. Also, a virtual machine provides separation from the host.</p> <p>Even if a driver in the virtual machine crashes the entire virtual machine, that crash will not take down the host and, therefore, will allow other services in other virtual machines to continue operating.</p> <p>The cost of these benefits is additional overhead to set up the system. Also, additional processing may be required at runtime between the virtual machine and the native hardware.</p> <p>AIC100 only supports PCIe passthrough to a virtual machine. This means that the virtual machine completely owns the AIC100 device. The AIC100 device cannot be shared between virtual machines, or between a virtual machine and the native host.</p> <p>The generic setup and operation of a KVM virtual machine is outside the scope of this document. This document assumes that the reader is familiar with those operations and, thus, only explains the elements directly related to assigning an AIC100 device to a KVM virtual machine.</p> <p>AIC100 supports only the operating systems listed here as the operating system running within the virtual machine as the guest OS.</p> <p>Within a virtual machine, AIC100 still requires the assignment of 32 message signal interrupts (MSI) to operate, which requires the virtual machine to emulate an IOMMU. During the creation of a virtual machine, the virtual machine must be configured to emulate a system that can emulate an IOMMU.</p> <p>One such system is the \u201cq35\u201d system. If using \u201cvirt-install\u201d, a q35 based virtual machine can be created by adding \u201c\u2014machine=q35\u201d to the virt-install command.</p> <p>The remainder of this section assumes that the virtual machine is created with virt-install and the \u2013machine=q35 option. Other systems may require different configurations than what is described to obtain the same end effect.</p> <p>After the virtual machine is created, it must be configured. This can be done with the \u201cvirsh edit\u201d command while the virtual machine is not running. See the virsh man page for more details.</p> <p>The virtual machine configuration must have the emulated IOMMU presented to the guest OS in the virtual machine, and the virtual machine must have the AIC100 device passed in.</p> <p>First, to present the IOMMU to the guest OS, add the following elements to the configuration XML:</p> <ol> <li>Configure the ioapic driver in split mode for interrupt remapping. This is done by adding \u201c\u201d under the \u201cfeatures\u201d section of the XML.</li> <li>Configure the emulated IOMMU to be present with the interrupt remapping functionality enabled. This is done by adding the following snippet under the \u201cfeatures\u201d section of the XML:   <pre><code>&lt;iommu model='intel'&gt;\n&lt;driver caching_mode='on' intremap='on'/&gt;\n&lt;/iommu&gt;\n</code></pre></li> </ol> <p>Second, configure what device to pass through to the virtual machine guest OS.</p> <ol> <li>Obtain the PCIe SBDF address of the AIC100 device using \u201clspci\u201d in the host.</li> <li>Add the device address obtained from Step 1 to the \u201cdevices\u201d section of the XML, as follows, but change the address tag values to match that of your specific system.</li> </ol> <p><pre><code>&lt;hostdev mode='subsystem' type='pci' managed='yes'&gt;\n&lt;source&gt;\n&lt;address domain='0x0' bus='0x0' slot='0x19' function='0x0'/&gt;\n&lt;/source&gt;\n&lt;/hostdev&gt;\n</code></pre> After making these edits, save the configuration. The next time the virtual machine is booted, you will observe the AIC100 device in lspci output in the virtual machine. It will have a different PCIE SBDF address than the host. Install the AIC100 Platform SDK and Apps SDK in the same manner as if you were operating a native system.</p>"},{"location":"Getting-Started/Installation/Hypervisors/hypervisor/#hyper-v","title":"Hyper-V","text":"<p>Hyper-V is a virtualization hypervisor commonly used with Microsoft Windows Server. This enables a native install of Windows Server to act as the host for a virtual machine. The virtual machine emulates a completely independent system that can be limited to a subset of the host hardware, and also can run completely different operating systems from the host.</p> <p>An overview of Hyper-V is at https://en.wikipedia.org/wiki/Hyper-V.</p> <p>The benefit of a virtual machine is strong isolation. A virtual machine is isolated from other virtual machines and also from the host. This isolation provides a high level of security because one application in a virtual machine may not be aware that it is in a virtual machine, much less that there may be other virtual machines with other applications. Also, a virtual machine provides separation from the host. Even if a driver in the virtual machine crashes the entire virtual machine, that crash will not take down the host and, therefore, will allow other services in other virtual machines to continue operating.</p> <p>The cost of these benefits is additional overhead to set up the system. Also, additional processing may be required at runtime between the virtual machine and the native hardware.</p> <p>AIC100 only supports PCIe passthrough to a virtual machine, which means that the virtual machine completely owns the AIC100 device. The AIC100 device cannot be shared between virtual machines, or between a virtual machine and the native host.</p> <p>The generic setup and operation of a Hyper-V virtual machine is outside the scope of this document. This document assumes that the reader is familiar with those operations and, thus, only explains the elements directly related to assigning an AIC100 device to a Hyper-V virtual machine.</p> <p>AIC100 supports only the operating systems listed in this table as the operating system running within the virtual machine as the guest OS.</p> <p>Within a virtual machine, AIC100 still requires the assignment of 32 message signal interrupts (MSI) to operate. Hyper-V does not support emulating an IOMMU in the guest OS of the virtual machine.</p> <p>However, Hyper-V supports a paravirtualized PCIe root controller that has a driver in the Linux kernel. This driver uses the Hyper-V hypervisor to configure the host IOMMU for the purposes of supplying MSIs for devices like AIC100.</p> <p>AIC100 requires the following fixes to the Hyper-V PCIe root controller driver within the Linux kernel to operate properly:</p> <ul> <li>https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v5.19-rc7&amp;id=08e61e861a0e47e5e1a3fb78406afd6b0cea6b6d</li> <li>https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v5.19-rc7&amp;id=455880dfe292a2bdd3b4ad6a107299fce610e64b</li> <li>https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v5.19-rc7&amp;id=b4b77778ecc5bfbd4e77de1b2fd5c1dd3c655f1f</li> <li>https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v5.19-rc7&amp;id=a2bad844a67b1c7740bda63e87453baf63c3a7f7</li> </ul> <p>Consult the provider of your Linux distribution to confirm that those fixes are present in the Linux kernel used for your Linux distribution. Once the Hyper-V virtual machine is created, the AIC100 device must be assigned to the virtual machine. Hyper-V calls this direct device assignment (DDA) and it is the same as PCIe passthrough. Assign the AIC100 device to the Hyper-V virtual machine as follows:</p> <ol> <li>Configure the virtual machine stop action.</li> <li>Disable the device in Windows.</li> <li>Unmount the device from Windows.       This step requires the \u201c-force\u201d option to be used. AIC100 currently does not have a partitioning driver. This may be provided in the future.</li> <li>Add the device to the virtual machine.</li> </ol> <p>Details on these steps can be found at the following DDA resources:</p> <ul> <li>https://docs.microsoft.com/en-us/windows-server/virtualization/hyper-v/deploy/deploying-graphics-devices-using-dda</li> <li>https://docs.microsoft.com/en-us/windows-server/virtualization/hyper-v/deploy/deploying-storage-devices-using-dda</li> </ul> <p>To identify the AIC100 device to disable and dismount, look for a device that has the following in the \u201cHardware Ids\u201d property: <pre><code>PCI\\VEN_17CB&amp;DEV_A100&amp;SUBSYS_A10017CB&amp;REV_00\n</code></pre> Once the AIC100 device is assigned to the virtual machine, it should appear in lspci output in the virtual machine the next time the virtual machine is booted. It will have a different PCIE SBDF address than the host. Install the AIC100 software in the same manner as if you were operating a native system.</p>"},{"location":"Getting-Started/Installation/Hypervisors/hypervisor/#esxi","title":"ESXi","text":"<p>ESXi is the virtualization hypervisor from VMWare. Refer to ESXi documentation for instructions on how to create a virtual machine (VM).</p> <p>Before powering-on the VM and installing the guest OS, a few setting changes are required to assign one or more AI 100 cards to the VM. To activate passthrough, follow these steps.</p> <ol> <li>Go to \u201cHost\u201d -&gt; \u2018Manage\u2019 tab on the left bar. Then click on the \u2018Hardware\u2019 tab.</li> <li>Search for \u201cQualcomm\u201d. All the installed AI 100 cards should show up.</li> <li>Check the cards and click \u201cToggle Passthrough\u201d. Each card should then list \u201cActive\u201d under the Passthrough attribute.</li> <li>Create a VM per instructions in the ESXi documentation. Under \"virtual hardware\", \"add other device\", and select \"pci device\". This will add a new entry for the PCI device. Verify that the correct AI 100 card is selected here. Repeat this process for every AI 100 card that should be assigned to the VM.</li> <li>Setup is now complete and the VM can be powered ON. It should automatically boot the guest OS ISO and start the installer. A preview of the console is shown in the virtual machine tab when the concerned VM is selected. The preview can be clicked to be expanded and used as an interface for the VM. Walk through the OS installer like any other system.</li> </ol>"},{"location":"Getting-Started/Installation/Hypervisors/hypervisor/#_1","title":"Hypervisors","text":""},{"location":"Getting-Started/Installation/Pre-requisites/pre-requisites/","title":"Pre-requisites","text":""},{"location":"Getting-Started/Installation/Pre-requisites/pre-requisites/#ubuntu-2004-2204-on-x86-64","title":"Ubuntu 20.04 / 22.04 on x86-64","text":"<ul> <li>Ubuntu 20.04 / 22.04 with default kernel installed, with minimum options.</li> <li> <p>Install the <code>dkms</code> package for the kernel.</p> <p><code>sudo apt-get install dkms</code></p> </li> <li> <p>Install the <code>linux-headers</code> package for the kernel.</p> </li> <li> <p>Install the following packages:</p> </li> </ul> Note <p>Python 3.8 64-bit is the only supported Python version by Cloud AI SDK.</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y software-properties-common sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y\nsudo apt-get update &amp;&amp; sudo apt-get install -y build-essential git vim libpci-dev libudev-dev python3-pip python3-setuptools python3-wheel python3.8 python3.8-dev  python3.8-venv\nsudo apt-get update &amp;&amp; sudo apt-get install -y unzip wget ca-certificates sudo pciutils libglib2.0-dev libssl-dev snap snapd libgl1-mesa-glx openssh- server pkg-config clang-format libpng-dev  sudo apt-get install libstdc++6\nsudo apt-get install libncurses5\nsudo pip3 install --upgrade pip\nsudo pip3 install wheel numpy opencv-python onnx\n</code></pre>"},{"location":"Getting-Started/Installation/Pre-requisites/pre-requisites/#addupdate-environment-variables","title":"Add/update environment variables:","text":"<pre><code>export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/opt/qti-aic/dev/lib/x86_64\"\nexport PATH=\"/usr/local/bin:$PATH\"\nexport PATH=\"$PATH:/opt/qti-aic/tools:/opt/qti-aic/exec:/opt/qti-aic/scripts\"\nexport QRAN_EXAMPLES=\"/opt/qti-aic/examples\"\nexport PYTHONPATH=\"$PYTHONPATH:/opt/qti-aic/dev/lib/x86_64\" export QAIC_APPS=\"/opt/qti-aic/examples/apps\"\nexport QAIC_LIB=\"/opt/qti-aic/dev/lib/x86_64/libQAic.so\"\nexport QAIC_COMPILER_LIB=\"/opt/qti-aic/dev/lib/x86_64/libQAicCompiler.so\"\\\n</code></pre>"},{"location":"Getting-Started/Installation/Pre-requisites/pre-requisites/#ubuntu-1804-on-x86-64","title":"Ubuntu 18.04 on x86-64","text":"<ul> <li> <p>Install the <code>dkms</code> package and the <code>linux-headers</code> package for the kernel.</p> <p><pre><code>sudo apt-get install dkms\n</code></pre> - The linux-headers package is already installed for the stock kernel, but for the 5.4.1 kernel, the headers package needs to be manually installed.</p> </li> <li> <p>Download kernel generic .deb files from here and install as follows:</p> <pre><code>sudo dpkg -i linux-headers-5.4.1-050401-generic_5.4.1-050401- generic-1_amd64.deb\nsudo dpkg -i linux-image-5.4.1-050401-generic_5.4.1-050401- generic-1_amd64.deb\n</code></pre> </li> <li> <p>Install the following packages:</p> </li> </ul> Note <p>Python 3.8 64-bit is the only supported Python version by Cloud AI SDK.</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y software-properties-common sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y\nsudo apt-get update &amp;&amp; sudo apt-get install -y build-essential git vim libpci-dev libudev-dev python3-pip python3-setuptools python3-wheel python3.8 python3.8-dev python3.8-venv\nsudo apt-get update &amp;&amp; sudo apt-get install -y unzip wget ca-certificates sudo pciutils libglib2.0-dev libssl-dev snap snapd libgl1-mesa-glx openssh- server pkg-config clang-format libpng-dev\nsudo apt-get install -y libstdc++6\nsudo apt-get install -y libncurses5\nsudo pip3 install --upgrade pip\nsudo pip3 install wheel numpy opencv-python onnx\n</code></pre>"},{"location":"Getting-Started/Installation/Pre-requisites/pre-requisites/#addupdate-environment-variables_1","title":"Add/update environment variables:","text":"<pre><code>export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/opt/qti-aic/dev/lib/x86_64\" export PATH=\"/usr/local/bin:$PATH\"\nexport PATH=\"$PATH:/opt/qti-aic/tools:/opt/qti-aic/exec:/opt/qti-aic/scripts\"\nexport QRAN_EXAMPLES=\"/opt/qti-aic/examples\"\nexport PYTHONPATH=\"$PYTHONPATH:/opt/qti-aic/dev/lib/x86_64\" export QAIC_APPS=\"/opt/qti-aic/examples/apps\"\nexport QAIC_LIB=\"/opt/qti-aic/dev/lib/x86_64/libQAic.so\"\nexport QAIC_COMPILER_LIB=\"/opt/qti-aic/dev/lib/x86_64/libQAicCompiler.so\"\n</code></pre>"},{"location":"Getting-Started/Installation/Pre-requisites/pre-requisites/#centos-7-rhel-on-x86-64","title":"Centos 7 / RHEL on x86-64","text":"<ul> <li>Centos 7 / RHEL with default kernel installed</li> <li>Run all commands as 'root'.</li> <li>Install the dkms package for the kernel.<ul> <li>Centos 7 \u2013 yum -y install dkms</li> <li>RHEL \u2013 Refer to this link on how to install EPEL packages</li> </ul> </li> <li>Install the appropriate linux-headers package for the kernel.</li> <li>Install the following packages:</li> </ul> Note <p>Python 3.8 64-bit is the only supported Python version by Cloud AI SDK.</p> <pre><code>yum update \u2013y\n[Optional] update-ca-trust force-enable\nyum -y install gcc-toolset-9-gcc gcc-toolset-9-gcc-c++ gcc-toolset-9- make gcc-toolset-9-binutils automake\nsource /opt/rh/gcc-toolset-9/enable\nyum install -y git vim cmake pciutils-devel rpm-build systemd-devel libudev-devel python38 python3-pip python3-setuptools python3-wheel python3-devel\nyum install -y unzip wget ca-certificates sudo pciutils libnsl ncurses-compat-libs libasan glib2-devel ncurses-compat-libs mesa- libGL libpng-devel\npip3 install --upgrade pip\npip3 install numpy opencv-python onnx wheel\n</code></pre>"},{"location":"Getting-Started/Installation/Pre-requisites/pre-requisites/#addupdate-environment-variables_2","title":"Add/update environment variables:","text":"<pre><code>export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/opt/qti-aic/dev/lib/x86_64\" export PATH=\"/usr/local/bin:$PATH\"\nexport PATH=\"$PATH:/opt/qti-aic/tools:/opt/qti-aic/exec:/opt/qti-aic/scripts\"\nexport QRAN_EXAMPLES=\"/opt/qti-aic/examples\"\nexport PYTHONPATH=\"$PYTHONPATH:/opt/qti-aic/dev/lib/x86_64\" export QAIC_APPS=\"/opt/qti-aic/examples/apps\"\nexport QAIC_LIB=\"/opt/qti-aic/dev/lib/x86_64/libQAic.so\"\nexport QAIC_COMPILER_LIB=\"/opt/qti-aic/dev/lib/x86_64/libQAicCompiler.so\"\n</code></pre>"},{"location":"Getting-Started/Installation/Pre-requisites/pre-requisites/#message-signaled-interrupts","title":"Message Signaled Interrupts","text":"<p>The QAic Kernel Driver for Linux requires 32 message signaled interrupts (MSI) for best performance. QAic kernel driver does support single MSI configuration but is not recommended. On x86-based host systems, Intel VT-d or IOMMU features must be enabled in the BIOS to enable the required number of MSIs.</p> <p>For host systems using Intel chipsets, ensure that Intel Virtualization (VT-d) is enabled in the BIOS. For host systems using AMD chipsets, ensure that the IOMMU feature is enabled in the BIOS.</p>"},{"location":"Getting-Started/Quick-Start-Guide/","title":"Quick Start Guide","text":"<p>This section illustrates the ease of running inference on Cloud AI platforms using a vision transformers model for classifying images. </p>"},{"location":"Getting-Started/Quick-Start-Guide/#device-status","title":"Device Status","text":"<p>Follow the checklist to make sure device is ready for use.</p>"},{"location":"Getting-Started/Quick-Start-Guide/#steps-to-run-a-sample-model-on-qualcomm-cloud-ai-platforms","title":"Steps to run a sample model on Qualcomm Cloud AI Platforms","text":""},{"location":"Getting-Started/Quick-Start-Guide/#1-import-libraries","title":"1. Import libraries","text":"<pre><code>import os, sys, requests, torch, numpy, PIL\nfrom transformers import ViTForImageClassification, ViTImageProcessor\nsys.path.append('/opt/qti-aic/examples/apps/qaic-python-sdk/qaic')\nimport qaic\n</code></pre>"},{"location":"Getting-Started/Quick-Start-Guide/#2-pick-a-model-from-hf","title":"2. Pick a model from HF","text":"Choose the Vision Transformers model for classifying images and its image input preprocessor<pre><code>model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n</code></pre>"},{"location":"Getting-Started/Quick-Start-Guide/#3-convert-to-onnx","title":"3. Convert to ONNX","text":"<pre><code>dummy_input = torch.randn(1, 3, 224, 224)       # Batch, channels, height, width\ntorch.onnx.export(model,                        # PyTorch model\ndummy_input,               # Input tensor\n'model.onnx',              # Output file\nexport_params = True,      # Export the model parameters\ninput_names   = ['input'], # Input tensor names\noutput_names  = ['output'] # Output tensor names\n)\n</code></pre>"},{"location":"Getting-Started/Quick-Start-Guide/#4-compile-the-model","title":"4. Compile the model","text":"<p>Compile the model with the <code>qaic-exec</code> CLI tool. You can find more details about its usage here. This quickstart issues the command from Python.</p> <pre><code>aic_binary_dir = 'aic-binary-dir'\ncmd = '/opt/qti-aic/exec/qaic-exec -aic-hw -aic-hw-version=2.0 -compile-only -convert-to-fp16 \\\n-aic-num-cores=4 -m=model.onnx -onnx-define-symbol=batch_size,4 -aic-binary-dir=' + aic_binary_dir\nos.system(cmd)\n</code></pre>"},{"location":"Getting-Started/Quick-Start-Guide/#5-get-example-input","title":"5. Get example input","text":"<pre><code>url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = PIL.Image.open(requests.get(url, stream=True).raw)\n</code></pre>"},{"location":"Getting-Started/Quick-Start-Guide/#6-run-the-model","title":"6. Run the model","text":"Create the AIC100 session and prepare inputs and outputs<pre><code>vit_sess = qaic.Session(model_path= aic_binary_dir+'/programqpc.bin',\\\n   num_activations=3) # (1)\ninputs = processor(images=image, return_tensors='pt')\ninput_shape, input_type = vit_sess.model_input_shape_dict['input']\ninput_data = inputs['pixel_values'].numpy().astype(input_type) \ninput_dict = {'input': input_data}\noutput_shape, output_type = vit_sess.model_output_shape_dict['output']\n</code></pre> <ol> <li>Make sure qpcPath matches where the output is generated in <code>qaic-exec</code> above. We are activating 3 instances on network. More options.</li> </ol> Run model on AIC100<pre><code>vit_sess.setup() # Load the model to the device.\noutput = vit_sess.run(input_dict) # Execute on AIC100 now.\n</code></pre> Obtain the prediction by finding the highest probability among all classes<pre><code>logits = numpy.frombuffer(output['output'], dtype=output_type).reshape(output_shape)\npredicted_class_idx = logits.argmax(-1).item()\nprint('Predicted class:', model.config.id2label[predicted_class_idx]) \n</code></pre>"},{"location":"Getting-Started/Quick-Start-Guide/#next-steps","title":"Next Steps","text":"<p>We showed the ease of onboarding and running inference on Cloud AI platforms in this section. Refer to the User Guide for details on SDK installation, inference workflow, system management etc. </p>"},{"location":"Getting-Started/Quick-Start-Guide/#appendix","title":"Appendix","text":"<p>Input image link</p>"},{"location":"Getting-Started/Quick-Start-Guide/#full-quickstart-code","title":"Full quickstart code","text":"quickstart.py <pre><code># Copyright (c) 2023 Qualcomm Innovation Center, Inc. All rights reserved.\n# SPDX-License-Identifier: BSD-3-Clause-Clear\n# Import relevant libraries\nimport os, sys, requests, torch, numpy, PIL\nfrom transformers import ViTForImageClassification, ViTImageProcessor\nsys.path.append('/opt/qti-aic/examples/apps/qaic-python-sdk/qaic')\nimport qaic\n# Choose the Vision Transformers model for classifying images and its image input preprocessor\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n# Convert to ONNX\ndummy_input = torch.randn(1, 3, 224, 224)     # Batch, channels, height, width\ntorch.onnx.export(model,                      # PyTorch model\ndummy_input,              # Input tensor\n'model.onnx',             # Output file\nexport_params=True,       # Export the model parameters\ninput_names  =['input'],  # Input tensor names\noutput_names =['output']  # Output tensor names\n)\n# Compile the model \naic_binary_dir = 'aic-binary-dir'\ncmd = '/opt/qti-aic/exec/qaic-exec -aic-hw -aic-hw-version=2.0 -compile-only -convert-to-fp16 \\\n-aic-num-cores=4 -m=model.onnx -onnx-define-symbol=batch_size,4 -aic-binary-dir=' + aic_binary_dir\nos.system(cmd)\n# Get example Egyptian cat image for classification\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = PIL.Image.open(requests.get(url, stream=True).raw)\n# Run the model\n## Create the AIC100 session and prepare inputs and outputs\nvit_sess = qaic.Session(model_path= aic_binary_dir+'/programqpc.bin',\\\n   num_activations=3)\ninputs = processor(images=image, return_tensors='pt')\ninput_shape, input_type = vit_sess.model_input_shape_dict['input']\ninput_data = inputs['pixel_values'].numpy().astype(input_type) \ninput_dict = {'input': input_data}\noutput_shape, output_type = vit_sess.model_output_shape_dict['output']\n## Access the hardware\nvit_sess.setup() # Load the model to the device.\noutput = vit_sess.run(input_dict) # Execute on AIC100 now.\n## Obtain the prediction by finding the highest probability among all classes.\nlogits = numpy.frombuffer(output['output'], dtype=output_type).reshape(output_shape)\npredicted_class_idx = logits.argmax(-1).item()\nprint('Predicted class:', model.config.id2label[predicted_class_idx]) \n</code></pre>"},{"location":"Getting-Started/Quick-Start-Guide/#output","title":"Output","text":"Output <pre><code>sudo python quickstart.py                  \n/usr/local/lib/python3.8/dist-packages/transformers/models/vit/modeling_vit.py:170: TracerWarning: Converting a  tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python  values, so this value will be treated as a constant in the future. This means that the trace might not  generalize to other inputs!\nif num_channels != self.num_channels:\n/usr/local/lib/python3.8/dist-packages/transformers/models/vit/modeling_vit.py:176: TracerWarning: Converting a  tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python  values, so this value will be treated as a constant in the future. This means that the trace might not  generalize to other inputs!\nif height != self.image_size[0] or width != self.image_size[1]:\n============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\nverbose: False, log level: Level.ERROR\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n\nReading ONNX Model from model.onnx\nCompile started ............... \nCompiling model with FP16 precision.\nGenerated binary is present at aic-binary-dir\nPredicted class: Egyptian cat\n</code></pre>"},{"location":"Getting-Started/System-Management/system-management/","title":"System Management","text":"<p><code>qaic-util</code> command line utility enables developers to query </p> <ul> <li>card and SoC(s) health</li> <li>firmware version</li> <li>compute/memory/IO resources available vs in-use</li> <li>card power and temperature </li> <li>status of certain device capabilities like ECC etc</li> </ul> <p>Cloud AI Platform SDK installation is required for <code>qaic-util</code> usage. </p> <p><code>QID</code> a.k.a <code>deviceID</code> are indentifiers (integers) assigned to each AI 100 SoC present in the system. Note that certain SKUs may contain more than one AI 100 SoC per Card. </p> <p><code>qaic-util</code> displays information in 2 formats:</p> <ul> <li>vertical format where cards/SoCs are queried once and the parameters are listed one per line.  <pre><code>sudo /opt/qti-aic/tools/qaic-util -q \nsudo /opt/qti-aic/tools/qaic-util -q  -d &lt;QID#&gt; #To display information for a specific `QID`\n</code></pre></li> <li>tabular format where certain parameters (compute, IO, power, temperature etc) are listed in a tabular format, refreshed every 'n' seconds (user input) <pre><code>sudo /opt/qti-aic/tools/qaic-util -q -t 1 \n</code></pre></li> </ul> <p><code>-d</code> flag can be used to display information for a specific <code>QID</code></p> <p>Developers can <code>grep</code> for keywords like <code>Status</code>, <code>Capabilities</code>, <code>Nsp</code>, <code>temperature</code>, <code>power</code> to get specific information from the cards/SoCs.</p>"},{"location":"Getting-Started/System-Management/system-management/#health","title":"Health","text":"<p><code>Status</code> field indicates the health of the card. </p> <ul> <li><code>Ready</code> indicates card is in good health.</li> <li><code>Error</code> indicates card is in error condition or user lacks permissions (use <code>sudo</code>). </li> </ul> <pre><code>sudo /opt/qti-aic/tools/qaic-util -q | grep -e Status -e QID\nQID 0\n        Status:Ready\nQID 1\n        Status:Ready\nQID 2\n        Status:Ready\nQID 3\n        Status:Ready\n</code></pre> <p>Verify the function steps can be used to run a sample workload on <code>QIDs</code> to ensure HW/SW is funtioning correctly. </p>"},{"location":"Getting-Started/System-Management/system-management/#soc-reset","title":"SoC Reset","text":"<p>Developers can reset the <code>QIDs</code> using <code>soc_reset</code> sysfs node to recover the SoCs if they are in <code>Error</code> condition. These are the steps to issue a <code>soc_reset</code>. </p> <ol> <li> <p>Identify the <code>MHI ID</code> associated with the <code>QID</code></p> <p><pre><code>sudo /opt/qti-aic/tools/qaic-util -q | grep -e MHI -e QID\n</code></pre> In the sample output below, <code>MHI ID:0</code> is associated with <code>QID 0</code> and so on. </p> Note <p>MHI and QID do not always map to the same integer. It is imperative for developers to identify the mapping first before issuing the <code>soc_reset</code></p> <pre><code>sudo /opt/qti-aic/tools/qaic-util -q | grep -e MHI -e QID\nQID 0\n        MHI ID:0\nQID 1\n        MHI ID:1\nQID 2\n        MHI ID:2\nQID 3\n        MHI ID:3\n</code></pre> </li> <li> <p>Issue <code>soc_reset</code> to the <code>MHI ID</code> identified in step 1. </p> <pre><code>sudo su \necho 1 &gt; /sys/bus/mhi/devices/mhi&lt;MHI ID&gt;/soc_reset  #MHI ID is 0,1,2...  \n</code></pre> <p>Verify the health/function of the SoCs/Cards after a <code>soc_reset</code>. </p> </li> </ol>"},{"location":"Getting-Started/System-Management/system-management/#advanced-system-management","title":"Advanced System Management","text":"<p>For advanced system management details, refer to Cloud AI Card Management </p> <p>This document is shared with System Integrators and covers the following topics. </p> <ul> <li>Boot and firmware management</li> <li>Security - Secure boot enablement and attestation </li> <li>BMC integration</li> <li>Platform validation tools </li> <li>Platform error management </li> </ul>"},{"location":"Getting-Started/System-Management/system-management/#python-apis","title":"Python APIs","text":"<p>Python APIs also provide the abilty to monitor the health and resources of the cards/SoCs. Refer to Util class</p>"},{"location":"ONNXRT%20QAIC/onnxruntime/","title":"QAIC execution provider","text":"<p>The QAIC Execution Provider for ONNX Runtime enables hardware accelerated execution on Qualcomm AIC100 chipset. It leverages AIC compiler and runtime API packaged in apps, platform SDKs.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime/#installation-pre-requisites","title":"Installation Pre-requisites","text":""},{"location":"ONNXRT%20QAIC/onnxruntime/#system-requirements","title":"System requirements","text":"<ul> <li>OS: Ubuntu 18.04+</li> <li>Packages: gcc/g++ 9, git, cmake 3.18+, python 3.6+</li> <li>Python requirements: numpy, wheel, flatbuffers-2.0 yaml</li> </ul>"},{"location":"ONNXRT%20QAIC/onnxruntime/#download-and-install","title":"Download and Install","text":"<ul> <li>Apps SDK</li> <li>Platform SDK (optional if intending to execute on simulator)</li> </ul>"},{"location":"ONNXRT%20QAIC/onnxruntime/#build","title":"Build","text":"<p>Set the environment</p> <pre><code> export QAIC_APPS=\"/opt/qti-aic/examples/apps\"\nexport QAIC_LIB=\"/opt/qti-aic/dev/lib/x86_64/libQAic.so\"\nexport QAIC_COMPILER_LIB=\"/opt/qti-aic/dev/lib/x86_64/libQAicCompiler.so\"\n</code></pre> <p><code>build_onnxrt_qaic.sh</code> is a wrapper script for onnxruntime's <code>build.sh</code>. </p> <p>It clones version <code>1.10.0</code> of onnxruntime into the folder onnxruntime_qaic, applies a patch comprising changes to support QAIC EP and builds it. Run the script with default options as follows.</p> <pre><code>bash /opt/qti-aic/integrations/qaic-onnxrt/build_onnxrt_qaic.sh\n</code></pre>"},{"location":"ONNXRT%20QAIC/onnxruntime/#configuration-options","title":"Configuration Options","text":"option type description config str [Required] Path to the model-settings YAML file. Contains AIC configuration parameters used by QAic execution provider of ONNX Runtime. The configuration for best performance and accuracy can be generated using model configurator tool. aic_device_id int [Optional] AIC device ID, auto-picked when not configured"},{"location":"ONNXRT%20QAIC/onnxruntime/#parameters-supported-in-model-settings-yaml-file","title":"Parameters supported in model settings yaml file","text":"Option Description Default Relevance Runtime parameters aic-binary-dir Absolute path or relative path ( wrt model settings file parent directory) to dir with programqpc.bin \"\" Required to skip compilation. device-id AIC device ID 0 Optional set-size Set Size for inference loop execution 10 Optional aic-num-of-activations Number of activations 1 Optional qaicRegisterCustomOp - Compiler C API register-custom-op Register custom op using this configuration file Required if model has AIC custom ops; vector of string Graph Config - Compiler API aic-depth-first-mem Sets DFS memory size Set by compiler Optional. Used in compilation with aic-enable-depth-first aic-enable-depth-first Enables DFS with default memory size; \"True\", \"False\" Set by compiler Optional. Used in compilation. aic-num-cores Number of aic cores to be used for inference on 1 Optional. Used in compilation. allocator-dealloc-delay Option to increase buffer lifetime 0 - 10, e.g 1 Set by compiler Optional. Used in compilation. batchsize Sets the number of batches to be used for execution 1 Optional. Used in compilation. convert-to-fp16 Run all floating-point in fp16; \"True\", \"False\" \"False\" Optional. Used in compilation. enable-channelwise Enable channelwise quantization of Convolution op; \"True\", \"False\" Set by compiler Optional. Used in compilation with pgq-profile. enable-rowwise Enable rowwise quantization of FullyConnected and SparseLengthsSum ops; \"True\", \"False\" Set by compiler Optional. Used in compilation with pgq-profile. execute-nodes-in-fp16 Run all insances of the operators in this list with FP16; \"True\", \"False\" Set by compiler Optional. Used in compilation with pgq-profile for mixed precision. hwVersion HW version of AI QAIC_HW_V2_0 Cannot be configured, set to QAIC_HW_V2_0. keep-original-precision-for-nodes Run operators in this list with original precision at generation Optional. Used in compilation with pgq-profile for mixed precision. mos Effort level to reduce the on-chip memory; eg: \"1\" Set by compiler Optional. Used in compilation. multicast-weights Reduce DDR bandwidth by loading weights used on multiple-cores only once and multicasting to other cores ols Factor to increasing splitting of network for parallelism Set by compiler Optional. Used in compilation. quantization-calibration Specify quantization calibration -\"None\", \"KLMinimization\", \"Percentile\", \"MSE\", \"SQNR\", \"KLMinimizationV2\" \"None\" Optional. Used in compilation with pgq-profile. quantization-schema-activations Specify quantization schema - \"asymmetric\", \"symmetric\", \"symmetric_with_uint8\", \"symmetric_with_power2_scale\" \"symmetric_with_uint8\" Optional. Used in compilation with pgq-profile. quantization-schema-constants Specify quantization schema -\"asymmetric\", \"symmetric\", \"symmetric_with_uint8\", \"symmetric_with_power2_scale\" \"symmetric_with_uint8\" Optional. Used in compilation with pgq-profile. size-split-granularity To set max tile size, KiB between 512 - 2048, e.g 1024 Set by compiler Optional. Used in compilation. aic-hw To set the target to QAIC_SIM or QAIC_HW; \"True\", \"False\" \"True\" Optional. Model Params - Compiler API model-path Path to model file Required. Used in compilation, OnnxRT framework. onnx-define-symbol Define an onnx symbol with its value. pairs of onnx symbol key,value separated by space. Required. Used in compilation, OnnxRT framework. external-quantization Path to load the externally generated quantization profile Optional node-precision-info Path to load model loader precision file for setting node instances to FP16 or FP32 Optional. Used in compilation with pgq-profile for mixed precision. Common relative-path aic-binary-dir absolute path will be constructed using base-path of model-settings file; \"True\", \"False\" \"False\" Optional. Set to true, to allow relative-path for aic-binary-dir."},{"location":"ONNXRT%20QAIC/onnxruntime/#usage","title":"Usage","text":""},{"location":"ONNXRT%20QAIC/onnxruntime/#python","title":"Python","text":"<p>Here are few basic commands you can use with ONNX Runtime and QAIC.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime/#load-a-model","title":"Load a model","text":"<pre><code>import onnxruntime as ort\nprovider_options = []  \nqaic_provider_options = {} \nqaic_provider_options['config'] = '/path/to/yaml/file' \nqaic_provider_options['device_id'] = aic_device_id \nprovider_options.append(qaic_provider_options) \nsession=onnxruntime.InferenceSession('/path/to/onnx/model', sess_options,                                                \nproviders = providers, provider_options = providers_options)\n</code></pre> <p>This will bind your model to AIC100 chip, with qaic exectuion provider.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime/#perform-inference","title":"Perform Inference","text":"<pre><code># Perform inference using OnnxRuntime\nresults = sess.run(None, {'input_name': input_data})\n</code></pre> <p>In the above code replace <code>'input_name'</code> with name of model input node and input_data with the actual input data.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime/#c","title":"C++","text":""},{"location":"ONNXRT%20QAIC/onnxruntime/#load-a-model_1","title":"Load a Model","text":"<pre><code>#include &lt;onnxruntime_cxx_api.h&gt;\n#include &lt;qaic_provider_factory.h&gt;\n// Set environment as required\nOrt::Env env(ORT_LOGGING_LEVEL_ERROR, \"test\");\n// Initialize session options, create session\nOrt::SessionOptions session_options;\nsession_options.SetIntraOpNumThreads(1);\nsession_options.SetGraphOptimizationLevel(\nGraphOptimizationLevel::0);\nauto s = OrtSessionOptionsAppendExecutionProvider_QAic(\nsession_options, \"/path/to/yaml/file\", aic_device_id);\nOrt::Session session(env, \"/path/to/onnx/model\", session_options);\n</code></pre>"},{"location":"ONNXRT%20QAIC/onnxruntime/#perform-inference_1","title":"Perform inference","text":"<pre><code>// Run the model\nauto output_tensors = session.Run(Ort::RunOptions{nullptr},\ninput_names.data(), &amp;input_tensor, 1, output_names.data(), 1);\n</code></pre> <p>In the above code, replace <code>\"/path/to/onnx/model/\"</code> to the path for your onnx file. Also ensure data and shape of your input tensor match the requirements of your model.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime/#end-to-end-examples","title":"End-to-end examples","text":"<p>End to end examples (cpp and python) for resnet50 are available at - <code>/opt/qti-aic/integrations/qaic_onnxrt/tests/</code>.</p>"},{"location":"ONNXRT%20QAIC/onnxruntime/#running-the-resnet-c-sample","title":"Running the ResNet C++ sample","text":"<p>Compile the Sample Resnet C++ test using build_tests.sh script. By default, test is built using libs from onnxruntime_qaic release build. To enable debugging, re-build onnxruntime_qaic project in Debug configuration and run ./build_test.sh with debug flag.  </p> <pre><code>build_tests.sh [--release|--debug]    </code></pre> <p>Run the executable. The commands below set the environment and run the ResNet-50 model with the provided image on QAic or CPU backend. The program outputs the most probable prediction class index for each iteration.   </p> <pre><code>cd build/release ./qaic-onnxrt-resnet50-test -i &lt;path/to/input/png/image&gt;  -m  ../../resnet50/resnet50.yaml </code></pre> <p>Test options   </p> Option Description -m, --model-config [Required] Path to the model-setting yaml file -i, --input-path [Required]  Path to the input PNG image file -b, --backend [Optional] Default='qaic', Specify qaic/cpu as backend -d, --device-id [Optional]  Default=0 Specify qaic device ID -n, --num-iter [Optional]"},{"location":"ONNXRT%20QAIC/onnxruntime/#running-the-resnet-python-sample","title":"Running the ResNet Python sample","text":"<pre><code>Run test_resnet.py at /opt/qti-aic/integrations/qaic_onnxrt/tests/resnet50  python test_resnet.py --model_config ./resnet50/resnet50.yaml   --input_file &lt;/path/to/png/image&gt;  </code></pre> <p>Test options</p> Option Description --model_config [Required] Path to the model-setting yaml file --input_file [Required]  Path to the input PNG image file --backend [Optional] Default='qaic', Specify qaic/cpu as backend --device_id [Optional]  Default=0 Specify qaic device ID --num_iter [Optional]"},{"location":"ONNXRT%20QAIC/onnxruntime/#running-models-with-generic-qaic-ep-test","title":"Running models with generic QAic EP test","text":"<p><code>test_qaic_ep.py</code> is a generic test runner for compilation, execution on AIC100.</p> <p>Run <code>test_qaic_ep.py</code> at <code>/opt/qti-aic/integrations/qaic_onnxrt/tests/</code> -</p> <pre><code>python test_qaic_ep.py --model_config ./resnet50/resnet50.yaml   --input_file_list &lt;/path/to/input/list&gt;   </code></pre> <p>Test options    </p> Option Description --model_config [Required] Path to the model-setting yaml file --input_file_list [Required]  Path of the file (.txt) containing list of batched inputs in .raw format --backend [Optional] Default='qaic', Specify qaic/cpu as backend --device_id [Optional]  Default=0 Specify qaic device ID --num_iter [Optional] --max_threads [Optional]  Default=1000 Maximum no. of threads to run inferences --log_level [Optional]"},{"location":"ONNXRT%20QAIC/onnxruntime/#execution-through-onnxruntime-test-framework","title":"Execution through Onnxruntime test framework","text":"<ul> <li> <p>QAic EP is enabled for execution with onnx_test_runner, onnxruntime_perf_test.</p> </li> <li> <p>For model directory requirements and comprehensive list of options supported, refer to onnxruntime perf test documentation.</p> </li> <li> <p>onnx_test_runner documentation.</p> </li> <li> <p>Sample testdata can be downloaded here.</p> </li> </ul> <pre><code>cd /opt/qti-aic/integrations/qaic_onnxrt/onnxruntime_qaic/build/Release\n\n./onnxruntime_perf_test -e qaic -i 'config|/path/to/resnet50.yaml aic_device_id|0' -m times -r 1000 /path/to/model.onnx\n</code></pre> <pre><code>./onnx_test_runner -e qaic /path/to/model/dir\n</code></pre>"},{"location":"Python-API/qaic/qaic/","title":"qaic package","text":"<p><code>qaic</code> - qaic package provides a way to run inference on Qualcomm Cloud AI 100 card.</p>"},{"location":"Python-API/qaic/qaic/#description","title":"Description","text":"<p>A user can create a session object by passing in </p> <ol> <li> <p>with a <code>.onnx</code> file.</p> </li> <li> <p>with a precompiled qpc as model_path, in case a user already has compiled qpc. Full path to qpc.bin should be passed while using precompiled binary.</p> </li> </ol> Note <p>QPC : Qualcomm Program Container</p>"},{"location":"Python-API/qaic/qaic/#example-on-how-to-run-an-inference","title":"Example on how to run an inference","text":"option 1 : Compiles the onnx file to generate QPC and sets up session for inference<pre><code>import qaic\nimport numpy as np\nsess = qaic.Session('/path/to/model/model.onnx') \ninput_dict = {'input_name': input_data}\noutput = sess.run(input_dict)\n</code></pre> option 2 : Uses generate QPC and sets up session for inference<pre><code>import qaic\nimport numpy as np\nsess = qaic.Session('/path/to/model/qpc.bin') # option 2 : Session uses compiled QPC file to \ninput_dict = {'input_name': input_data}\noutput = sess.run(input_dict)\n</code></pre>"},{"location":"Python-API/qaic/qaic/#example-for-benchmarking","title":"Example for benchmarking","text":"<pre><code>import qaic\nsess = qaic.Session(model_path='/path/to/model', backend='aic', options_path = '/path/to/yaml') # model_path can be either onnx or precompiled qpc\ninf_completed, inf_rate, inf_time, batch_size = sess.run_benchmark()\n</code></pre>"},{"location":"Python-API/qaic/qaic/#limitations","title":"Limitations","text":"<ul> <li>Currently only QAic backend is supported. We plan support for QNN backend in future releases. </li> <li>APIs are compatible with only Python 3.8 </li> <li>These APIs are supported only on x86-64 platforms </li> </ul>"},{"location":"Python-API/qaic/qaic/#class-session","title":"class Session","text":"<p>Session is the entry point of these APIs. Session is a factory method which user needs to call to create an instance of session. A model is compiled by default when creating a session.</p> <pre><code>Session(model_path, **kwargs)\n</code></pre> <p>Session creates a session object based on the qpc provided.</p>"},{"location":"Python-API/qaic/qaic/#parameters","title":"Parameters","text":"Parameter Type Description <code>model_path</code> <code>str</code> path to .onnx file or .bin file i.e. the compiled model QPC <code>**kwargs</code> refer to the keyword arguments listed below Keyword Arguments Type Description <code>dev_id</code> <code>int</code> Device on which to run the inference. Default is 0 <code>num_activations</code> <code>int</code> Number of instances on network to be activated. <code>set_size</code> <code>int</code> Number of ExecObj to be created. <code>mos</code> <code>int</code> Effort level to reduce the on-chip memory. <code>ols</code> <code>int</code> Factor to increasing splitting of network for parallelism <code>aic_num_cores</code> <code>int</code> Number of aic cores to be used for inference <code>convert_to_fp16</code> <code>bool</code> Run all floating-point in fp16 <code>onnx_define_symbol</code> <code>(list[tuple(str, int)])</code> Define an onnx symbol with its value <code>output_dir</code> <code>str</code> Stores model binaries at directory location provided <code>output_node_names</code> <code>(list[str])</code> Output node names should be in the order as present in model file. This option is mandatory for TF models <code>model_inputs list</code> <code>dict</code> `Provide input node name with its data type and shape. Dict must contain keys 'input_name', 'input_type','input_shape'.  This is mandatory for pytorch models <code>allocator_dealloac_delay</code> <code>int</code> Option to increase the lifetime of buffers to reduce false dependencies <code>size_split_granularity</code> <code>int</code> Option to specify a maximum tile size target for operations that may be too large to execute out of fast memory. Tile size in KiB between 512 - 2048 <code>vtcm_working_set_limit_ratio</code> <code>float</code> Option to Specify the maximum ratio amount of fast memory to DDR any single instruction is allowed use of all available value between <code>execute_nodes_in_fp16</code> <code>(list[str])</code> Run all insances of the operators in this list with FP16 <code>node_precision_info_file</code> <code>str</code> Load model loader precision file which contains first output name of operator instances required to be executed in FP16 or FP32. <code>keep_original_precision_for_nodes</code> <code>(list[str])</code> Run all insances of the operators in this list with original precision during generation of quantized precision model even if the operator is supported in Int8 precision <code>custom_io_list_file</code> <code>str</code> Custom I/O config file in yaml format containing layout, precision scale and offset for each input and output of the model. <code>dump_custom_io_config_template_file</code> <code>str</code> Dumps the yaml template for Custom I/O configuration <code>external_quantization_file</code> <code>str</code> Load the externally generated quantization profile <code>quantization_schema_activations</code> <code>str</code> Specify which quantization schema to use for activations. Valid options: asymmetric, symmetric, symmetric_with_uint8 (default), symmetric_with_power2_scale <code>quantization_schema_constants</code> <code>str</code> Specify which quantization schema to use for constants. Valid options: asymmetric, symmetric, symmetric_with_uint8 <code>quantization_calibration</code> <code>str</code> Specify which quantization calibration to use Default is None (MinMax calibration is applied). Valid options: None (default), KLMinimization, KLMinimizationV2, Percentile, MSE and SQNR. <code>percentile_calibration_value</code> <code>float</code> Specify the percentile value to be used with Percentile calibration method. The specified float value must lie within 90 and 100, default: 100. <code>num_histogram_bins</code> <code>int</code> Sets the num of histogram bins that will be used in profiling every node. Default value is 512 <code>quantization_precision</code> <code>str</code> Specify which quantization precision to use. Int8(default) is only supported precision for now. <code>quantization_precision_bias</code> <code>str</code> Specify which quantization precision to use. Value options: Int8, Int32 (default) <code>enable_rowwise</code> <code>bool</code> Enable rowwise quantization of FullyConnected and SparseLengthsSum ops. <code>enable_channelwise</code> <code>bool</code> Enable channelwise quantization of Convolution op. <code>dump_profile</code> <code>str</code> Perform quantization profiling for a given graph and dump result to the file. Compilation will be done after dumping profile unlike qaic-exec <code>load_profile</code> <code>str</code> Load quantization profile file and quantize the graph. The profile file to be loaded is the one which is dumped through option -dump-profile. <code>convert_to_quantize</code> <code>bool</code> If -load-profile option is not provided then input data is profiled and run in quantized mode. Default is off. Also set-quantization-* options as per requirement. Do not use this option along with -dump-profile or -load-profile. <code>load_embedding_tables</code> <code>str</code> Load embedding tables from this zip file for DLRM and RecSys models. <code>dump_embedding_tables</code> <code>str</code> Extract embedding tables from pytorch model and dump them in the zip file specified. <code>mdp_load_partition_config</code> <code>str</code> Load config file for partitioning a graph across devices. <code>mdp_dump_partition_config</code> <code>str</code> Dump config file for partitioning a graph across devices. <code>host_preproc</code> <code>bool</code> Enable all pre-/post-processing on host <code>aic_preproc</code> <code>bool</code> Disable all pre-/post-processing on host. Operations are performed on AI 100 instead. <code>aic_enable_depth_first</code> <code>bool</code> Enables DFS with default memory size <code>aic_depth_first_mem</code> <code>int</code> Sets DFS memory size. number must be choosen from [8,32] <code>stats_batchsize</code> <code>int</code> This option is used to normalize performance statistics to be per inference <code>always_expand_onnx_functions</code> <code>bool</code> This option forces the expansion ONNX functions. <code>enable_debug</code> <code>bool</code> Enables debug mode during model compilation <code>time_passes</code> <code>bool</code> Enables printing of compile-time statistics <code>io_crc</code> <code>bool</code> Enables CRC check for inputs and outputs of the network. <code>io_crc_stride</code> <code>int</code> Specifies size of stride to calculate CRC in the stride section <code>sdp_cluster_sizes</code> <code>(list[int])</code> Enables single device partitioning and sets the cluster configuration <code>profiling_threads</code> <code>int</code> This option is used to assign the number of threads to use for for quantization profile generation <code>compile_threads</code> <code>int</code> Sets the number of parallel threads used for compilation. <code>use_producer_dma</code> <code>bool</code> Initiate NSP DMAs from the thread that produces data being transferred <code>aic_perf_warnings</code> <code>bool</code> Print performance warning messages <code>aic_perf_metrics</code> <code>bool</code> Print compiler performance metrics <code>aic_pmu_recipe</code> <code>str</code> Enable the PMU selection based on built-in recipe: AxiRd, AxiWr, AxiRdWr, KernelUtil, HmxMacs <code>aic_pmu_events</code> <code>str</code> Track events in NSP cores. Up to 8 events are supported <code>dynamic_shape_input</code> <code>(list[str])</code> Inform the compiler which inputs should be treated as having dynamic shape <code>multicast_weights</code> <code>bool</code> Reduce DDR bandwidth by loading weights used on multiple-cores only once and multicasting to other cores. <code>ddr_stats</code> <code>bool</code> Used to collect DDR traffic details at per core level. <code>combine_inputs</code> <code>bool</code> When enabled combines inputs into fewer buffers for  transfer to device. <code>combine_outputs</code> <code>bool</code> When enabled combines outputs into a single buffer for transfer to host. <code>enable_metrics</code> <code>bool</code> Set value to True if you are interested in getting performance metrics for inference runs on a session. (Can not be used if enable_profiling is set to True.) <code>enable_profiling</code> <code>bool</code> Set value to True if you want to profile the inferences and get performance metrics for inference runs on a session. (Can not be used if enable_metrics is set to True.)"},{"location":"Python-API/qaic/qaic/#returns","title":"Returns","text":"<p>Session object.</p>"},{"location":"Python-API/qaic/qaic/#example","title":"Example","text":"using options_path yaml file<pre><code>sess = qaic.Session('/path/to/model', options_path = '/path/to/options.yaml') \ninput_dict = {'input_name': input_data}\noutput = sess.run(input_dict)\n</code></pre> sample contents of yaml file<pre><code>aic_num_cores: 4\nnum_activations: 1\nconvert_to_fp16: true\nonnx_define_symbol:\nbatch: 1\noutput_dir: './resnet_qpc'\n</code></pre> using keyword args<pre><code>    sess = qaic.Session('/path/to/model_qpc/*.bin', num_activations=4, set_size=10)\ninput_dict = {'input_name': input_data}\noutput = sess.run(input_dict)\n</code></pre>"},{"location":"Python-API/qaic/qaic/#api-list-function-variables-for-session-object","title":"API List (Function variables for session object)","text":"<p>Session class has following methods.</p>"},{"location":"Python-API/qaic/qaic/#backend_options","title":"backend_options()","text":"<p>Returns</p> <p>A dict of options that can be configured after creating session </p> Usage example<pre><code>backend_options_dict = session.backend_options()\n</code></pre>"},{"location":"Python-API/qaic/qaic/#get_metrics","title":"get_metrics()","text":"<p>Returns</p> <p>A dictionary containing the following metrics:</p> <pre><code>- num_of_inferences (int): The number of inferences.\n- min_latency (float): The minimum inference time.\n- max_latency (float): The maximum inference time.\n- P25 (float): The 25th percentile latency.\n- P50 (float): The 50th percentile latency (median).\n- P75 (float): The 75th percentile latency.\n- P90 (float): The 90th percentile latency.\n- P99 (float): The 99th percentile latency.\n- P999 (float): The 99.9th percentile latency.\n- total_inference_time (float): The sum of individual insference times.\n- avg_latency (float): The average latency.\n</code></pre> Usage example<pre><code>metrics_dict = session.get_metrics()\n</code></pre>"},{"location":"Python-API/qaic/qaic/#model_input_shape_dict","title":"model_input_shape_dict()","text":"<p>Returns</p> <p>A dict with input_name as key and input_shape, input_type as values     </p> Usage example<pre><code>input_shape_dict = session.model_input_shape_dict() \n</code></pre>"},{"location":"Python-API/qaic/qaic/#model_output_shape_dict","title":"model_output_shape_dict()","text":"<p>Returns</p> <p>A dict with output_name as key and output_shape, output_type as values  </p> Usage example<pre><code>output_shape_dict = session.model_output_shape_dict()\n</code></pre>"},{"location":"Python-API/qaic/qaic/#print_metrics","title":"print_metrics()","text":"<p>Returns</p> <p><code>None</code></p> Usage example<pre><code>session.print_metrics() \n</code></pre> Note <p>This method assumes that either the 'enable_profiling' or 'enable_metrics' attribute is set to True.</p> <p>Sample Output:</p> <pre><code>Number of inferences utilized for calculation are 999\nMinimum latency observed 0.0009578340000000001 s\nMaximum latency observed 0.002209001 s\nAverage latency / inference time observed is 0.0012380756316316324 s\nP25 / 25% of inferences observed latency less than 0.001095435 s\nP50 / 50% of inferences observed latency less than 0.0012522870000000001 s\nP75 / 75% of inferences observed latency less than 0.001299786 s\nP90 / 90% of inferences observed latency less than 0.002209001 s\nP99 / 99% of inferences observed latency less than 0.0016082370000000002 s\nSum of all the inference times 1.2368375560000007 s\nAverage latency / inference time observed is 0.0012380756316316324 s\n</code></pre>"},{"location":"Python-API/qaic/qaic/#print_profile_data","title":"print_profile_data","text":"<p>Returns</p> <p><code>None</code></p> Usage example<pre><code>session.print_profile_data(n)   \n</code></pre> <p>Print profiling data for the first n iterations</p> Note <p>This function only works when 'enable_profiling' is set to True for the Session.</p> <ul> <li>This method assumes that the 'enable_profiling' attribute is set to True, and the 'profiling_results' attribute contains the profiling data for each iteration.</li> <li>The method prints the profiling data in a tabular format, including the file, line, function, number of calls, function time (seconds), and total time (seconds) for each function.</li> </ul> <p>Sample Output:</p> <pre><code>|  File-Line-Function  | |  num calls  | |  func time  | |  tot time  |\n\n('~', 0, \"&lt;method 'astype' of 'numpy.ndarray' objects&gt;\") 1 0.000149101 0.000149101\n\n('~', 0, '&lt;built-in method numpy.empty&gt;') 1 2.38e-06 2.38e-06\n\n('~', 0, '&lt;built-in method numpy.frombuffer&gt;') 1 4.22e-06 4.22e-06\n</code></pre>"},{"location":"Python-API/qaic/qaic/#reset","title":"reset()","text":"<p>Returns</p> <p><code>None</code> </p> Usage example<pre><code>session.reset() \n</code></pre> <p>Releases all the device resources acquired by session </p>"},{"location":"Python-API/qaic/qaic/#setup","title":"setup()","text":"<p>Returns</p> <p><code>None</code> </p> Usage example<pre><code>session.setup() \n</code></pre> <p>Loads the network to the device.</p> <p>Network is usually loaded during first call of run. If this is called before that, network will be already loaded when first run is called.</p>"},{"location":"Python-API/qaic/qaic/#runinput_dict","title":"run(input_dict)","text":"<p>Returns</p> <p>A dict with output_name and output_value of inference   </p> Usage example<pre><code>output = session.run(input_dict)    \n</code></pre> <p>input_dict should have input_name as key and value should be a numpy array</p>"},{"location":"Python-API/qaic/qaic/#run_benchmark","title":"run_benchmark()","text":"<p>Returns</p> <p>inf_completed: Total number of inferences run inf_rate: Inf/Sec of the model inf_time: total time taken to run inferences batch_size: Batch Size used by model</p> Usage example<pre><code>inf_completed, inf_rate, inf_time, batch_size = session.run_benchmark() \n</code></pre> <p>It accepts following args:</p> <p>num_inferences: num of inferences to run in benchmarking. Default 40 inf_time: duration for which inference is to be run in seconds. Default None input_dict: Input to be used in inference. Default random</p> Note <p>num_inferences and time cannot be used together.</p> <p>This API uses C++ benchmarking APIs and doesn't take into account python overheads</p>"},{"location":"Python-API/qaic/qaic/#update_backend_optionskwargs","title":"update_backend_options(**kwargs)","text":"<p>Returns</p> <p><code>None</code> </p> Usage example<pre><code>session.update_backend_options(num_activations = 2)     \n</code></pre> <p>Update option specified in kwargs</p> <p>For example:</p> <p><code>num_activation</code>, <code>dev_id</code>, <code>set_size</code> can be configured with this API.</p>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/","title":"class InferenceBenchmarkSet","text":"<p>Help on class InferenceBenchmarkSet in module <code>qaicrt</code>:</p>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#inferencebenchmarksetlogger","title":"InferenceBenchmarkSet(Logger)","text":"<p><pre><code>InferenceBenchmarkSet(context, qpc, devID, setSize, numActivations, inferenceDataVector, properties) -&gt; infrenceBenchmarkSet\n</code></pre> Creates and returns a new object for running benchmarks.</p> <p>Parameters</p> Parameter Description <code>context</code> A previously created context. <code>qpc</code> A previously created Qpc Object. <code>devID</code> [optional] Device on which benchmark is to be run. By default device will be auto-picked <code>setSize</code> Number of ExecObj per program. <code>numActivations</code> Number of separate activations. Set this value to 1 to have a single instance of the program on device Set this value to a specific number of activations. <code>inferenceDataVector</code> A vector of QBuffers for initial input and output data. This is the first set of data to be use for the first submission during benchmark. If new data is needed for each inference, the user can register a callback which will return the ExecObj pointer, which can be used to  setData for the next inference. Alternatively, the same data can be used repeatedly <code>properties</code> [optional] Properties for the program and queues to created by the benchmark set. One of the key properties is number of threads per queue which can be changed in the queue properties <p>Once the Benchmark object is created, the specified program will be initialized. The inferenceDataVector must be compatible with the program. Depending on the <code>setSize</code> and <code>numActivations</code> configured, <code>ExecObj(s)</code>  will be created per program. <code>numInferencesToRun</code> will be completed each time the start signal is given through <code>run()</code> method. </p> <p>Methods defined here:</p>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#init","title":"init","text":"<pre><code>__init__(self: qaicrt.InferenceBenchmarkSet, context: qaicrt.Context, qpc: qaicrt.Qpc, dev: Optional[int] = None, setSize: int, numActivations: int, inferenceDataVector: List[qaicrt.QBuffer], properties: qaicrt.BenchmarkProperties = None) -&gt; None\n</code></pre>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#disable","title":"disable","text":"<p><pre><code>disable(self: qaicrt.InferenceBenchmarkSet) -&gt; qaicrt.QStatus\n</code></pre> Description</p> <p>Disable InferenceBenchmarkSet.  This will deactivate and release all resources associated with this benchmark.</p> <p>Returns</p> <ul> <li>Status:<ul> <li>qaicrt.QStatus.QS_SUCCESS Successful completion</li> <li>qaicrt.QStatus.QS_ERROR Failed to release benchmark object programs and resources</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#disableexecobjcompletioncallback","title":"disableExecObjCompletionCallback","text":"<pre><code>disableExecObjCompletionCallback(self: qaicrt.InferenceBenchmarkSet) -&gt; None\n</code></pre> <p>Description: </p> <p>Disable ExecObj Completion Callback</p>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#disableopstatscallback","title":"disableOpStatsCallback","text":"<pre><code>disableOpStatsCallback(self: qaicrt.InferenceBenchmarkSet) -&gt; qaicrt.QStatus\n</code></pre> <p>Returns:</p> <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#enable","title":"enable","text":"<pre><code>enable(self: qaicrt.InferenceBenchmarkSet) -&gt; qaicrt.QStatus\n</code></pre> <p>Description: </p> <p>Enable InferenceBenchmarkSet. This will trigger the load and activation of all the program instances on device.  This may fail if resources are not available.</p> <p>Returns: </p> <p>Operational status</p> <ul> <li>Operational status:<ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Failed to setup benchmark object programs and resources</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#enableexecobjcompletioncallback","title":"enableExecObjCompletionCallback","text":"<pre><code>enableExecObjCompletionCallback(self: qaicrt.InferenceBenchmarkSet, callback: Callable[[qaicrt.ExecObj, qaicrt.ExecObjInfo], None]) -&gt; None\n</code></pre> <p>Description</p> <p>Enable ExecObj Completion Callback. When enabled, each time an ExecObj completes, this callback will be called with the ExecObj that completed. This can be used to perform data validation or to extract the inference results.</p> <p>Parameters</p> Parameter Description <code>callback</code> Callback function <p>Returns</p> <p>Operational status.</p> <ul> <li>Operational Status: <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#enableopstatscallback","title":"enableOpStatsCallback","text":"<pre><code>enableOpStatsCallback(self: qaicrt.InferenceBenchmarkSet, callback: Callable[[qaicrt.vector_char, int, qaicrt.QAicOpStatsFormatEnum, int, qaicrt.ExecObjInfo], None], initialSampleIndex: int, numSamplesPerProgramInstance: int, format: qaicrt.QAicOpStatsFormatEnum) -&gt; qaicrt.QStatus\n</code></pre> <p>Description</p> <p>Enable Operational Statistics Callback. OpStats are data collected by the AIC neural network core during execution.</p> <p>Parameters </p> Parameter Description <code>callback</code> Callback function <code>initialSampleIndex</code> The index from 0 to the first ExecObj completion the callback should be called. For example, if set to 3, starting at the 3rd ExecObj completion the callback will be called function <code>format</code> The format of the data to be collected. This may be ASCII or JSON <p>Returns</p> <p>Operational status.</p> <ul> <li>Operational Status: <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Invalid parameter format</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#getinfcompleted","title":"getInfCompleted","text":"<pre><code>getInfCompleted(self: qaicrt.InferenceBenchmarkSet) -&gt; List[int]\n</code></pre> <p>Description</p> <p>Get the number of inferences completed.</p> <p>Returns</p> <p>The number of executions completed, not accounting for the batchSize, and for all activations.</p>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#getlastrunstats","title":"getLastRunStats","text":"<pre><code>getLastRunStats(self: qaicrt.InferenceBenchmarkSet) -&gt; Tuple[int, float, int, int]\n</code></pre> <p>Description</p> <p>Get stats on the last run of the InferenceBenchmarkSet</p> <p>Returns</p> <ul> <li>Tuple of <code>infCompleted</code>, <code>infRate</code>, <code>runtimeUs</code> and <code>batchSize</code>.<ul> <li><code>infCompleted</code>   : Number of inferences completed for all activations.</li> <li><code>infRate</code>        : Inference Rate in inferences per second for all activations.</li> <li><code>runtimeUs</code>      : Duration in microseconds of the last run.</li> <li><code>batchSize</code>      : Number of individual inferences completed in one execution of ExecObj.                    When the program is compiled it may be configured to have a batchsize                    of 1 or larger. The effective inference rate is the infRate multiplied                    by the batchSize.</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#run","title":"run","text":"<pre><code>run(self: qaicrt.InferenceBenchmarkSet, numInferencesToRun: int) -&gt; qaicrt.QStatus\n</code></pre> <p>Description</p> <p>Run the benchmark. This call will block until the numInferencesToRun are completed.</p> <p>Parameters</p> Parameter Description <code>numInferencesToRun</code> Number of inferences to run <p>Returns</p> <ul> <li>Operational Status.<ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Invalid param <code>numInferencesToRun</code></li> <li><code>qaicrt.QStatus.QS_ERROR</code> Inference run failed</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#runforduration","title":"runForDuration","text":"<pre><code>runForDuration(self: qaicrt.InferenceBenchmarkSet, duration: int) -&gt; qaicrt.QStatus\n</code></pre> <p>Description</p> <p>Run the benchmark. This call will block until all the submitted inferences are completed. </p> <p>Parameters</p> Parameter Description <code>duration</code> Duration for which to keep submitting new inference <p>Returns</p> <ul> <li>Operational Status.<ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Invalid param duration</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Inference run failed</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#setliveprofilinglevel","title":"setLiveProfilingLevel","text":"<pre><code>setLiveProfilingLevel(self: qaicrt.InferenceBenchmarkSet, level: qaicrt.LiveReportingLevelEnum) -&gt; qaicrt.QStatus\n</code></pre> <p>Description</p> <p>Set the reporting level once liveReporting is enabled. Will retain the same period of reporting, but will use the new reporting level specified.</p> <p>Parameters</p> Parameter Description <code>level</code> New reporting level.  levels not defined <p>Returns</p> <ul> <li>Operational Status.<ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Live reporting is not enabled, level change fails</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#setlivereporting","title":"setLiveReporting","text":"<pre><code>setLiveReporting(self: qaicrt.InferenceBenchmarkSet, level: qaicrt.LiveReportingLevelEnum = &lt;LiveReportingLevelEnum.LIVE_REPORTING_SUMMARY: 1&gt;, reportingPeriodMs: int = 1000) -&gt; qaicrt.QStatus\n</code></pre> <p>Description</p> <p>Enable live reporting. Live reporting will print to sys.stdout a summary of the  inferences completed at the given period. The data printed may be detailed or summarized, depending on the level selected.</p> <p>Parameters</p> Parameter Description <code>level</code> Level of detail requested for periodic live reporting.  levels not defined <code>reportingPeriodMs</code> The period at which the report will be generated, in milliseconds. <p>Returns</p> <ul> <li>Operational Status.<ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Invalid param level</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_InferenceBenchmarkSet/#static-methods","title":"Static methods","text":"<pre><code>initDefaultProperties from builtins.PyCapsule\ninitDefaultProperties(properties: qaicrt.BenchmarkProperties) -&gt; None\n</code></pre> <p>Initialize properties for InferenceBenchmarkSet to default.</p> <p>Parameters</p> Parameter Description properties Properties to initialize."},{"location":"Python-API/qaicrt/class_util/","title":"class Util","text":"<p>Help on class Util in module <code>qaicrt</code>:</p> <p>Utility class to get information on the AIC device. Supports basic operations on device such as getting device ID, device information, resource information, and so on.</p>"},{"location":"Python-API/qaicrt/class_util/#util","title":"Util()","text":"<p><code>Util() -&gt; util</code></p> <p>Creates and returns a new util object.</p> <p>Methods defined here:</p>"},{"location":"Python-API/qaicrt/class_util/#init","title":"init","text":"<pre><code>__init__(self: qaicrt.Util) -&gt; None\n</code></pre>"},{"location":"Python-API/qaicrt/class_util/#checklibraryversion","title":"checkLibraryVersion","text":"<pre><code>checkLibraryVersion(self: qaicrt.Util) -&gt; qaicrt.QStatus\n</code></pre> <p>Description: </p> <p>Checks Library Version with AicApi header.</p> <ul> <li>Major library version should be equal to LRT_LIB_MAJOR_VERSION.</li> <li>Minor library version should be less than LRT_LIB_MINOR_VERSION.</li> </ul> <p>Returns:</p> <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Driver init failed</li> </ul>"},{"location":"Python-API/qaicrt/class_util/#getaicversion","title":"getAicVersion","text":"<pre><code>getAicVersion(self: qaicrt.Util) -&gt; Tuple[qaicrt.QStatus, int, int, str, str]\n</code></pre> <p>Description: </p> <p>Gets Aic version of the library.</p> <p>Returns: </p> <p>Tuple of operational status, major, minor, patch, variant</p> <ul> <li>Operational status:<ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Internal error in getting aic version</li> </ul> </li> <li>major: Major version of the library</li> <li>minor: Minor version of the library</li> <li>patch: Patch being used by the library</li> <li>variant: Variant of the library being used</li> </ul>"},{"location":"Python-API/qaicrt/class_util/#getdeviceids","title":"getDeviceIds","text":"<pre><code>getDeviceIds(self: qaicrt.Util, deviceType: qaicrt.QAicDeviceType = &lt;QAicDeviceType.QAIC_DEVICE_TYPE_DEFAULT: 1&gt;) -&gt; Tuple[qaicrt.QStatus, qaicrt.QIDList]\n</code></pre> <p>Description</p> <p>Get the list of AIC devices. Optional argument deviceType specifies the type of the device.</p> <p>Parameters</p> Parameter Description <code>deviceType</code> [Optional] Type of device <p>Returns</p> <p>Tuple of operational status and the list of AIC devices.</p> <ul> <li>Operational Status: <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Internal error in getting device list</li> <li><code>qaicrt.QStatus.QS_NODEV</code> No valid device</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Driver init failed or Bad device AddrInfo</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_util/#getdeviceinfo","title":"getDeviceInfo","text":"<p><pre><code>getDeviceInfo(self: qaicrt.Util, devID: int) -&gt; Tuple[qaicrt.QStatus, qaicrt.QDevInfo]\n</code></pre> Description</p> <p>Get device information for the device specified. </p> <p>Parameters </p> Parameter Description <code>devID</code> A valid device ID returned from getDeviceIds() <p>Returns</p> <p>Tuple of operational status and Device info.</p> <ul> <li>Operational Status: <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Internal error in getting device info</li> <li><code>qaicrt.QStatus.QS_NODEV</code> No valid device</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Driver init failed</li> <li><code>qaicrt.QStatus.QS_AGAIN</code> Error on reload Device Id</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_util/#getperformanceinfo","title":"getPerformanceInfo","text":"<p><pre><code>getPerformanceInfo(self: qaicrt.Util, devID: int) -&gt; Tuple[qaicrt.QStatus, qaicrt.QPerformanceInfo]\n</code></pre> Description</p> <p>Get device performance information, this is a simple query that returns performance info. Note the same information is availablethrough GetDeviceInfo, however this is a more lightweight ap allowing to retrieve only the performance info.</p> <p>Parameters</p> Parameter Description <code>devID</code> A valid device ID returned from getDeviceIds() <p>Returns</p> <p>Tuple of operational status and Performance info.</p> <ul> <li>Operational Status: <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Internal error in getting performance info</li> <li><code>qaicrt.QStatus.QS_NODEV</code> Device not found, the device ID provided is not available</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Driver init failed</li> <li><code>qaicrt.QStatus.QS_DEV_ERROR</code> Device validity error</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_util/#getresourceinfo","title":"getResourceInfo","text":"<pre><code>getResourceInfo(self: qaicrt.Util, devID: int) -&gt; Tuple[qaicrt.QStatus, qaicrt.QResourceInfo]\n</code></pre> <p>Description</p> <p>Get device performance information. This is a simple query that returns status of dynamic resources such as free memory, etc..Note that the same information is available through GetDeviceInfo,however this is a more lightweight api allowing to retrieve only the resourceinfo.</p> <p>Parameters</p> Parameter Description <code>devID</code> A valid device ID returned from getDeviceIds() <p>Returns</p> <p>Tuple of operational status  and Resource info.</p> <ul> <li>Operational Status: <ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_INVAL</code> Internal error in getting resource info</li> <li><code>qaicrt.QStatus.QS_NODEV</code> Device not found, the device ID provided is not available</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Driver init failed</li> <li><code>qaicrt.QStatus.QS_DEV_ERROR</code> Device validity error</li> </ul> </li> </ul>"},{"location":"Python-API/qaicrt/class_util/#lockdevice","title":"lockDevice","text":"<pre><code>lockDevice(self: qaicrt.Util, devID: int, lock: bool, block: bool) -&gt; qaicrt.QStatus\n</code></pre> <p>Description</p> <p>Lock a device for exclusive access. This is an advisory lock, so if an uncooperative app accesses a locked device the operations are not blocked. This functionality is enabled by setting env var QAIC_SERIALIZE_DEVICE to 1.If the env var is unset, the call returns QS_UNSUPPORTED.</p> <p>Parameters</p> Parameter Description <code>devID</code> A valid device ID returned from getDeviceIds() <code>lock</code> locks or unlocks the device <code>block</code> Blocking or non-blocking. No-op if lock is false. <p>Returns</p> <p>Operational Status.</p> <ul> <li>Operational Status<ul> <li><code>qaicrt.QStatus.QS_SUCCESS</code> Successful completion</li> <li><code>qaicrt.QStatus.QS_NODEV</code> Device not found</li> <li><code>qaicrt.QStatus.QS_ERROR</code> Driver init failed</li> <li><code>qaicrt.QStatus.QS_UNSUPPORTED</code> Env var for advisory lock not set</li> <li><code>qaicrt.QStatus.QS_BUSY</code> Device locked</li> </ul> </li> </ul>"},{"location":"images/","title":"Index","text":"<p>Space for images used in documentation</p>"}]}